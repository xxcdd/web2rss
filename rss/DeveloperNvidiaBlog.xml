<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>DeveloperNvidiaBlog</title>
    <link>https://developer.nvidia.com/zh-cn/blog/recent-posts/</link>
    <description>Latest posts from https://developer.nvidia.com/zh-cn/blog/recent-posts/. follow.is: None</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Wed, 24 Sep 2025 01:41:36 +0000</lastBuildDate>
    <item>
      <title>如何利用跨规模网络将分布式数据中心连接成大型 AI 工厂</title>
      <link>https://developer.nvidia.com/zh-cn/blog/how-to-connect-distributed-data-centers-into-large-ai-factories-with-scale-across-networking/</link>
      <description>2025年 9月 9日如何利用跨规模网络将分布式数据中心连接成大型 AI 工厂AI 技术日益复杂，训练与推理领域的新进展对数据中心提出了更高的要求。尽管数据中心的功能正在迅速扩展，但其基础设施受限于基本的物理条件，1 MIN READ</description>
    </item>
    <item>
      <title>NVIDIA Blackwell Ultra 首次亮相 MLPerf 完成推理新纪录</title>
      <link>https://developer.nvidia.com/zh-cn/blog/nvidia-blackwell-ultra-sets-new-inference-records-in-mlperf-debut/</link>
      <description>2025年 9月 9日NVIDIA Blackwell Ultra 首次亮相 MLPerf 完成推理新纪录随着大语言模型（LLM）规模的不断扩大，其智能水平也显著提升，领先开发者推出的开放模型已具备数千亿参数。与此同时，3 MIN READ</description>
    </item>
    <item>
      <title>NVIDIA Rubin CPX 加速百万级以上 token 上下文工作负载的推理性能和效率</title>
      <link>https://developer.nvidia.com/zh-cn/blog/nvidia-rubin-cpx-accelerates-inference-performance-and-efficiency-for-1m-token-context-workloads/</link>
      <description>2025年 9月 9日NVIDIA Rubin CPX 加速百万级以上 token 上下文工作负载的推理性能和效率推理正成为人工智能复杂性的前沿领域。现代模型正逐步演变为具备多步推理能力、持久化记忆和长时程上下文理解的代理式系统，使其能够胜任软件开发、2 MIN READ</description>
    </item>
    <item>
      <title>借助 NVIDIA RTX PRO Blackwell 服务器版本，将蛋白质结构推理速度提高 100 多倍</title>
      <link>https://developer.nvidia.com/zh-cn/blog/accelerate-protein-structure-inference-over-100x-with-nvidia-rtx-pro-6000-blackwell-server-edition/</link>
      <description>2025年 9月 10日借助 NVIDIA RTX PRO Blackwell 服务器版本，将蛋白质结构推理速度提高 100 多倍了解蛋白质结构的研究比以往任何时候都更加重要。从加快药物研发到为未来可能的疫情做好准备，2 MIN READ</description>
    </item>
    <item>
      <title>使用 NVIDIA NIM Operator 3.0.0 部署可扩展的 AI 推理</title>
      <link>https://developer.nvidia.com/zh-cn/blog/deploy-scalable-ai-inference-with-nvidia-nim-operator-3-0-0/</link>
      <description>2025年 9月 10日使用 NVIDIA NIM Operator 3.0.0 部署可扩展的 AI 推理AI 模型、推理引擎后端以及分布式推理框架在架构、复杂性和规模上持续演进。面对快速的技术变革，3 MIN READ</description>
    </item>
    <item>
      <title>借助 NVIDIA Rivermax 和 NEIO FastSocket，更大限度地提高金融服务的低延迟网络性能</title>
      <link>https://developer.nvidia.com/zh-cn/blog/maximizing-low-latency-networking-performance-for-financial-services-with-nvidia-rivermax-and-neio-fastsocket/</link>
      <description>2025年 9月 10日借助 NVIDIA Rivermax 和 NEIO FastSocket，更大限度地提高金融服务的低延迟网络性能超低延迟与可靠的数据包传输是金融服务、云游戏以及媒体和娱乐等现代应用的关键需求。在这些领域中，2 MIN READ</description>
    </item>
    <item>
      <title>开发者现在可以直接从自己喜欢的第三方平台下载 CUDA</title>
      <link>https://developer.nvidia.com/zh-cn/blog/developers-can-now-get-cuda-directly-from-their-favorite-third-party-platforms/</link>
      <description>2025年 9月 10日开发者现在可以直接从自己喜欢的第三方平台下载 CUDA对开发者而言，构建和部署应用往往充满挑战，需要协调软件与硬件功能之间的复杂关系。确保每个基础软件组件不仅正确安装，而且版本符合要求，1 MIN READ</description>
    </item>
    <item>
      <title>使用 AI 击杀链框架对 AI 驱动应用进行攻击建模</title>
      <link>https://developer.nvidia.com/zh-cn/blog/modeling-attacks-on-ai-powered-apps-with-the-ai-kill-chain-framework/</link>
      <description>2025年 9月 11日使用 AI 击杀链框架对 AI 驱动应用进行攻击建模AI 赋能的应用带来了传统安全模型难以全面覆盖的新攻击面，尤其是当这些代理式系统具备自主性时。应对持续演变的攻击面，其核心原则十分明确：2 MIN READ</description>
    </item>
    <item>
      <title>使用 NVIDIA CUDA 加速的 VC-6 构建高性能视觉 AI 工作流</title>
      <link>https://developer.nvidia.com/zh-cn/blog/build-high-performance-vision-ai-pipelines-with-nvidia-cuda-accelerated-vc-6/</link>
      <description>2025年 9月 11日使用 NVIDIA CUDA 加速的 VC-6 构建高性能视觉 AI 工作流NVIDIA GPU 持续提升的计算吞吐量为优化视觉 AI 工作负载带来了新的机遇：让硬件持续高效地处理数据。随着 GPU 性能的不断增强，4 MIN READ</description>
    </item>
    <item>
      <title>量化感知训练如何实现低精度恢复</title>
      <link>https://developer.nvidia.com/zh-cn/blog/how-quantization-aware-training-enables-low-precision-accuracy-recovery/</link>
      <description>2025年 9月 11日量化感知训练如何实现低精度恢复训练 AI 模型后，可采用多种压缩技术来优化模型的部署。其中较为常见的是后训练量化（PTQ），该方法通过数值缩放技术，3 MIN READ</description>
    </item>
    <item>
      <title>在 OpenRouter 上使用 NVIDIA Nemotron 构建报告生成 AI 智能体</title>
      <link>https://developer.nvidia.com/zh-cn/blog/build-a-report-generator-ai-agent-with-nvidia-nemotron-on-openrouter/</link>
      <description>2025年 9月 15日在 OpenRouter 上使用 NVIDIA Nemotron 构建报告生成 AI 智能体与传统系统遵循预设路径不同，AI智能体依托大语言模型（LLM）进行决策，能够适应动态变化的需求，并执行复杂的推理任务。6 MIN READ</description>
    </item>
    <item>
      <title>全新 Qwen3-Next 开源模型预览：MoE 架构在 NVIDIA 平台实现更高精度与加速并行处理速度</title>
      <link>https://developer.nvidia.com/zh-cn/blog/new-open-source-qwen3-next-models-preview-hybrid-moe-architecture-delivering-improved-accuracy-and-accelerated-parallel-processing-across-nvidia-platform/</link>
      <description>2025年 9月 15日全新 Qwen3-Next 开源模型预览：MoE 架构在 NVIDIA 平台实现更高精度与加速并行处理速度随着 AI 模型规模不断扩大且处理的文本序列越来越长，效率变得与规模同样重要。 为展示未来趋势，2 MIN READ</description>
    </item>
    <item>
      <title>利用 NVIDIA Run:ai 模型流技术降低大型语言模型推理的冷启动延迟</title>
      <link>https://developer.nvidia.com/zh-cn/blog/reducing-cold-start-latency-for-llm-inference-with-nvidia-runai-model-streamer/</link>
      <description>2025年 9月 16日利用 NVIDIA Run:ai 模型流技术降低大型语言模型推理的冷启动延迟部署大语言模型（LLM）在优化推理效率方面带来了显著挑战。其中，冷启动延迟——即模型加载到 GPU 显存所需的时间较长…5 MIN READ</description>
    </item>
    <item>
      <title>适用于 Python GPU 加速视频处理的 PyNvVideoCodec 2.0 新增功能</title>
      <link>https://developer.nvidia.com/zh-cn/blog/whats-new-in-pynvvideocodec-2-0-for-python-gpu-accelerated-video-processing/</link>
      <description>2025年 9月 16日适用于 Python GPU 加速视频处理的 PyNvVideoCodec 2.0 新增功能Python 中的硬件加速视频处理变得更加便捷。 PyNvVideoCodec 是一个基于 NVIDIA Python 的库，1 MIN READ</description>
    </item>
    <item>
      <title>NVIDIA RAPIDS 25.08 版本新增 cuML 分析器、Polars GPU 引擎更新、增加算法支持及更多功能</title>
      <link>https://developer.nvidia.com/zh-cn/blog/nvidia-rapids-25-08-adds-new-profiler-for-cuml-updates-to-the-polars-gpu-engine-additional-algorithm-support-and-more/</link>
      <description>2025年 9月 17日NVIDIA RAPIDS 25.08 版本新增 cuML 分析器、Polars GPU 引擎更新、增加算法支持及更多功能RAPIDS 25.08 版本持续突破极限，新增多项功能，进一步提升了加速数据科学的易用性和可扩展性，包括： 请在下方详细了解新增功能。3 MIN READ</description>
    </item>
    <item>
      <title>用于降低 AI 推理延迟的预测性解码简介</title>
      <link>https://developer.nvidia.com/zh-cn/blog/an-introduction-to-speculative-decoding-for-reducing-latency-in-ai-inference/</link>
      <description>2025年 9月 17日用于降低 AI 推理延迟的预测性解码简介使用大语言模型（LLM）生成文本时，通常会面临一个基本瓶颈。尽管 GPU 能够提供强大的计算能力，但由于自回归生成本质上是顺序进行的，2 MIN READ</description>
    </item>
  </channel>
</rss>
