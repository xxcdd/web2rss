<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>DeveloperNvidiaBlog</title>
    <link>https://developer.nvidia.com/zh-cn/blog/recent-posts/</link>
    <description>Latest posts from https://developer.nvidia.com/zh-cn/blog/recent-posts/. follow.is: None</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Thu, 12 Feb 2026 02:36:18 +0000</lastBuildDate>
    <item>
      <title>NVIDIA TensorRT for RTX 中的自适应推理可实现自动优化</title>
      <link>https://developer.nvidia.cn/blog/adaptive-inference-in-nvidia-tensorrt-for-rtx-enables-automatic-optimization/</link>
      <description>2026年 1月 26日NVIDIA TensorRT for RTX 中的自适应推理可实现自动优化传统上，在各种消费级硬件中部署 AI 应用需要进行权衡。可以针对特定 GPU 配置进行优化，以牺牲便携性为代价来提升性能；3 MIN READ</description>
    </item>
    <item>
      <title>如何使用 NVIDIA Earth-2 解锁粗略气候投影的局部细节</title>
      <link>https://developer.nvidia.cn/blog/how-to-unlock-local-detail-in-coarse-climate-projections-with-nvidia-earth-2/</link>
      <description>2026年 1月 26日如何使用 NVIDIA Earth-2 解锁粗略气候投影的局部细节全球气候模型擅长大局把握，但飓风和台风等局部极端气候现象往往在细节中被忽略。这些现象依然存在，只需借助合适的工具，3 MIN READ</description>
    </item>
    <item>
      <title>使用 NVIDIA TensorRT LLM 部署 MiniMax M2/M2.1 稀疏 MoE 大模型</title>
      <link>https://developer.nvidia.cn/blog/deploying-minimax-sparse-moe-llm-using-tensorrt-llm/</link>
      <description>2026年 1月 27日使用 NVIDIA TensorRT LLM 部署 MiniMax M2/M2.1 稀疏 MoE 大模型MiniMax M2/M2.1 是一款面向 Agents 和 Coding 工作流的开源稀疏 MoE 模型，在工具调用、2 MIN READ</description>
    </item>
    <item>
      <title>通过开放式即插即用产品加速扩散模型</title>
      <link>https://developer.nvidia.cn/blog/accelerating-diffusion-models-with-an-open-plug-and-play-offering/</link>
      <description>2026年 1月 27日通过开放式即插即用产品加速扩散模型大规模扩散模型的近期进展深刻改变了生成式 AI 在多个领域的应用，涵盖图像合成、音频生成、3D 素材创建、分子设计等。2 MIN READ</description>
    </item>
    <item>
      <title>基于时间的公平共享实现 Kubernetes 集群 GPU 分配均衡</title>
      <link>https://developer.nvidia.cn/blog/ensuring-balanced-gpu-allocation-in-kubernetes-clusters-with-time-based-fairshare/</link>
      <description>2026年 1月 28日基于时间的公平共享实现 Kubernetes 集群 GPU 分配均衡NVIDIA Run:ai v2.24 引入了基于时间的公平分享，这是一种全新的调度模式，可为 Kubernetes 集群实现公平分享调度，2 MIN READ</description>
    </item>
    <item>
      <title>借助动态上下文并行和 NVIDIA Megatron Core 加速可变长度训练</title>
      <link>https://developer.nvidia.cn/blog/speeding-up-variable-length-training-with-dynamic-context-parallelism-and-nvidia-megatron-core/</link>
      <description>2026年 1月 28日借助动态上下文并行和 NVIDIA Megatron Core 加速可变长度训练本文介绍了应用于 NVIDIA Megatron Core 中的一种新型调度方法 — — 动态上下文并行 (Dynamic-CP) ，4 MIN READ</description>
    </item>
    <item>
      <title>更新视觉语言模型的分类器规避</title>
      <link>https://developer.nvidia.cn/blog/updating-classifier-evasion-for-vision-language-models/</link>
      <description>2026年 1月 28日更新视觉语言模型的分类器规避AI 架构的进步解锁了多模态功能，使 Transformer 模型能够在统一的上下文中处理多种类型的数据。例如，3 MIN READ</description>
    </item>
    <item>
      <title>借助 CUDA Tile IR 后端推进 OpenAI Triton 的 GPU 编程</title>
      <link>https://developer.nvidia.cn/blog/advancing-gpu-programming-with-the-cuda-tile-ir-backend-for-openai-triton/</link>
      <description>2026年 1月 30日借助 CUDA Tile IR 后端推进 OpenAI Triton 的 GPU 编程NVIDIA CUDA Tile 是基于 GPU 的编程模型，其设计目标是为 NVIDIA Tensor Cores 提供可移植性，2 MIN READ</description>
    </item>
    <item>
      <title>使用通用稀疏张量建立可扩展的稀疏生态系统</title>
      <link>https://developer.nvidia.cn/blog/establishing-a-scalable-sparse-ecosystem-with-the-universal-sparse-tensor/</link>
      <description>2026年 1月 30日使用通用稀疏张量建立可扩展的稀疏生态系统稀疏张量是向量、矩阵以及高维数组在包含大量零元素情况下的推广形式。由于其在存储、计算和功耗方面的高效性，稀疏张量在科学计算、5 MIN READ</description>
    </item>
    <item>
      <title>沙箱代理工作流与执行风险管理的实用安全指南</title>
      <link>https://developer.nvidia.cn/blog/practical-security-guidance-for-sandboxing-agentic-workflows-and-managing-execution-risk/</link>
      <description>2026年 1月 30日沙箱代理工作流与执行风险管理的实用安全指南AI 编码智能体通过简化任务和推动测试驱动的自动化开发，使开发者能够更高效地工作。然而，它们也引入了一个常被忽视的重要攻击面：1 MIN READ</description>
    </item>
    <item>
      <title>使用混合专家并行优化混合专家训练的通信</title>
      <link>https://developer.nvidia.cn/blog/optimizing-communication-for-mixture-of-experts-training-with-hybrid-expert-parallel/</link>
      <description>2026年 2月 2日使用混合专家并行优化混合专家训练的通信在 LLM 训练中，超大规模多专家模型 (MoE) 的专家并行 (EP) 通信面临巨大挑战。EP 通信本质上属于多对多模式，4 MIN READ</description>
    </item>
    <item>
      <title>基于 NVIDIA GPU 加速端点构建 Kimi K2.5 多模态视觉语言模型</title>
      <link>https://developer.nvidia.cn/blog/build-with-kimi-k2-5-multimodal-vlm-using-nvidia-gpu-accelerated-endpoints/</link>
      <description>2026年 2月 4日基于 NVIDIA GPU 加速端点构建 Kimi K2.5 多模态视觉语言模型Kimi K2.5 是 Kimi 模型家族最新推出的开放式视觉语言模型（VLM）。作为通用型多模态模型，Kimi K2.5…1 MIN READ</description>
    </item>
    <item>
      <title>如何使用 Nemotron 为 RAG 构建文档处理流程</title>
      <link>https://developer.nvidia.cn/blog/how-to-build-a-document-processing-pipeline-for-rag-with-nemotron/</link>
      <description>2026年 2月 4日如何使用 Nemotron 为 RAG 构建文档处理流程如果您的 AI 智能体能够像读取文本文件一样轻松地即时解析复杂的 PDF、提取嵌套表格并“查看”图表中的数据，该怎么办？3 MIN READ</description>
    </item>
    <item>
      <title>如何构建合规的 AI 模型蒸馏合成数据工作流</title>
      <link>https://developer.nvidia.cn/blog/how-to-build-license-compliant-synthetic-data-pipelines-for-ai-model-distillation/</link>
      <description>2026年 2月 5日如何构建合规的 AI 模型蒸馏合成数据工作流专用 AI 模型用于执行特定任务或解决特定问题。然而，如果您曾尝试对特定领域的模型进行微调或蒸馏，可能会遇到一些障碍，例如：4 MIN READ</description>
    </item>
    <item>
      <title>Painkiller RTX 如何通过生成式 AI 大规模重塑游戏资产</title>
      <link>https://developer.nvidia.cn/blog/how-painkiller-rtx-uses-generative-ai-to-modernize-game-assets-at-scale/</link>
      <description>2026年 2月 5日Painkiller RTX 如何通过生成式 AI 大规模重塑游戏资产Painkiller RTX为小型团队如何通过集成生成式 AI，在庞大的视觉目标与有限资源之间实现平衡，树立了新的标杆。2 MIN READ</description>
    </item>
    <item>
      <title>借助 NVIDIA TensorRT LLM AutoDeploy 实现推理优化自动化</title>
      <link>https://developer.nvidia.cn/blog/automating-inference-optimizations-with-nvidia-tensorrt-llm-autodeploy/</link>
      <description>2026年 2月 9日借助 NVIDIA TensorRT LLM AutoDeploy 实现推理优化自动化NVIDIA TensorRT LLM 使开发者能够为大语言模型 (LLM) 构建高性能推理引擎，但传统上部署新架构往往需要大量手动工作。3 MIN READ</description>
    </item>
  </channel>
</rss>
