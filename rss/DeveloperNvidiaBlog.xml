<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>DeveloperNvidiaBlog</title>
    <link>https://developer.nvidia.com/zh-cn/blog/recent-posts/</link>
    <description>Latest posts from https://developer.nvidia.com/zh-cn/blog/recent-posts/. follow.is: None</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Fri, 27 Feb 2026 02:27:08 +0000</lastBuildDate>
    <item>
      <title>使用混合专家并行优化混合专家训练的通信</title>
      <link>https://developer.nvidia.cn/blog/optimizing-communication-for-mixture-of-experts-training-with-hybrid-expert-parallel/</link>
      <description>2026年 2月 2日使用混合专家并行优化混合专家训练的通信在 LLM 训练中，超大规模多专家模型 (MoE) 的专家并行 (EP) 通信面临巨大挑战。EP 通信本质上属于多对多模式，4 MIN READ</description>
    </item>
    <item>
      <title>基于 NVIDIA GPU 加速端点构建 Kimi K2.5 多模态视觉语言模型</title>
      <link>https://developer.nvidia.cn/blog/build-with-kimi-k2-5-multimodal-vlm-using-nvidia-gpu-accelerated-endpoints/</link>
      <description>2026年 2月 4日基于 NVIDIA GPU 加速端点构建 Kimi K2.5 多模态视觉语言模型Kimi K2.5 是 Kimi 模型家族最新推出的开放式视觉语言模型（VLM）。作为通用型多模态模型，Kimi K2.5…1 MIN READ</description>
    </item>
    <item>
      <title>如何使用 Nemotron 为 RAG 构建文档处理流程</title>
      <link>https://developer.nvidia.cn/blog/how-to-build-a-document-processing-pipeline-for-rag-with-nemotron/</link>
      <description>2026年 2月 4日如何使用 Nemotron 为 RAG 构建文档处理流程如果您的 AI 智能体能够像读取文本文件一样轻松地即时解析复杂的 PDF、提取嵌套表格并“查看”图表中的数据，该怎么办？3 MIN READ</description>
    </item>
    <item>
      <title>如何构建合规的 AI 模型蒸馏合成数据工作流</title>
      <link>https://developer.nvidia.cn/blog/how-to-build-license-compliant-synthetic-data-pipelines-for-ai-model-distillation/</link>
      <description>2026年 2月 5日如何构建合规的 AI 模型蒸馏合成数据工作流专用 AI 模型用于执行特定任务或解决特定问题。然而，如果您曾尝试对特定领域的模型进行微调或蒸馏，可能会遇到一些障碍，例如：4 MIN READ</description>
    </item>
    <item>
      <title>Painkiller RTX 如何通过生成式 AI 大规模重塑游戏资产</title>
      <link>https://developer.nvidia.cn/blog/how-painkiller-rtx-uses-generative-ai-to-modernize-game-assets-at-scale/</link>
      <description>2026年 2月 5日Painkiller RTX 如何通过生成式 AI 大规模重塑游戏资产Painkiller RTX为小型团队如何通过集成生成式 AI，在庞大的视觉目标与有限资源之间实现平衡，树立了新的标杆。2 MIN READ</description>
    </item>
    <item>
      <title>NVFP4 加速 AI 训练与推理的三大方式</title>
      <link>https://developer.nvidia.cn/blog/3-ways-nvfp4-accelerates-ai-training-and-inference/</link>
      <description>2026年 2月 6日NVFP4 加速 AI 训练与推理的三大方式新兴的 AI 模型在规模和复杂性上持续增长，对训练和推理的计算性能需求日益提升，已远超摩尔定律所能满足的范畴。2 MIN READ</description>
    </item>
    <item>
      <title>借助 NVIDIA TensorRT LLM AutoDeploy 实现推理优化自动化</title>
      <link>https://developer.nvidia.cn/blog/automating-inference-optimizations-with-nvidia-tensorrt-llm-autodeploy/</link>
      <description>2026年 2月 9日借助 NVIDIA TensorRT LLM AutoDeploy 实现推理优化自动化NVIDIA TensorRT LLM 使开发者能够为大语言模型 (LLM) 构建高性能推理引擎，但传统上部署新架构往往需要大量手动工作。3 MIN READ</description>
    </item>
    <item>
      <title>R²D²：基于 NVIDIA Isaac Lab 拓展多模态机器人学习</title>
      <link>https://developer.nvidia.cn/blog/r2d2-scaling-multimodal-robot-learning-with-nvidia-isaac-lab/</link>
      <description>2026年 2月 10日R²D²：基于 NVIDIA Isaac Lab 拓展多模态机器人学习构建强大的智能机器人需要在复杂环境中进行测试。然而，在现实世界中收集数据不仅成本高昂、耗时漫长，还常常伴随高风险。3 MIN READ</description>
    </item>
    <item>
      <title>使用加速计算在大型研究设施中实时引导科学实验</title>
      <link>https://developer.nvidia.cn/blog/using-accelerated-computing-to-live-steer-scientific-experiments-at-massive-research-facilities/</link>
      <description>2026年 2月 10日使用加速计算在大型研究设施中实时引导科学实验设计和建造独特科学研究设施的科学家与工程师同样面临诸多挑战，其中包括处理超出当前计算基础设施承载能力的海量数据速率，3 MIN READ</description>
    </item>
    <item>
      <title>构建 AI 就绪型知识系统：掌握 5 种核心多模态 RAG 功能</title>
      <link>https://developer.nvidia.cn/blog/build-ai-ready-knowledge-systems-using-5-essential-multimodal-rag-capabilities/</link>
      <description>2026年 2月 17日构建 AI 就绪型知识系统：掌握 5 种核心多模态 RAG 功能企业数据本身具有高度复杂性：现实世界中的文档是多模态的，包含文本、表格、图表与图形、图像、扫描页面、表单以及嵌入式元数据。3 MIN READ</description>
    </item>
    <item>
      <title>在 NVIDIA Run:ai 中利用 GPU 解锁大规模 Token 吞吐能力</title>
      <link>https://developer.nvidia.cn/blog/unlock-massive-token-throughput-with-gpu-fractioning-in-nvidia-runai/</link>
      <description>2026年 2月 18日在 NVIDIA Run:ai 中利用 GPU 解锁大规模 Token 吞吐能力随着 AI 工作负载的扩展，实现高吞吐量、高效资源利用和可预测的延迟变得愈发关键。 NVIDIA Run:ai 通过智能调度和动态 GPU…4 MIN READ</description>
    </item>
    <item>
      <title>登顶 GPU 内核排行榜：借助 NVIDIA CUDA.compute 实现卓越性能</title>
      <link>https://developer.nvidia.cn/blog/topping-the-gpu-mode-kernel-leaderboard-with-nvidia-cuda-compute/</link>
      <description>2026年 2月 18日登顶 GPU 内核排行榜：借助 NVIDIA CUDA.compute 实现卓越性能Python 在符合人体工程学的机器学习领域占据主导地位，但编写真正高效的 GPU 代码历来需要使用 C++ 编写自定义内核，2 MIN READ</description>
    </item>
    <item>
      <title>NVIDIA 极致软硬件协同设计如何助力 Sarvam AI 主权模型实现惊人推理性能跃升</title>
      <link>https://developer.nvidia.cn/blog/how-nvidia-extreme-hardware-software-co-design-delivered-a-large-inference-boost-for-sarvam-ais-sovereign-models/</link>
      <description>2026年 2月 18日NVIDIA 极致软硬件协同设计如何助力 Sarvam AI 主权模型实现惊人推理性能跃升随着全球人工智能采用的加速，开发者面临日益严峻的挑战：如何提供符合现实世界延迟和成本要求的大语言模型（LLM）性能。4 MIN READ</description>
    </item>
    <item>
      <title>借助 NVIDIA 多实例 GPU 和 NUMA 节点定位加速数据处理</title>
      <link>https://developer.nvidia.cn/blog/accelerating-data-processing-with-nvidia-multi-instance-gpu-and-numa-node-localization/</link>
      <description>2026年 2月 19日借助 NVIDIA 多实例 GPU 和 NUMA 节点定位加速数据处理NVIDIA Ampere、NVIDIA Hopper 和 NVIDIA Blackwell 系列中的 NVIDIA 旗舰数据中心 GPU…3 MIN READ</description>
    </item>
    <item>
      <title>采用 NVFP4 低精度训练提升吞吐量，兼顾精度无损</title>
      <link>https://developer.nvidia.cn/blog/using-nvfp4-low-precision-model-training-for-higher-throughput-without-losing-accuracy/</link>
      <description>2026年 2月 23日采用 NVFP4 低精度训练提升吞吐量，兼顾精度无损随着 AI 模型和数据集规模的不断扩大，仅依赖更高精度的 BF16 训练已难以满足需求。训练吞吐量预期、内存限制以及成本上升等关键挑战，3 MIN READ</description>
    </item>
    <item>
      <title>借助 NVIDIA Blackwell Ultra 提升 Softmax 的效率</title>
      <link>https://developer.nvidia.cn/blog/making-softmax-more-efficient-with-nvidia-blackwell-ultra/</link>
      <description>2026年 2月 25日借助 NVIDIA Blackwell Ultra 提升 Softmax 的效率LLM 上下文长度呈爆炸式增长，架构正朝着更复杂的注意力机制发展，例如多头潜在注意力（MLA）和分组查询注意力（GQA）。因此，2 MIN READ</description>
    </item>
  </channel>
</rss>
