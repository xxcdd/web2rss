<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Microsoft Research</title>
	<atom:link href="https://www.microsoft.com/en-us/research/feed/" rel="self" type="application/rss+xml" />
	<link>https://www.microsoft.com/en-us/research/</link>
	<description></description>
	<lastBuildDate>Sat, 07 Dec 2024 00:48:07 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.7.1</generator>
	<item>
		<title>Abstracts: NeurIPS 2024 with Weizhu Chen</title>
		<link>https://www.microsoft.com/en-us/research/podcast/abstracts-neurips-2024-with-weizhu-chen/</link>
		
		<dc:creator><![CDATA[Brenda Potts]]></dc:creator>
		<pubDate>Sat, 07 Dec 2024 00:48:04 +0000</pubDate>
				<category><![CDATA[Microsoft Research Podcast]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1107414</guid>

					<description><![CDATA[<p>Next-token prediction trains a language model on all tokens in a sequence. VP Weizhu Chen discusses his team’s 2024 NeurIPS paper on how distinguishing between useful and “noisy” tokens in pretraining can improve token efficiency and model performance.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/podcast/abstracts-neurips-2024-with-weizhu-chen/">Abstracts: NeurIPS 2024 with Weizhu Chen</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img fetchpriority="high" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Weizhu-Chen_Abstracts_Hero_Feature_No_Text_1400x788.jpg" alt="Illustrated image of Weizhu Chen." class="wp-image-1108905" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Weizhu-Chen_Abstracts_Hero_Feature_No_Text_1400x788.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Weizhu-Chen_Abstracts_Hero_Feature_No_Text_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Weizhu-Chen_Abstracts_Hero_Feature_No_Text_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Weizhu-Chen_Abstracts_Hero_Feature_No_Text_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Weizhu-Chen_Abstracts_Hero_Feature_No_Text_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Weizhu-Chen_Abstracts_Hero_Feature_No_Text_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Weizhu-Chen_Abstracts_Hero_Feature_No_Text_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Weizhu-Chen_Abstracts_Hero_Feature_No_Text_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Weizhu-Chen_Abstracts_Hero_Feature_No_Text_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Weizhu-Chen_Abstracts_Hero_Feature_No_Text_1400x788-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure>


<div class="wp-block-msr-podcast-container my-4">
	<iframe src="https://player.blubrry.com/?podcast_id=139393524&modern=1" class="podcast-player" frameborder="0" height="164px" width="100%" scrolling="no" title="Podcast Player"></iframe>
</div>



<p>Members of the research community at Microsoft work continuously to advance their respective fields. <em>Abstracts </em>brings its audience to the cutting edge with them through short, compelling conversations about new and noteworthy achievements.</p>



<p>In this episode, <a href="https://www.microsoft.com/en-us/research/people/wzchen/" target="_blank" rel="noreferrer noopener">Weizhu Chen</a>, vice president of Microsoft GenAI, joins host Amber Tingle to discuss the paper <a href="https://www.microsoft.com/en-us/research/publication/not-all-tokens-are-what-you-need-for-pretraining/" target="_blank" rel="noreferrer noopener">“Not All Tokens Are What You Need for Pretraining,”</a> an oral presentation at this year’s <a href="https://www.microsoft.com/en-us/research/event/neurips-2024/?msockid=35739e94ab6c69d41b738b93aa076831" target="_blank" rel="noreferrer noopener">Conference on Neural Information Processing Systems (NeurIPS)</a>. Based on an examination of model training at the token level, Chen and his coauthors present an alternate approach to model pretraining: instead of training language models to predict <em>all</em> tokens, they make a distinction between useful and “noisy” tokens. Doing so, the work shows, improves token efficiency and model performance.</p>



<div class="wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex">
<div class="wp-block-button"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/not-all-tokens-are-what-you-need-for-pretraining/">Read the paper</a></div>



<div class="wp-block-button is-style-fill-github"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://github.com/microsoft/rho" target="_blank" rel="noreferrer noopener">Get the code</a></div>
</div>


<div class="wp-block-msr-show-more">
	<div class="bg-neutral-100 p-5">
		<div class="show-more-show-less" data-mount="show-more-show-less">
			<div>
				<span>
					

<h2 class="wp-block-heading" id="transcript">Transcript</h2>



<p>[MUSIC]</p>



<p><strong>AMBER TINGLE:</strong> Welcome to <em>Abstracts</em>, a Microsoft Research Podcast that puts the spotlight on world-class research <em>in brief</em>. I’m Amber Tingle. In this series, members of the research community at Microsoft give us a quick snapshot—or a <em>podcast abstract</em>—of their new and noteworthy papers.</p>



<p>[MUSIC FADES]&nbsp;</p>



<p>Our guest today is Weizhu Chen. He is vice president of Microsoft GenAI and coauthor of a paper called “Not All Tokens Are What You Need for Pretraining.” This paper is an oral presentation during the 38th annual Conference on Neural Information Processing Systems, also known as NeurIPS, which is happening this week in Vancouver. Weizhu, thank you for joining us today on <em>Abstracts</em>!&nbsp;</p>



				</span>
				<span id="show-more-show-less-toggle-1" class="show-more-show-less-toggleable-content">
					



<p><strong>WEIZHU CHEN:</strong> Thank you for having me, Amber.&nbsp;</p>



<p><strong>TINGLE: </strong>So let&#8217;s start with a brief overview of your paper. In a couple sentences, tell us about the problem your research addresses and, more importantly, why the research community and beyond should know about <em>this</em> work.&nbsp;</p>



<p><strong>CHEN:</strong> So my team basically in Microsoft GenAI, we are working on model training. So one of the things actually we do in the pretraining, we realize the importance of the data. And we found that actually when we do this kind of data for each of the tokens, some token is more important than the other. That&#8217;s one. The other one actually is some token actually is very, very hard to be predicted during the pretraining. So, for example, just like if someone see the text of “Weizhu,” and what&#8217;s the next token? It can be “Chen”; it can be any of the last name. So it&#8217;s very hard to be predicted. And if we try to enforce a language model to focus on this, kind of, the hard-to-predict token, just like actually it&#8217;s going to confuse the language model. There are so many different kinds of the example like this. Just like, for example, the serial number in your UPS. So the focus of this paper is try to identify which token actually is more important for the language model to learn. And actually the other token maybe is just the noise. And how can we try to <em>discriminate</em> the token—which is good token, which is noise token. Basically, you try to understand this kind of dynamic of the tokens. </p>



<p><strong>TINGLE: </strong>How did you conduct this research?&nbsp;</p>



<p><strong>CHEN: </strong>Actually we do a lot of work in the model training, including the pretraining <em>and</em> the post-training. So for the pretraining side, actually the most important thing to us is the data. We also try to understand, how can we leverage the existing data, and how can we create much more data, as well? And data basically is one of the most important thing to build a better foundation model. So we try to understand how much more we can get from the data. And the important thing for the data is about data filtering. So you think about actually in the previous literature, we do the data filtering, for example, just like we build a classifier to classify, OK, this page is more important than the other. And this page actually is a noise because there&#8217;s so much noise data in the web. So we just keep the best data to get into the pretraining corpus. And further away, we think about, OK, yeah, so this is … maybe it&#8217;s not fine grain enough, so can we try to understand even for the same page we want to keep? So some token is more important than the other. Maybe some token just some noise token. Actually you put this data into the pretraining, it&#8217;s going to hurt the model quality. So there is the motivation actually we try to think about.</p>



<p><strong>TINGLE: </strong>And what were your major findings?&nbsp;</p>



<p><strong>CHEN: </strong>Our major finding is about basically, definitely this works so well. And it&#8217;s so important that actually we are able to get the best token from the corpus and then make it available and try to ask the model during the pretraining to ignore the token we don&#8217;t want to get into the model itself. So that is one. The second thing definitely data is the other very important thing. If you&#8217;re able to figure out the better way to build a better data is most likely you’re able to build a much better foundation model. The third thing actually is also connected to a lot of other existing work, just like data synthesis, just like distillation, just like data filtering, and so a lot of things are really connected together. And actually, this work, basically, you can associate with also a lot of other work we are working on, just like distillation. You can think about, for example, for this work, we also try to build a model, a reference model—we call as the reference model—to try to identify actually this data, this token, is more important than the other and try to understand the discrepancy between the reference model and the running model, their prediction on each tokens. So you can think about also it&#8217;s some kind of the try to distill from the reference model to the existing model, as well. </p>



<p><strong>TINGLE:</strong> Let&#8217;s talk a little bit about real-world impact. Who benefits most from this work? And how significant is this within your discipline and even downstream for people using applications?&nbsp;</p>



<p><strong>CHEN:</strong> This actually is very, very fundamental work because just like I share a little bit before, actually we build the data and this data is—build the data much better—is able to build a much better foundation model. If we&#8217;re able to build a better model actually is able to benefit so many different kinds of application. This also is going to help us to build a much better small language model. And we can also serve this model even in the edge side, in the client side, in the coding scenario. So we are going to see actually huge impact from this kind of the foundation model if you are able to benefit from building much better training data.&nbsp;</p>



<p><strong>TINGLE: </strong>Are there any unanswered questions or unsolved problems in this area? What&#8217;s next on your research agenda?&nbsp;</p>



<p><strong>CHEN: </strong>Yeah, I think that is a very good questions. And definitely there&#8217;s a lot of things about how to build a better data [that] is unsolved yet in the literature. And especially because when you do the pretraining, the most important part is the data, but the data is very limited. And how can we make better use from the existing limited data is a big challenge. Because we can increase the model by 10x, but it’s super hard to increase the data by 10x, especially when we want to deal with the high quality of data. The other way, even given the data, how can you identify, especially for this work, the importance of each token to build a much better model? I think all these things are very connected together. To me, actually, data is the oxygen. So there are still so many things we are able to do in the data, including building for even the small language model or the large model.&nbsp;</p>



<p><strong>TINGLE: </strong>Data is oxygen—I love that! So other than <em>that</em> being a key takeaway, is there any other one thing that you&#8217;d like our listeners to walk away from this conversation knowing?&nbsp;</p>



<p><strong>CHEN: </strong>I would love to say actually focus more on this kind of data and focus more about how can I get more from the data actually; it is the very important thing. And the other thing actually, we are working on something that&#8217;s very exciting. You can feel free to come to join us if you are very interested in this area.&nbsp;</p>



<p>[MUSIC]&nbsp;</p>



<p><strong>TINGLE: </strong>Well, Weizhu Chen, thank you for joining us today. We really appreciate it.&nbsp;</p>



<p><strong>CHEN: </strong>Thank you. Thank you for having me.&nbsp;</p>



<p><strong>TINGLE: </strong>And thanks to our listeners for tuning in. If you’d like to read the full paper, you may find a link at aka.ms/abstracts. You can also find the paper on arXiv and on the NeurIPS conference website. I’m Amber Tingle from Microsoft Research, and we hope you’ll join us next time on <em>Abstracts</em>!&nbsp;</p>



<p>[MUSIC FADES]&nbsp;</p>

				</span>
			</div>
			<button
				class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle"
				aria-expanded="false"
				data-show-less-text="Show less"
				type="button"
				aria-controls="show-more-show-less-toggle-1"
				aria-label="Show more content"
				data-alternate-aria-label="Show less content">
				Show more			</button>
		</div>
	</div>
</div>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/podcast/abstracts-neurips-2024-with-weizhu-chen/">Abstracts: NeurIPS 2024 with Weizhu Chen</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Abstracts: NeurIPS 2024 with Dylan Foster</title>
		<link>https://www.microsoft.com/en-us/research/podcast/abstracts-neurips-2024-with-dylan-foster/</link>
		
		<dc:creator><![CDATA[Brenda Potts]]></dc:creator>
		<pubDate>Fri, 06 Dec 2024 14:00:00 +0000</pubDate>
				<category><![CDATA[Microsoft Research Podcast]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1107372</guid>

					<description><![CDATA[<p>Can existing algorithms designed for simple reinforcement learning problems be used to solve more complex RL problems? Researcher Dylan Foster discusses the modular approach he and his coauthors explored in their 2024 NeurIPS paper on RL under latent dynamics.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/podcast/abstracts-neurips-2024-with-dylan-foster/">Abstracts: NeurIPS 2024 with Dylan Foster</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Dylan-Foster_Abstracts_Hero_Feature_No_Text_1400x788.jpg" alt="Illustrated image of Dylan Foster for the Abstracts series on the Microsoft Research Podcast." class="wp-image-1108887" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Dylan-Foster_Abstracts_Hero_Feature_No_Text_1400x788.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Dylan-Foster_Abstracts_Hero_Feature_No_Text_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Dylan-Foster_Abstracts_Hero_Feature_No_Text_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Dylan-Foster_Abstracts_Hero_Feature_No_Text_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Dylan-Foster_Abstracts_Hero_Feature_No_Text_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Dylan-Foster_Abstracts_Hero_Feature_No_Text_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Dylan-Foster_Abstracts_Hero_Feature_No_Text_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Dylan-Foster_Abstracts_Hero_Feature_No_Text_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Dylan-Foster_Abstracts_Hero_Feature_No_Text_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Dylan-Foster_Abstracts_Hero_Feature_No_Text_1400x788-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure>


<div class="wp-block-msr-podcast-container my-4">
	<iframe loading="lazy" src="https://player.blubrry.com/?podcast_id=139303363&modern=1" class="podcast-player" frameborder="0" height="164px" width="100%" scrolling="no" title="Podcast Player"></iframe>
</div>



<p>Members of the research community at Microsoft work continuously to advance their respective fields.<em> Abstracts&nbsp;</em>brings its audience to the cutting edge with them through short, compelling conversations about new and noteworthy achievements.&nbsp;</p>



<p>In this episode, Principal Researcher <a href="https://www.microsoft.com/en-us/research/people/dylanfoster/">Dylan Foster</a> joins host Amber Tingle to discuss the paper <a href="https://www.microsoft.com/en-us/research/publication/reinforcement-learning-under-latent-dynamics-toward-statistical-and-algorithmic-modularity/">“Reinforcement Learning Under Latent Dynamics: Toward Statistical and Algorithmic Modularity,”</a> an oral presentation at this year’s <a href="https://www.microsoft.com/en-us/research/event/neurips-2024/">Conference on Neural Information Processing Systems</a> <a href="https://www.microsoft.com/en-us/research/event/neurips-2024/">(NeurIPS)</a>. In the paper, Foster and his coauthors explore whether well-studied RL algorithms for simple problems can be leveraged to solve RL problems with high-dimensional observations and latent dynamics, part of larger efforts to identify algorithm design principles that can enable agents to learn quickly via trial and error in unfamiliar environments.</p>



<div class="wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex">
<div class="wp-block-button"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/reinforcement-learning-under-latent-dynamics-toward-statistical-and-algorithmic-modularity/">Read the paper</a></div>
</div>


<div class="wp-block-msr-show-more">
	<div class="bg-neutral-100 p-5">
		<div class="show-more-show-less" data-mount="show-more-show-less">
			<div>
				<span>
					

<h2 class="wp-block-heading" id="transcript">Transcript</h2>



<p>[MUSIC]</p>



<p><strong>AMBER TINGLE:</strong> Welcome to <em>Abstracts</em>, a Microsoft Research Podcast that puts the spotlight on world-class research <em>in brief</em>. I’m Amber Tingle. In this series, members of the research community at Microsoft give us a quick snapshot—or a <em>podcast abstract</em>—of their new and noteworthy papers.</p>



<p>[MUSIC FADES]</p>



<p>Our guest today is Dylan Foster. He is a principal researcher at Microsoft Research and coauthor of a paper called “Reinforcement Learning Under Latent Dynamics: Toward Statistical and Algorithmic Modularity.” The work is among the oral presentations at this year&#8217;s Conference on Neural Information Processing Systems, or <em>NeurIPS</em>, in Vancouver. Dylan, welcome and thank you for joining us on the podcast!</p>



				</span>
				<span id="show-more-show-less-toggle-2" class="show-more-show-less-toggleable-content">
					



<p><strong>DYLAN FOSTER:</strong> Thanks for having me.</p>



<p><strong>TINGLE: </strong>Let&#8217;s start with a brief overview of this paper. Tell us about the problem this work addresses and why the research community should know about it.</p>



<p><strong>FOSTER:</strong> So this is a, kind of, a theoretical work on reinforcement learning, or RL. When I say reinforcement learning, broadly speaking, this is talking about the question of how can we design AI agents that are capable of, like, interacting with unknown environments and learning how to solve problems through trial and error. So this is part of some broader agenda we&#8217;ve been doing on, kind of, theoretical foundations of RL. And the key questions we&#8217;re looking at here are what are called, like, exploration and sample efficiency. So this just means we&#8217;re trying to understand, like, what are the algorithm design principles that can allow you to explore an unknown environment and learn as quickly as possible? What we&#8217;re doing in this paper is we&#8217;re, kind of, looking at, how can you most efficiently solve reinforcement learning problems where you&#8217;re faced with very high-dimensional observations, but the underlying dynamics of the system you&#8217;re interacting with are simple? So this is a setting that occurs in a lot of natural reinforcement learning and control problems, especially in the context of, like, say, embodied decision-making. So if you think about, say, games like Pong, you know, the state of the game, like, the state of, like, Pong, is extremely simple. It&#8217;s just, you know, what is the position and velocity of the ball, and, like, where are the paddles? But what we&#8217;d like to be able to do is learn to, you know, like, control or, like, solve games like this from raw pixels or, like, images kind of in the same way that a human would, like, just solve them from vision. So if you look at these types of problems, you know, we call these, like, <em>RL with rich observations</em> or RL with <em>latent dynamics</em>. You know, these are interesting because they, kind of, require you to explore the system, but they also require, you know, representation learning. Like, you want to be able to use neural nets to learn a mapping from, say, the images you see to the latent state of the system. This is a pretty interesting and nontrivial algorithmic problem. And, kind of, what we do in this work is we take a first step towards something like a unified understanding for how to solve these sorts of, like, rich-observation, or latent dynamics, RL problems.</p>



<p><strong>TINGLE:</strong> So how did you go about developing this theoretical framework?</p>



<p><strong>FOSTER: </strong>Yeah, so if you look at these sort of RL problems with latent dynamics, this is something that&#8217;s actually received a lot of investigation in theory. And a lot of this goes back to, kind of, early work from our lab from, like, 2016, 2017 or so. There&#8217;s some really interesting results here, but progress was largely on a, like, case-by-case basis, meaning, you know, there are many different ways that you can try to model the latent dynamics of your problem, and, you know, each of these somehow leads to a different algorithm, right. So, like, you know, you think very hard about this modeling assumption. You think about, what would an optimal algorithm look like? And you end up, you know, writing an entire paper about it. And there&#8217;s nothing wrong with that per se, but if you want to be able to iterate quickly and, kind of, try different modeling assumptions and see what works in practice, you know, this is not really tenable. It&#8217;s just too slow. And so the starting point for this work was to, kind of, try to take a different and more modular approach. So the idea is, you know, there are many, many different types of, sort of, systems or modeling assumptions for the dynamics that have been already studied extensively and have entire papers about them for the simpler setting in which you can directly see the state of the system. And so what we wanted to ask here is, is it possible to use these existing results in more of, like, a modular fashion? Like, if someone has already written a paper on how to optimally solve a particular type of MDP, or <em>Markov decision process</em>, can we just take their algorithm as is and perhaps plug it into some kind of meta-algorithm that can directly, kind of, combine this with representation learning and use it to solve the corresponding rich-observation, or latent dynamics, RL problem?</p>



<p><strong>TINGLE: </strong>What were your major findings? What did you learn during this process?</p>



<p><strong>FOSTER:</strong> We started by asking the question sort of exactly the way that I just posed it, right. Like, can we take existing algorithms and use them to solve rich-observation RL problems in a modular fashion? And this turned out to be really tricky. Like, there&#8217;s a lot of natural algorithms you might try that seem promising at first but don&#8217;t exactly work out. And what this, kind of, led us to and, sort of, the first main result in this paper is actually a <em>negative</em> result. So what we actually showed is most, sort of, well-studied types of systems or, like, MDPs that have been studied in, like, the prior literature on RL, even if they&#8217;re tractable when you&#8217;re able to directly see the state of the system, they can become statistically intractable once you add, sort of, high-dimensional observations to the picture. And statistically tractable here means the amount of interaction that you need, like the amount of, sort of, attempts to explore the system that you need, in order to learn a good decision-making policy becomes, like, very, very large, like much, much larger than the corresponding, sort of, complexity if you were able to directly see the states of the system. You know, you could look at this and say, I guess we&#8217;re out of luck. You know, maybe there&#8217;s just no hope of solving these sorts of problems. But that&#8217;s perhaps a little too pessimistic. You know, really the way you should interpret this result is just that you need <em>more assumptions</em>. And that&#8217;s precisely what the, sort of, second result we have in this paper is. So our second result shows that you can, sort of, bypass this impossibility result and, you know, achieve truly modular algorithms under a couple different types of additional assumptions.</p>



<p><strong>TINGLE: </strong>Dylan, I&#8217;d like to know—and I&#8217;m sure our audience would, too—what this work means when it comes to real-world application. What impact will this have on the research community?</p>



<p><strong>FOSTER:</strong> Yeah, so maybe I&#8217;ll answer that, um, with two different points. The first one is a broader point, which is, why is it important to understand this problem of exploration and sample efficiency in reinforcement learning? If you look at the, sort of, setting we study in this paper—you know, this, like, RL or decision-making with high-dimensional observations—on the empirical side, people have made a huge amount of progress on this problem through deep reinforcement learning. This was what kind of led to these amazing breakthroughs in solving games like Atari in the last decade. But if you look at these results, the gains are somehow more coming from the, like, inductive bias or the, like, generalization abilities of deep learning and not necessarily from the specific algorithms. So, like, current algorithms do not actually explore very deliberately, and so their sample efficiency is very high. Like, it&#8217;s hard to draw a one-to-one comparison, but you can argue they need, like, far more experience than a human would to solve these sorts of problems. So it&#8217;s not clear that we&#8217;re really anywhere near the ceiling of what can be achieved in terms of, like, how efficiently can you have, you know, an agent learn to solve new problems from trial and error. And I think better algorithms here could potentially be, like, transformative in a lot of different domains. To get into this specific work, I think there&#8217;s a couple of important takeaways for researchers. One is that by giving this impossibility result that shows that RL with latent dynamics is impossible without further assumptions, we&#8217;re kind of narrowing down the search space where other researchers can look for efficient algorithms. The second takeaway is, you know, we are showing that this problem becomes tractable when you make additional assumptions. But I view these more as, like, a proof of concept. Like, we&#8217;re kind of, showing for the first time that it <em>is</em> possible to do something nontrivial, but I think a lot more work and research will be required in order to like, you know, build on this and take this to something that can lead to, like, practical algorithms.<s></s></p>



<p><strong>TINGLE: </strong>Well, Dylan Foster, thank you for joining us today to discuss your paper on reinforcement learning under latent dynamics. We certainly appreciate it.</p>



<p><strong>FOSTER: </strong>Thanks a lot. Thanks for having me.</p>



<p>[MUSIC]</p>



<p><strong>TINGLE: </strong>And to our listeners, thank you all for tuning in. If you&#8217;d like to read Dylan&#8217;s paper, you may find a link at aka.ms/abstracts. You can also find the paper on arXiv and on the NeurIPS conference website. I&#8217;m Amber Tingle from Microsoft Research, and we hope you&#8217;ll join us next time on <em>Abstracts</em>!</p>



<p>[MUSIC FADES]</p>

				</span>
			</div>
			<button
				class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle"
				aria-expanded="false"
				data-show-less-text="Show less"
				type="button"
				aria-controls="show-more-show-less-toggle-2"
				aria-label="Show more content"
				data-alternate-aria-label="Show less content">
				Show more			</button>
		</div>
	</div>
</div>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/podcast/abstracts-neurips-2024-with-dylan-foster/">Abstracts: NeurIPS 2024 with Dylan Foster</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Abstracts: NeurIPS 2024 with Pranjal Chitale</title>
		<link>https://www.microsoft.com/en-us/research/podcast/abstracts-neurips-2024-with-pranjal-chitale/</link>
		
		<dc:creator><![CDATA[Brenda Potts]]></dc:creator>
		<pubDate>Fri, 06 Dec 2024 14:00:00 +0000</pubDate>
				<category><![CDATA[Microsoft Research Podcast]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1107426</guid>

					<description><![CDATA[<p>Pranjal Chitale discusses the ’24 NeurIPS work CVQA. Spanning 31 languages and the cultures of 30 countries, this VQA benchmark was created with native speakers and cultural experts to evaluate model performance across diverse linguistic and cultural contexts.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/podcast/abstracts-neurips-2024-with-pranjal-chitale/">Abstracts: NeurIPS 2024 with Pranjal Chitale</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Pranjal_Abstracts_Hero_Feature_No_Text_1400x788.jpg" alt="diagram" class="wp-image-1108932" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Pranjal_Abstracts_Hero_Feature_No_Text_1400x788.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Pranjal_Abstracts_Hero_Feature_No_Text_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Pranjal_Abstracts_Hero_Feature_No_Text_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Pranjal_Abstracts_Hero_Feature_No_Text_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Pranjal_Abstracts_Hero_Feature_No_Text_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Pranjal_Abstracts_Hero_Feature_No_Text_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Pranjal_Abstracts_Hero_Feature_No_Text_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Pranjal_Abstracts_Hero_Feature_No_Text_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Pranjal_Abstracts_Hero_Feature_No_Text_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Pranjal_Abstracts_Hero_Feature_No_Text_1400x788-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>


<div class="wp-block-msr-podcast-container my-4">
	<iframe loading="lazy" src="https://player.blubrry.com/?podcast_id=139311215&modern=1" class="podcast-player" frameborder="0" height="164px" width="100%" scrolling="no" title="Podcast Player"></iframe>
</div>



<p>Members of the research community at Microsoft work continuously to advance their respective fields.<em> Abstracts </em>brings its audience to the cutting edge with them through short, compelling conversations about new and noteworthy achievements. </p>



<p>In this episode, Research Fellow <a href="https://www.microsoft.com/en-us/research/people/t-pchitale/?msockid=35739e94ab6c69d41b738b93aa076831">Pranjal Chitale</a> joins host Gretchen Huizinga to discuss&nbsp;the paper <a href="https://www.microsoft.com/en-us/research/publication/cvqa-culturally-diverse-multilingual-visual-question-answering-benchmark/">“CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark,”</a> an oral presentation at this year’s <a href="https://www.microsoft.com/en-us/research/event/neurips-2024/">Conference on Neural Information Processing Systems (NeurIPS)</a>. CVQA, which comprises questions and images representative of 31 languages and the cultures of 30 countries, was created in collaboration with native speakers and cultural experts to evaluate how well models perform across diverse linguistic and cultural contexts, an important step toward improving model inclusivity.</p>



<div class="wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex">
<div class="wp-block-button"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/cvqa-culturally-diverse-multilingual-visual-question-answering-benchmark/">Read the paper</a></div>



<div class="wp-block-button is-style-fill-download"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://huggingface.co/datasets/afaji/cvqa" target="_blank" rel="noreferrer noopener">Get the dataset</a></div>
</div>


<div class="wp-block-msr-show-more">
	<div class="bg-neutral-100 p-5">
		<div class="show-more-show-less" data-mount="show-more-show-less">
			<div>
				<span>
					

<h2 class="wp-block-heading" id="transcript">Transcript</h2>



<p>[MUSIC]</p>



<p><strong>GRETCHEN HUIZINGA:</strong> Welcome to <em>Abstracts</em>, a Microsoft Research Podcast that puts the spotlight on world-class research in brief. I’m Gretchen Huizinga. In this series, members of the research community at Microsoft give us a quick snapshot—or a <em>podcast abstract</em>— of their new and noteworthy papers.</p>



<p>[MUSIC FADES]</p>



<p>Today I&#8217;m talking to Pranjal Chitale, a research fellow at Microsoft Research India. Pranjal is coauthor of a paper called “CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark,” and this paper is an oral presentation at this week&#8217;s 38th annual Conference on Neural Information Processing Systems, or NeurIPS, in Vancouver, BC. Pranjal, thanks for joining us today on <em>Abstracts</em>!<s></s></p>



				</span>
				<span id="show-more-show-less-toggle-3" class="show-more-show-less-toggleable-content">
					



<p><strong>PRANJAL CHITALE:</strong> Hi, Gretchen. Thanks for having me.</p>



<p><strong>HUIZINGA:</strong> So, Pranjal, give us an overview of this paper. In a couple sentences, what problem are you trying to solve, and why should people care about it?</p>



<p><strong>CHITALE:</strong> So we are witnessing some exciting times as LLMs are rapidly evolving as tools for countless use cases. While most of these LLMs were initially leveraged for natural language processing tasks, they are now expanded across languages and modalities. However, a major gap lies in the availability of multimodal data for non-English languages. Therefore, most multimodal models might not have coverage for non-English languages altogether or might just heavily rely on translations of the associated text in English-centric datasets so as to support multiple languages. The drawback of this approach is that it often misses the cultural nuances of local languages. And another reason why this is not optimal is the images are mostly Western-centric [and] therefore would not be well reflective of the local culture of a lot of regions. So this kind of bias can skew these models towards a Western perspective, raising concerns about inclusivity and safety of the content which they generate when serving a global population, which involves multicultural and multilingual users. Therefore, for a truly inclusive AI ecosystem, models must demonstrate cultural understanding to ensure that the generated content is safe, respectful for diverse communities. Evaluating cultural awareness, though, is extremely challenging because how to define culture itself is an unsolved problem. However, in this work, we are trying to take a step towards having a proxy which could measure cultural understanding.</p>



<p><strong>HUIZINGA:</strong> Well, talk about how you did this. What methodology did you use for this paper, and what were your major findings?</p>



<p><strong>CHITALE:</strong> Now that we have defined our broader problem, it is important to decide the scope of our solution because, as we discussed, culture is an umbrella term. So we need to define a smaller scope for this problem. We chose visual question answering, which is a multimodal task, and it is one of the most critical multimodal tasks for the scope of this work. So recognizing the limitations of existing VQA benchmarks, which often rely on translations and lack cultural representation, we developed CVQA, which is <em>Culturally-diverse multilingual VQA</em> benchmark. CVQA spans 30 countries, 31 languages, and has over 10,000 culturally nuanced questions, which were crafted by native speakers and cultural experts. So our focus was on creating questions which required what we term as <em>cultural common sense</em> to answer. For instance, with just the image, it is not possible to answer the question. You need some cultural awareness about the local culture to be able to answer the question. So these questions draw inspiration from knowledge of local culture. So one important aspect of this dataset is that we include both local language as well as English variants of the same question to allow robust testing of models across linguistic concepts. I would say the crux of this effort is that while most of the prior efforts may be small in terms of language—it could be language-group specific or country specific for most—but we wanted this to be a much larger global-scale collaborative effort. So this covers 31 languages across 30 countries. So to build CVQA, we worked with qualified volunteers from diverse age group and genders, ensuring that the questions authentically represented their cultures. So images which were collected, those were ensured to be copyright free, grounded in culture, and safe for work with strict guidelines to ensure that we avoid images which reflect some stereotypes or privacy violations. And we also had 10 categories, which involved topics ranging from daily life, sports, cuisine to history of the region, so a holistic view of the culture of the region. So each question was crafted as a multiple-choice task with challenging answer options which required both the image as well as cultural knowledge to solve. We also employed a maker-checker approach to ensure quality and consistency.</p>



<p><strong>HUIZINGA:</strong> So you&#8217;ve created the benchmark. You&#8217;ve tested it. What were your major findings?</p>



<p><strong>CHITALE:</strong> Now that we have created a benchmark, the next step is to evaluate how these multimodal models are performing on this benchmark. So we benchmark several state-of-the-art multimodal models, which include both open-source offerings like CLIP, BLIP, LLaVA-1.5, and proprietary offerings like GPT-4o or Gemini 1.5 Flash. So what we observed is there is a huge gap when it comes … in performance when we compare these proprietary offerings versus the open-source models. So GPT-4o was the highest-performing model with 75.4% accuracy on English prompts and 74.3% accuracy on local prompts. However, the story is completely different when we go to open-source models. These open-source models significantly lag behind the proprietary models. And one key finding over these open-source models is that these models perform even worse when prompted in the native language when we compare it to prompting in English. This potentially highlights that these models lack multilingual understanding capabilities, which may be because multilingual training data is pretty scarce.</p>



<p><strong>HUIZINGA:</strong> Yeah.</p>



<p><strong>CHITALE:</strong> So LLaVA-1.5 turned out to be the best open-source model. So one thing to notice, LLaVA-1.5 performs well across a large set of English VQA benchmarks, but when it comes to cultural understanding, it is a pretty weak model. Further, we also did some ablations to understand if adding location-specific information to the textual prompts has some impact or not, but we identified that it does not result in any significant performance improvements. Further, we also conducted a category-wise analysis. So, as we had mentioned, there are 10 categories to which these images belong. So what we observed is that certain categories, like people and everyday life, consistently saw higher accuracy across a large set of models. This may be likely due to abundance of human activity data in training datasets. However, when it comes to niche categories like cooking and food, pop culture, which are much more challenging, especially in local languages, these models struggle. Therefore, these are the kind of highly diverse cultural contexts which need improvement.</p>



<p><strong>HUIZINGA:</strong> How&#8217;s this work going to make an impact outside the lab and in the real world?</p>



<p><strong>CHITALE:</strong> CVQA is significant because it addresses a fundamental gap in how we evaluate vision-language and multimodal models today. While proprietary models are making impressive strides, open-source models, which are more accessible and easier to deploy, significantly lag behind in terms of cultural awareness and safety. So CVQA fills this gap and provides a much-needed benchmark to help us identify these gaps in the first place. So as to fix them, we first need to identify the gaps, and whether we are progressing or not can be captured by this benchmark. So for the real world, this benchmark does have some far-reaching implications. Models which understand culture are not just technically better, but they would create interactions which are far more engaging, natural, and safe for users from diverse backgrounds. So this benchmark offers entirely new axis for improvement, cultural awareness, and linguistic diversity. Therefore, by improving a model&#8217;s ability to handle culturally nuanced questions, CVQA ensures researchers and developers think beyond accuracy and also focus on cultural awareness and inclusivity before shipping these models into production.</p>



<p><strong>HUIZINGA:</strong> Pranjal, what are the unanswered questions or unsolved problems in this field, and what do you plan to do about it?</p>



<p><strong>CHITALE:</strong> So while CVQA makes some strides in addressing cultural and linguistic diversity, there is still much more to explore in this space. So this dataset only covers 31 languages and cultures, but this is just, like, a subset of the incredible diversity that exists globally. Many languages and cultures remain underrepresented, especially some of them are endangered or have limited digital resources. So expanding CVQA to include more of these languages would be a natural next step. Secondly, CVQA just focuses on single-turn question-answer pairs. But in reality, human interaction is often multi-turn and conversational in nature. So a multi-turn version of CVQA could better simulate real-world use cases and challenge models to maintain cultural and contextual awareness over extended dialogues. Another interesting area is personalization. So it would be very interesting if we could teach models to adapt to a user&#8217;s cultural background, preferences, or even regional nuances in real time. This remains a significant challenge, although this benchmark could help us move a step towards our broader goal.</p>



<p>[MUSIC]<s></s></p>



<p><strong>HUIZINGA:</strong> Well, Pranjal Chitale, this is super important research, and thank you for joining us today. To our listeners, thanks for tuning in. If you&#8217;re interested in learning more about this paper, you can find it at aka.ms/abstracts. You can also find it on arXiv and on the NeurIPS website. And if you&#8217;re at NeurIPS, you can also go hear about it. See you next time on <em>Abstracts</em>!</p>



<p>[MUSIC FADES]<s></s></p>

				</span>
			</div>
			<button
				class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle"
				aria-expanded="false"
				data-show-less-text="Show less"
				type="button"
				aria-controls="show-more-show-less-toggle-3"
				aria-label="Show more content"
				data-alternate-aria-label="Show less content">
				Show more			</button>
		</div>
	</div>
</div>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/podcast/abstracts-neurips-2024-with-pranjal-chitale/">Abstracts: NeurIPS 2024 with Pranjal Chitale</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Ideas: Economics and computation with Nicole Immorlica</title>
		<link>https://www.microsoft.com/en-us/research/podcast/ideas-economics-and-computation-with-nicole-immorlica/</link>
		
		<dc:creator><![CDATA[Brenda Potts]]></dc:creator>
		<pubDate>Thu, 05 Dec 2024 15:26:25 +0000</pubDate>
				<category><![CDATA[Microsoft Research Podcast]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1107621</guid>

					<description><![CDATA[<p>When research manager Nicole Immorlica discovered she could use math to make the world a better place for people, she was all in. She discusses working in computer science theory and economics, including studying the impact of algorithms and AI on markets. </p>
<p>The post <a href="https://www.microsoft.com/en-us/research/podcast/ideas-economics-and-computation-with-nicole-immorlica/">Ideas: Economics and computation with Nicole Immorlica</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1401" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Nicole_Immorlica_Ideas_Hero_Feature_No_Text_1400x788-1.jpg" alt="Line illustration of Nicole Immorlica" class="wp-image-1107630" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Nicole_Immorlica_Ideas_Hero_Feature_No_Text_1400x788-1.jpg 1401w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Nicole_Immorlica_Ideas_Hero_Feature_No_Text_1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Nicole_Immorlica_Ideas_Hero_Feature_No_Text_1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Nicole_Immorlica_Ideas_Hero_Feature_No_Text_1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Nicole_Immorlica_Ideas_Hero_Feature_No_Text_1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Nicole_Immorlica_Ideas_Hero_Feature_No_Text_1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Nicole_Immorlica_Ideas_Hero_Feature_No_Text_1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Nicole_Immorlica_Ideas_Hero_Feature_No_Text_1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Nicole_Immorlica_Ideas_Hero_Feature_No_Text_1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Nicole_Immorlica_Ideas_Hero_Feature_No_Text_1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1401px) 100vw, 1401px" /></figure>


<div class="wp-block-msr-podcast-container my-4">
	<iframe loading="lazy" src="https://player.blubrry.com/?podcast_id=139256805&modern=1" class="podcast-player" frameborder="0" height="164px" width="100%" scrolling="no" title="Podcast Player"></iframe>
</div>



<p>Behind every emerging technology is a great idea propelling it forward. In the Microsoft Research Podcast series <em>Ideas</em>, members of the research community at Microsoft discuss the beliefs that animate their research, the experiences and thinkers that inform it, and the positive human impact it targets.</p>



<p>In this episode, host Gretchen Huizinga talks with Senior Principal Research Manager <a href="https://www.microsoft.com/en-us/research/people/nicimm/">Nicole Immorlica</a>. As Immorlica describes it, when she and others decided to take a computational approach to pushing the boundaries of economic theory, there weren’t many computer scientists doing research in economics. Since then, contributions such as applying approximation algorithms to the classic economic challenge of pricing and work on the stable marriage problem have earned Immorlica numerous honors, including the 2023 Test of Time Award from the ACM Special Interest Group on Economics and Computation and selection as a 2023 Association for Computing Machinery (ACM) Fellow. Immorlica traces the journey back to a graduate market design course and a realization that captivated her: she could use her love of math to help improve the world through systems that empower individuals to make the best decisions possible for themselves.</p>



<div class="wp-block-group msr-pattern-link-list is-layout-flow wp-block-group-is-layout-flow">
<hr class="wp-block-separator has-alpha-channel-opacity"/>



<h2 class="wp-block-heading h5" id="learn-more-1">Learn more:</h2>



<ul class="wp-block-list list-unstyled">
<li><a href="https://www.microsoft.com/en-us/research/publication/impact-of-decentralized-learning-on-player-utilities-in-stackelberg-games/" target="_blank" rel="noreferrer noopener">Impact of Decentralized Learning on Player Utilities in Stackelberg Games</a>&nbsp;<br>Publication, June 2024&nbsp;</li>



<li><a href="https://www.microsoft.com/en-us/research/publication/the-disparate-effects-of-strategic-manipulation/" target="_blank" rel="noreferrer noopener">The Disparate Effects of Strategic Manipulation</a><br>Publication, January 2019 </li>



<li><a href="https://www.microsoft.com/en-us/research/publication/marriage-honesty-and-stability/">Marriage, honesty, and stability</a><br>Publication, January 2005</li>



<li><a href="https://www.microsoft.com/en-us/research/collaboration/ai-cognition-and-the-economy-aice/">AI, Cognition, and the Economy (AICE)</a><br>Project homepage</li>
</ul>
</div>



<section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast">
	<div class="subscribe-to-podcast__inner border-top border-bottom border-width-2">
		<h2 class="h5 subscribe-to-podcast__heading">
			Subscribe to the <a href="https://www.microsoft.com/en-us/research/podcast">Microsoft Research Podcast</a>:		</h2>
		<ul class="subscribe-to-podcast__list list-unstyled">
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://itunes.apple.com/us/podcast/microsoft-research-a-podcast/id1318021537?mt=2" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="black" viewBox="0 0 32 32">  <path d="M7.12 0c-3.937-0.011-7.131 3.183-7.12 7.12v17.76c-0.011 3.937 3.183 7.131 7.12 7.12h17.76c3.937 0.011 7.131-3.183 7.12-7.12v-17.76c0.011-3.937-3.183-7.131-7.12-7.12zM15.817 3.421c3.115 0 5.932 1.204 8.079 3.453 1.631 1.693 2.547 3.489 3.016 5.855 0.161 0.787 0.161 2.932 0.009 3.817-0.5 2.817-2.041 5.339-4.317 7.063-0.812 0.615-2.797 1.683-3.115 1.683-0.12 0-0.129-0.12-0.077-0.615 0.099-0.792 0.192-0.953 0.64-1.141 0.713-0.296 1.932-1.167 2.677-1.911 1.301-1.303 2.229-2.932 2.677-4.719 0.281-1.1 0.244-3.543-0.063-4.672-0.969-3.595-3.907-6.385-7.5-7.136-1.041-0.213-2.943-0.213-4 0-3.636 0.751-6.647 3.683-7.563 7.371-0.245 1.004-0.245 3.448 0 4.448 0.609 2.443 2.188 4.681 4.255 6.015 0.407 0.271 0.896 0.547 1.1 0.631 0.447 0.192 0.547 0.355 0.629 1.14 0.052 0.485 0.041 0.62-0.072 0.62-0.073 0-0.62-0.235-1.199-0.511l-0.052-0.041c-3.297-1.62-5.407-4.364-6.177-8.016-0.187-0.943-0.224-3.187-0.036-4.052 0.479-2.323 1.396-4.135 2.921-5.739 2.199-2.319 5.027-3.543 8.172-3.543zM16 7.172c0.541 0.005 1.068 0.052 1.473 0.14 3.715 0.828 6.344 4.543 5.833 8.229-0.203 1.489-0.713 2.709-1.619 3.844-0.448 0.573-1.537 1.532-1.729 1.532-0.032 0-0.063-0.365-0.063-0.803v-0.808l0.552-0.661c2.093-2.505 1.943-6.005-0.339-8.296-0.885-0.896-1.912-1.423-3.235-1.661-0.853-0.161-1.031-0.161-1.927-0.011-1.364 0.219-2.417 0.744-3.355 1.672-2.291 2.271-2.443 5.791-0.348 8.296l0.552 0.661v0.813c0 0.448-0.037 0.807-0.084 0.807-0.036 0-0.349-0.213-0.683-0.479l-0.047-0.016c-1.109-0.885-2.088-2.453-2.495-3.995-0.244-0.932-0.244-2.697 0.011-3.625 0.672-2.505 2.521-4.448 5.079-5.359 0.547-0.193 1.509-0.297 2.416-0.281zM15.823 11.156c0.417 0 0.828 0.084 1.131 0.24 0.645 0.339 1.183 0.989 1.385 1.677 0.62 2.104-1.609 3.948-3.631 3.005h-0.015c-0.953-0.443-1.464-1.276-1.475-2.36 0-0.979 0.541-1.828 1.484-2.328 0.297-0.156 0.709-0.235 1.125-0.235zM15.812 17.464c1.319-0.005 2.271 0.463 2.625 1.291 0.265 0.62 0.167 2.573-0.292 5.735-0.307 2.208-0.479 2.765-0.905 3.141-0.589 0.52-1.417 0.667-2.209 0.385h-0.004c-0.953-0.344-1.157-0.808-1.553-3.527-0.452-3.161-0.552-5.115-0.285-5.735 0.348-0.823 1.296-1.285 2.624-1.291z"/></svg>
						<span class="subscribe-to-podcast__link-text">Apple Podcasts</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribebyemail.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.4 6a2.392 2.392 0 00-2.372 2.119L16 15.6l11.972-7.481A2.392 2.392 0 0025.6 6H6.4zM4 10.502V22.8a2.4 2.4 0 002.4 2.4h19.2a2.4 2.4 0 002.4-2.4V10.502l-11.365 7.102a1.2 1.2 0 01-1.27 0L4 10.502z"/></svg>
						<span class="subscribe-to-podcast__link-text">Email</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribeonandroid.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M12.414 4.02c-.062.012-.126.023-.18.06a.489.489 0 00-.12.675L13.149 6.3c-1.6.847-2.792 2.255-3.18 3.944h13.257c-.388-1.69-1.58-3.097-3.179-3.944l1.035-1.545a.489.489 0 00-.12-.675.492.492 0 00-.675.135l-1.14 1.68a7.423 7.423 0 00-2.55-.45c-.899 0-1.758.161-2.549.45l-1.14-1.68a.482.482 0 00-.494-.195zm1.545 3.824a.72.72 0 110 1.44.72.72 0 010-1.44zm5.278 0a.719.719 0 110 1.44.719.719 0 110-1.44zM8.44 11.204A1.44 1.44 0 007 12.644v6.718c0 .795.645 1.44 1.44 1.44.168 0 .33-.036.48-.09v-9.418a1.406 1.406 0 00-.48-.09zm1.44 0V21.76c0 .793.646 1.44 1.44 1.44h10.557c.793 0 1.44-.647 1.44-1.44V11.204H9.878zm14.876 0c-.169 0-.33.035-.48.09v9.418c.15.052.311.09.48.09a1.44 1.44 0 001.44-1.44v-6.719a1.44 1.44 0 00-1.44-1.44zM11.8 24.16v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84zm5.759 0v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84z"/></svg>
						<span class="subscribe-to-podcast__link-text">Android</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://open.spotify.com/show/4ndjUXyL0hH1FXHgwIiTWU" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M16 4C9.383 4 4 9.383 4 16s5.383 12 12 12 12-5.383 12-12S22.617 4 16 4zm5.08 17.394a.781.781 0 01-1.086.217c-1.29-.86-3.477-1.434-5.303-1.434-1.937.002-3.389.477-3.403.482a.782.782 0 11-.494-1.484c.068-.023 1.71-.56 3.897-.562 1.826 0 4.365.492 6.171 1.696.36.24.457.725.217 1.085zm1.56-3.202a.895.895 0 01-1.234.286c-2.338-1.457-4.742-1.766-6.812-1.747-2.338.02-4.207.466-4.239.476a.895.895 0 11-.488-1.723c.145-.041 2.01-.5 4.564-.521 2.329-.02 5.23.318 7.923 1.995.419.26.547.814.286 1.234zm1.556-3.745a1.043 1.043 0 01-1.428.371c-2.725-1.6-6.039-1.94-8.339-1.942h-.033c-2.781 0-4.923.489-4.944.494a1.044 1.044 0 01-.474-2.031c.096-.023 2.385-.55 5.418-.55h.036c2.558.004 6.264.393 9.393 2.23.497.292.663.931.371 1.428z"/></svg>
						<span class="subscribe-to-podcast__link-text">Spotify</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.667 4a2.676 2.676 0 00-2.612 2.13v.003c-.036.172-.055.35-.055.534v18.666c0 .183.019.362.055.534v.003a2.676 2.676 0 002.076 2.075h.002c.172.036.35.055.534.055h18.666A2.676 2.676 0 0028 25.333V6.667a2.676 2.676 0 00-2.13-2.612h-.003A2.623 2.623 0 0025.333 4H6.667zM8 8h1.333C17.42 8 24 14.58 24 22.667V24h-2.667v-1.333c0-6.618-5.382-12-12-12H8V8zm0 5.333h1.333c5.146 0 9.334 4.188 9.334 9.334V24H16v-1.333A6.674 6.674 0 009.333 16H8v-2.667zM10 20a2 2 0 11-.001 4.001A2 2 0 0110 20z"/></svg>
						<span class="subscribe-to-podcast__link-text">RSS Feed</span>
					</a>
				</li>
					</ul>
	</div>
</section>


<div class="wp-block-msr-show-more">
	<div class="bg-neutral-100 p-5">
		<div class="show-more-show-less" data-mount="show-more-show-less">
			<div>
				<span>
					

<h2 class="wp-block-heading" id="transcript">Transcript</h2>



<p>[TEASER] </p>



<p>[MUSIC PLAYS UNDER DIALOGUE]</p>



<p><strong>NICOLE IMMORLICA</strong>: So honestly, when generative AI came out, I had a bit of a moment, a like crisis of confidence, so to speak, in the value of theory in my own work. And I decided to dive into a data-driven project, which was not my background at all. As a complete newbie, I was quite shocked by what I found, which is probably common knowledge among experts: data is <em>very</em> messy and <em>very</em> noisy, and it&#8217;s very hard to get any signal out of it. Theory is an essential counterpart to any data-driven research. It provides a guiding light. But even more importantly, theory allows us to illuminate things that have not even happened. So with models, we can hypothesize about possible futures and use that to shape what direction we take.</p>



<p>[TEASER ENDS]</p>



<p><strong>GRETCHEN HUIZINGA</strong>: You’re listening to <em>Ideas</em>, a Microsoft Research Podcast that dives deep into the world of technology research and the profound questions behind the code. I’m Gretchen Huizinga. In this series, we’ll explore the technologies that are shaping our future and the big ideas that propel them forward.</p>



<p>[MUSIC FADES]</p>



<p>My guest on this episode is Nicole Immorlica, a senior principal research manager at Microsoft Research New England, where she leads the Economics and Computation Group. Considered by many to be an expert on social networks, matching markets, and mechanism design, Nicole has a long list of accomplishments and honors to her name and some pretty cool new research besides. Nicole Immorlica, I&#8217;m excited to get into <em>all</em> the things with you today. Welcome to <em>Ideas</em>!&nbsp;</p>



				</span>
				<span id="show-more-show-less-toggle-4" class="show-more-show-less-toggleable-content">
					



<p><strong>NICOLE IMMORLICA:</strong> Thank you.&nbsp;</p>



<p><strong>HUIZINGA:</strong> So before we get into specifics on the big ideas behind your work, let&#8217;s find out a little bit about how and why you started doing it. Tell us your research origin story and, if there was one, what big idea or animating “what if” inspired young Nicole and launched a career in theoretical economics and computation research?&nbsp;</p>



<p><strong>IMMORLICA:</strong> So I took a rather circuitous route to my current research path. In high school, I thought I actually wanted to study physics, specifically cosmology, because I was super curious about the origins and evolution of the universe. In college, I realized on a day-to-day basis, what I really enjoyed was the math underlying physics, in particular proving theorems. So I changed my major to computer science, which was the closest thing to math that seemed to have a promising career path. [LAUGHTER] But when graduation came, I just wasn&#8217;t ready to be a grownup and enter the workforce! So I defaulted to graduate school thinking I&#8217;d continue my studies in theoretical computer science. It was in graduate school where I found my passion for the intersection of CS theory and micro-economics. I was just really enthralled with this idea that I could use the math that I so love to understand society and to help shape it in ways that improve the world for everyone in it.&nbsp;</p>



<p><strong>HUIZINGA:</strong> I&#8217;ve yet to meet an accomplished researcher who didn&#8217;t have at least one inspirational “who” behind the “what.” So tell us about the influential people in your life. Who are your heroes, economic or otherwise, and how did their ideas inspire yours and even inform your career?&nbsp;</p>



<p><strong>IMMORLICA:</strong> Yeah, of course. So when I was a graduate student at MIT, you know, I was happily enjoying my math, and just on a whim, I decided to take a course, along with a bunch of my other MIT graduate students, at Harvard from Professor Al Roth. And this was a market design course. We didn&#8217;t even really know what market design was, but in the context of that course, Al himself and the course content just demonstrated to me the transformative power of algorithms and economics. So, I mean, you might have heard of Al. He eventually won a Nobel Prize in economics for his work using a famous matching algorithm to optimize markets for doctors and separately for kidney exchange programs. And I thought to myself, wow, this is such meaningful work. This is something that I want to do, something I can contribute to the world, you know, something that my skill set is well adapted to. And so I just decided to move on with that, and I&#8217;ve never really looked back. It&#8217;s so satisfying to do something that&#8217;s both … I like both the means and I care very deeply about the ends.&nbsp;</p>



<p><strong>HUIZINGA:</strong> So, Nicole, you mentioned you took a course from Al Roth. Did he become anything more to you than that one sort of inspirational teacher? Did you have any interaction with him? And were there any other professors, authors, or people that inspired you in the coursework and graduate studies side of things?&nbsp;</p>



<p><strong>IMMORLICA:</strong> Yeah, I mean, Al has been transformative for my whole career. Like, I first met him in the context of that course, but I, and many of the graduate students in my area, have continued to work with him, speak to him at conferences, be influenced by him, so he&#8217;s been there throughout my career for me.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Right, right, right …&nbsp;</p>



<p><strong>IMMORLICA:</strong> In terms of other inspirations, I&#8217;ve really admired throughout my career … this is maybe more structurally how different individuals operate their careers. So, for example, Jennifer Chayes, who was the leader of the Microsoft Research lab that I joined …&nbsp;</p>



<p><strong>HUIZINGA:</strong> Yeah!&nbsp;</p>



<p><strong>IMMORLICA:</strong> … and nowadays Sue Dumais. Various other classic figures like Éva Tardos. Like, all of these are incredibly strong, driven women that have a vision of research, which has been transformative in their individual fields but also care very deeply about the community and the larger context than just themselves and creating spaces for people to really flourish. And I really admire that, as well.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Yeah, I&#8217;ve had both Sue and Jennifer on the show before, and they are amazing. Absolutely. Well, listen, Nicole, as an English major, I was thrilled—and a little surprised—to hear that literature has influenced your work in economics. I did not have that on my bingo card. Tell us about your interactions with literature and how they broadened your vision of optimization and economic models.</p>



<p><strong>IMMORLICA:</strong> Oh, I read a lot, especially fiction. And I care very deeply about being a broad human being, like, with a lot of different facets. And so I seek inspiration not just from my fellow economists and computer scientists but also from artists and writers. One specific example would be Walt Whitman. So I took up this poetry class as an MIT alumni, Walt Whitman, and we, in the context of that course, of course, read his famous poem “<em>Song of Myself</em>.” And I remember one specific verse just really struck me, where he writes, “Do I contradict myself? Very well then I contradict myself, (I am large, I contain multitudes.)” And this just was so powerful because, you know, in traditional economic models, we assume that individuals seek to optimize a single objective function, which we call their utility, but what Whitman is pointing out is that we actually have many different objective functions, which can even conflict with one another, and some at times are more salient than others, and they arise from my many identities as a member of my family, as an American, as you know, a computer scientist, as an economist, and maybe we should actually try to think a little bit more seriously about these multiple identities in the context of our modeling.&nbsp;</p>



<p><strong>HUIZINGA:</strong> That just warms my English major heart … [LAUGHS]&nbsp;</p>



<p><strong>IMMORLICA:</strong> I&#8217;m glad! [LAUGHS]&nbsp;</p>



<p><strong>HUIZINGA:</strong> Oh my gosh. And it&#8217;s so interesting because, yeah, we always think of, sort of, <em>singular</em> optimization. And so it&#8217;s like, how do we expand our horizon on that sort of optimization vision? So I love that. Well, you&#8217;ve received what I can only call a <em>flurry</em> of honors and awards last year. Most recently, you were named an ACM Fellow—ACM being Association for Computing Machinery, for those who don&#8217;t know—which acknowledges people who bring, and I quote, “transformative contributions to computing science and technology.” Now your citation is for, and I quote again, “contributions to economics and computation, including market design, auctions, and social networks.” That&#8217;s a mouthful, but if we&#8217;re talking about transformative contributions, how were things different before you brought your ideas to this field, and how were your contributions transformative or groundbreaking?&nbsp;</p>



<p><strong>IMMORLICA:</strong> Yeah, so it&#8217;s actually a relatively new thing for computer scientists to study economics, and I was among the first cohort to do so seriously. So before our time, economists mostly focused on finding optimal solutions to the problems they posed without regard for the computational or informational requirements therein. But computer scientists have an extensive toolkit to manage such complexities. So, for example, in a paper on pricing, which is a classic economic problem—how do we set up prices for goods in a store?—my coauthors and I used the computer science notion of approximation to show that a very simple menu of prices generates almost optimal revenue for the seller. And prior to this work, economists only knew how to characterize optimal but infinitely large and thereby impractical menus of prices. So this is an example of the kind of work that I and other computer scientists do that can really transform economics.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Right. Well, in addition to the ACM fellowship, another honor you received from ACM in 2023 was the Test of Time Award, where the Special Interest Group on Economics and Computation, or SIGecom, recognizes influential papers published between 10 and 25 years ago that significantly impacted research or applications in economics and computation. Now you got this award for a paper you cowrote in 2005 called “Marriage, Honesty, and Stability.” Clearly, I&#8217;m not an economist because I thought this was about how to avoid getting a divorce, but actually, it&#8217;s about a well-known and very difficult problem called the <em>stable marriage problem</em>. Tell us about this problem and the paper and why, as the award states, it’s stood the test of time.&nbsp;</p>



<p><strong>IMMORLICA:</strong> Sure. You&#8217;re not the only one to have misinterpreted the title. [LAUGHTER] I remember I gave a talk once and someone came and when they left the talk, they said, I did not think that this was about math! But, you know, math, as I learned, is about life, and the stable marriage problem has, you know, interpretation about marriage and divorce. In particular, the problem asks, how can we match market participants to one another such that no pair prefer each other to their assigned match? So to relate this to the somewhat outdated application of marriage markets, the market participants could be men and women, and the stable marriage problem asks if there is a set of marriages such that no pair of couples seeks a divorce in order to marry each other. And so, you know, that&#8217;s not really a problem we solve in real life, but there&#8217;s a lot of modern applications of this problem. For example, assigning medical students to hospitals for their residencies, or if you have children, many cities in the United States and around the world use this stable marriage problem to think about the assignment of K-to-12 students to public schools. And so in these applications, the stability property has been shown to contribute to the longevity of the market. And in the 1960s, David Gale and Lloyd Shapley proved, via an algorithm, interestingly, that stable matches exist! Well, in fact, there can be exponentially many stable matches. And so this leads to a very important question for people that want to apply this theory to practice, which is, which stable match should they select among the many ones that exist, and what algorithm should they use to select it? So our work shows that under very natural conditions, namely that preference lists are short and sufficiently random, it doesn&#8217;t matter. Most participants have a unique stable match. And so, you know, you can just design your market without worrying too much about what algorithm you use or which match you select because for most people it doesn&#8217;t matter. And since our paper, many researchers have followed up on our work studying conditions under which matchings are essentially unique and thereby influencing policy recommendations.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Hmm. So this work was clearly focused on the economics side of things like markets. So this seems to have wide application outside of economics. Is that accurate?&nbsp;</p>



<p><strong>IMMORLICA:</strong> Well, it depends how you define economics, so I would …&nbsp;</p>



<p><strong>HUIZINGA:</strong> I suppose! [LAUGHTER]&nbsp;</p>



<p><strong>IMMORLICA:</strong> I define economics as the problem … I mean, Al Roth, for example, wrote a book whose title was <em>Who Gets What—and Why.</em>&nbsp;</p>



<p><strong>HUIZINGA: </strong>Ooh.</p>



<p><strong>IMMORLICA: </strong>So economics is all about, how do we allocate stuff? How do we allocate scarce resources? And many economic problems are not about spending money. It&#8217;s about how do we create outcomes in the world.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Yeah.&nbsp;</p>



<p><strong>IMMORLICA:</strong> And so I would say all of these problem domains are economics.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Well, finally, as regards the “flurry” of honors, besides being named an ACM Fellow and also this Test of Time Award, you were also named an Economic Theory Fellow by the Society for [the] Advancement of Economic Theory, or SAET. And the primary qualification <em>here</em> was to have “substantially or creatively advanced theoretical economics.” So what were the big challenges you tackled, and what big ideas did you contribute to advance economic theory?&nbsp;</p>



<p><strong>IMMORLICA:</strong> So as we&#8217;ve discussed, I and others with my background have done a lot to advance economic theory through the lens of computational thinking.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Mmm &#8230;&nbsp;</p>



<p><strong>IMMORLICA:</strong> We&#8217;ve introduced ideas such as approximation, which we discussed earlier, or machine learning to economic models and proposing them as solution concepts. We&#8217;ve also used computer science tools to solve problems within these models. So two examples from my own work include randomized algorithm analysis and stochastic gradient descent. And importantly, we&#8217;ve introduced very relevant new <em>settings</em> to the field of economics. So, you know, I&#8217;ve worked hard on large-scale auction design and associated auto-bidding algorithms, for instance, which are a primary source of revenue for tech companies these days. I&#8217;ve thought a lot about how data enters into markets and how we should think about data in the context of market design. And lately, I&#8217;ve spent a lot of time thinking about generative AI and its impact in the economy at both the micro and macro levels.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Yeah. Let&#8217;s take a detour for a minute and get into the philosophical weeds on this idea of theory. And I want to cite an article that was written way back in 2008 by the editor of <em>Wired</em> magazine at the time, Chris Anderson. He wrote an article titled “The End of Theory,” which was provocative in itself. And he began by quoting the British statistician George Box, who famously said, “All models are wrong, but some are useful.” And then he argued that in an era of massively abundant data, companies didn&#8217;t have to settle for wrong models. And then he went even further and attacked the very idea of theory and, citing Google, he said, “Out with every theory of human behavior, from linguistics to sociology. Forget taxonomy, ontology, psychology. Who knows why people do what they do? The point is they do it, and we can track and measure it with unprecedented fidelity.” So, Nicole, from your perch, 15 years later, in the age of generative AI, what did Chris Anderson get right, and what did he get wrong?&nbsp;</p>



<p><strong>IMMORLICA:</strong> So, honestly, when generative AI came out, I had a bit of a moment, a like crisis of confidence, so to speak, in the value of theory in my own work.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Really!&nbsp;</p>



<p><strong>IMMORLICA:</strong> And I decided to dive into a data-driven project, which was not my background <em>at all</em>. As a complete newbie, I was quite shocked by what I found, which is probably common knowledge among experts: data is <em>very</em> messy and <em>very</em> noisy, and it&#8217;s very hard to get any signal out of it. Theory is an essential counterpart to any data-driven research. It provides a guiding light. But even more importantly, theory allows us to illuminate things that have not even happened. So with models, we can hypothesize about possible futures and use that to shape what direction we take. Relatedly, what I think that article got most wrong was the statement that correlation supersedes causation, which is actually how the article closes, this idea that causation is dead or dying. I think causation will never become irrelevant. Causation is what allows us to reason about counterfactuals. It&#8217;s fundamentally irreplaceable. It&#8217;s like, you know, data, you can only see data about things that happened. You can&#8217;t see data about things that <em>could</em> happen but haven&#8217;t or, you know, about alternative futures.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Interesting.&nbsp;</p>



<p><strong>IMMORLICA:</strong> And that&#8217;s what theory gives you.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Yeah. Well, let&#8217;s continue on that a little bit because this show is yet another part of our short “series within a series” featuring some of the work going on in the AI, Cognition, and the Economy initiative at Microsoft Research. And I just did an episode with Brendan Lucier and Mert Demirer on the micro- and macro-economic impact of generative AI. And you were part of that project, but another fascinating project you&#8217;re involved in right now looks at the impact of generative AI on what you call the “content ecosystem.” So what&#8217;s the problem behind this research, and what unique incentive challenges are content creators facing in light of large language and multimodal AI models?&nbsp;</p>



<p><strong>IMMORLICA:</strong> Yeah, so this is a project with Brendan, as well, whom you interviewed previously, and also Nageeb Ali, an economist and AICE Fellow at Penn State, and Meena Jagadeesan, who was my intern from Microsoft Research from UC Berkeley. So when you think about content or really any consumption good, there&#8217;s often a whole supply chain that produces it. For music, for example, there&#8217;s the composition of the song, the recording, the mixing, and finally the delivery to the consumer. And all of these steps involve multiple humans producing things, generating things, getting paid along the way. One way to think about generative AI is that it allows the consumer to bypass this supply chain and just generate the content directly.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Right …&nbsp;</p>



<p><strong>IMMORLICA:</strong> So, for example, like, I could ask a model, an AI model, to compose and play a song about my cat named Whiskey. [LAUGHTER] And it would do a decent job of it, and it would tailor the song to my specific situation. But there are drawbacks, as well. One thing many researchers fear is that AI needs human-generated content to train. And so if people start bypassing the supply chain and just using AI-generated content, there won&#8217;t be any content for AI to train on and AI will cease to improve.</p>



<p><strong>HUIZINGA:</strong> Right.&nbsp;</p>



<p><strong>IMMORLICA:</strong> Another thing that could be troubling is that there are economies of scale. So there is a nontrivial cost to producing music, even for AI, and if we share that cost among many listeners, it becomes more affordable. But if we each access the content ourselves, it&#8217;s going to impose a large per-song cost. And then finally, and this is, I think, most salient to most people, there&#8217;s some kind of social benefit to having songs that everyone listens to. It provides a common ground for understanding. It&#8217;s a pillar of our culture, right. And so if we bypass that, aren&#8217;t we losing something? So for all of these reasons, it becomes very important to understand the market conditions under which people will choose to bypass supply chains and the associated costs and benefits of this. What we show in this work, which is very much work in progress, is that when AI is very costly, neither producers nor consumers will use it, but as it gets cheaper, at first, it actually helps content producers that can leverage it to augment their own ability, creating higher-quality content, more personalized content more cheaply. But then, as the AI gets super cheap, this bypassing behavior starts to emerge, and the content creators are driven out of the market.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Right. So what do we do about that?&nbsp;</p>



<p><strong>IMMORLICA:</strong> Well, you know, you have to take a stance on whether that&#8217;s even a good thing or a bad thing, …&nbsp;</p>



<p><strong>HUIZINGA:</strong> Right!&nbsp;</p>



<p><strong>IMMORLICA:</strong> … so it could be that we do nothing about it. We could also impose a sort of minimum wage on AI, if you like, to artificially inflate its costs. We could try to amplify the parts of the system that lead towards more human-generated content, like this sociability, the fact that we all are listening to the same stuff. We could try to make that more salient for people. But, you know, generally speaking, I&#8217;m not really in a place to take a stance on whether this is a good thing or a bad thing. I think this is for policymakers.&nbsp;</p>



<p><strong>HUIZINGA:</strong> It feels like we&#8217;re at an inflection point. I&#8217;m really interested to see what your research in <em>this</em> arena, the content ecosystem, brings. You know, I&#8217;ll mention, too, recently I read a blog written by Yoshua Bengio and Vincent Conitzer, and they acknowledged that the image that they used at the top had been created by an AI bot. And then they said they made a donation to an art museum to say, we&#8217;re giving something back to the artistic community that we may have used. Where do you see this, you know, #NoLLM situation coming in this content ecosystem market?&nbsp;</p>



<p><strong>IMMORLICA:</strong> Yeah, that&#8217;s a very interesting move on their part. I know Vince quite well, actually. I&#8217;m not sure that artists of the sort of “art museum nature” suffer, so …&nbsp;</p>



<p><strong>HUIZINGA:</strong> Right? [LAUGHS]&nbsp;</p>



<p><strong>IMMORLICA:</strong> One of my favorite artists is Laurie Anderson. I don&#8217;t know if you&#8217;ve seen her work at all …&nbsp;</p>



<p><strong>HUIZINGA:</strong> Yeah, I have, yeah.&nbsp;</p>



<p><strong>IMMORLICA:</strong> … but she has a piece in the MASS MoCA right now, which is just brilliant, where she actually uses generative AI to create a sequence of images that creates an alternate story about her family history. And it&#8217;s just really, really cool. I&#8217;m more worried about people who are doing art vocationally, and I think, and maybe you heard some of this from Mert and Brendan, like what&#8217;s going to happen is that careers are going to shift and different vocations will become more salient, and we&#8217;ve seen this through every technological revolution. People shift their work towards the things that are uniquely human that we can provide and if generating an image at the top of a blog is not one of them, you know, so be it. People will do something else.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Right, right, right. Yeah, I just … we&#8217;re on the cusp, and there&#8217;s a lot of things that are going to happen in the next couple of years, maybe a couple of months, who knows? [LAUGHTER] Well, we hear a lot of dystopian fears—some of them we&#8217;ve just referred to—around AI and its impact on humanity, but those fears are often dismissed by tech optimists as what I might call “unwishful thinking.” So your research interests involve the design and use of sociotechnical systems to quote, “explain, predict, and shape behavioral patterns in various online and offline systems, markets, and games.” Now I&#8217;m with you on the “explain and predict” but when we get to shaping behavioral patterns, I wonder how we tease out the bad from the good. So, in light of the power of these sociotechnical systems, what could possibly go wrong, Nicole, if in fact you got everything right?&nbsp;</p>



<p><strong>IMMORLICA:</strong> Yeah, first I should clarify something. When I say I&#8217;m interested in <em>shaping</em> behavioral patterns, I don&#8217;t mean that I want to impose particular behaviors on people but rather that I want to design systems that expose to people relevant information and possible actions so that <em>they</em> have the power to shape their own behavior to achieve their own goals. And if we&#8217;re able to do that, and do it really well, then things can only really go wrong if you believe people aren&#8217;t good at making themselves happy. I mean, there&#8217;s certainly evidence of this, like the field of behavioral economics, to which I have contributed some, tries to understand how and when people make mistakes in their behavioral choices. And it proposes ways to help people mitigate these mistakes. But I caution us from going too far in this direction because at the end of the day, I believe people know things about themselves that no external authority can know. And you don&#8217;t want to impose constraints that prevent people from acting on that information.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Yeah.&nbsp;</p>



<p><strong>IMMORLICA:</strong> Another issue here is, of course, externalities. It could be that my behavior makes me happy but makes you unhappy. [LAUGHTER] So another thing that can go wrong is that we, as designers of technology, fail to capture these underlying externalities. I mean, ideally, like an economist would say, well, you should pay with your own happiness for any negative externality you impose on others. And the fields of market and mechanism design have identified very beautiful ways of making this happen automatically in simple settings, such as the famous Vickrey auction. But getting this right in the complex sociotechnical systems of our day is quite a challenge.&nbsp;</p>



<p><strong>HUIZINGA:</strong> OK, go back to that auction. What did you call it? The Vickrey auction?&nbsp;</p>



<p><strong>IMMORLICA:</strong> Yeah, so Vickrey was an economist, and he proposed an auction format that … so an auction is trying to find a way to allocate goods, let&#8217;s say, to bidders such that the bidders that value the goods the most are the ones that win them.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Hm.&nbsp;</p>



<p><strong>IMMORLICA:</strong> But of course, these bidders are imposing a negative externality on the people who lose, right? [LAUGHTER] And so what Vickrey showed is that a well-designed system of prices can compensate the losers exactly for the externality that is imposed on them. A very simple example of a Vickrey auction is if you&#8217;re selling just one good, like a painting, then what you should do, according to Vickrey, is solicit bids, give it to the highest bidder, and charge them the second-highest price.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Interesting …&nbsp;</p>



<p><strong>IMMORLICA:</strong> And so &#8230; that&#8217;s going to have good outcomes for society.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Yeah, yeah. I want to expand on a couple of thoughts here. One is as you started out to answer this question, you said, well, I&#8217;m not interested in shaping behaviors in terms of making you do what I want you to do. But maybe someone else is. What happens if it falls into the wrong hands?&nbsp;</p>



<p><strong>IMMORLICA:</strong> Yeah, I mean, there&#8217;s definitely competing interests. Everybody has their own objectives, and …&nbsp;</p>



<p><strong>HUIZINGA:</strong> Sure, sure.&nbsp;</p>



<p><strong>IMMORLICA:</strong> … I might be very fundamentally opposed to some of them, but everybody&#8217;s trying to optimize something, and there are competing optimization objectives. And so what&#8217;s going to happen if people are leveraging this technology to optimize for themselves and thereby harming me a lot?&nbsp;</p>



<p><strong>HUIZINGA:</strong> Right?&nbsp;</p>



<p><strong>IMMORLICA:</strong> Ideally, we&#8217;ll have regulation to kind of cover that. I think what I&#8217;m more worried about is the idea that the technology itself might not be aligned with me, right. Like at the end of the day, there are companies that are producing this technology that I&#8217;m then using to achieve my objectives, but the company&#8217;s objectives, the creators of the technology, might not be completely aligned with the person&#8217;s objectives. And so I&#8217;ve looked a little bit in my research about how this potential misalignment might result in outcomes that are not all that great for either party.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Wow. Is that stuff that&#8217;s in the works?&nbsp;</p>



<p><strong>IMMORLICA:</strong> We have a few published papers on the area. I don&#8217;t know if you want me to get into them.&nbsp;</p>



<p><strong>HUIZINGA:</strong> No, actually, what we&#8217;ll probably do is put some in the show notes. We&#8217;ll link people to those papers because I think that&#8217;s an interesting topic. Listen, most research is incremental in nature, where the ideas are basically iterative steps on existing work. But sometimes there are out-of-the-box ideas that feel like bigger swings or even outrageous, and Microsoft is well known for making room for these. Have you had an idea that felt outrageous, any idea that felt outrageous, or is there anything that you might even consider outrageous now that you&#8217;re currently working on or even thinking about?&nbsp;</p>



<p><strong>IMMORLICA:</strong> Yeah, well, I mean, this whole moment in history feels outrageous, honestly! [LAUGHTER] It&#8217;s like I&#8217;m kind of living in the sci-fi novels of my youth.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Right?&nbsp;</p>



<p><strong>IMMORLICA:</strong> So together with my economics and social science colleagues at Microsoft Research, one thing that we&#8217;re really trying to think through is this outrageous idea of <em>agentic AI</em>.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Mmm &#8230;&nbsp;</p>



<p><strong>IMMORLICA:</strong> That is, every single individual and business can have their own AI that acts like their own personal butler that knows them intimately and can take actions on their behalf. In such a world, what will become of the internet, social media, platforms like Amazon, Spotify, Uber? On the one hand, you know, maybe this is good because these individual agentic AIs can just bypass all of these kinds of intermediaries. For example, if I have a busy day of back-to-back meetings at work, my personal AI can notice that I have no time for lunch, contact the AI of some restaurant to order a sandwich for me, make sure that sandwich is tailored to my dietary needs and preferences, and then contact the AI of a delivery service to make sure that sandwich is sitting on my desk when I walk into my noon meeting, right.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Right &#8230;&nbsp;</p>



<p><strong>IMMORLICA:</strong> And this is a huge disruption to how things currently work. It&#8217;s shifting the power away from centralized platforms, back to individuals and giving them the agency over their data and the power to leverage it to fulfill their needs. So the, sort of, big questions that we&#8217;re thinking about right now is, how will such decentralized markets work? How will they be monetized? Will it be a better world than the one we live in now, or are we losing something? And if it is a better world, how can we get from here to there? And if it&#8217;s a worse world, how can we steer the ship in the other direction, you know?&nbsp;</p>



<p><strong>HUIZINGA:</strong> Right.&nbsp;</p>



<p><strong>IMMORLICA:</strong> These are all very important questions in this time.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Does this feel like it&#8217;s imminent?&nbsp;</p>



<p><strong>IMMORLICA:</strong> I do think it&#8217;s imminent. And I think, you know, in life, you can, kind of, decide whether to embrace the good or embrace the bad, see the glass as half-full or half-empty, and …&nbsp;</p>



<p><strong>HUIZINGA:</strong> Yeah.&nbsp;</p>



<p><strong>IMMORLICA:</strong> … I am hoping that society will see the half-full side of these amazing technologies and leverage them to do really great things in the world.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Man, I would love to talk to you for another hour, but we have to close things up. To close <em>this</em> show, I want to do something new with you, a sort of lightning round of short questions with short answers that give us a little window into your life. So are you ready?&nbsp;</p>



<p><strong>IMMORLICA:</strong> Yup!&nbsp;</p>



<p><strong>HUIZINGA:</strong> OK. First one, what are you reading right now for work?&nbsp;</p>



<p><strong>IMMORLICA:</strong> Lots of papers of my students that are on the job market to help prepare recommendation letters. It&#8217;s actually very inspiring to see the creativity of the younger generation. In terms of books, I&#8217;m reading the <em>Idea Factory</em>, which is about the creation of Bell Labs.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Ooh! Interesting!&nbsp;</p>



<p><strong>IMMORLICA:</strong> You might be interested in it actually. It talks about the value of theory and understanding the fundamentals of a problem space and the sort of business value of that, so it&#8217;s very intriguing.&nbsp;</p>



<p><strong>HUIZINGA:</strong> OK, second question. What are you reading for pleasure?&nbsp;</p>



<p><strong>IMMORLICA:</strong> The book on my nightstand right now is the <em>Epic of Gilgamesh</em>, the graphic novel version. I&#8217;m actually quite enthralled by graphic novels ever since I first encountered <em>Maus</em> by Art Spiegelman in the ’90s. But my favorite reading leans towards magic realism, so like Gabriel García Márquez, Italo Calvino, Isabel Allende, and the like. I try to read nonfiction for pleasure, too, but I generally find life is a bit too short for that genre! [LAUGHTER]&nbsp;</p>



<p><strong>HUIZINGA:</strong> Well, and I made an assumption that what you were reading for work wasn&#8217;t pleasurable, but um, moving on, question number three, what app doesn&#8217;t exist but should?&nbsp;</p>



<p><strong>IMMORLICA:</strong> Teleportation.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Ooh, fascinating. What app exists but shouldn&#8217;t?&nbsp;</p>



<p><strong>IMMORLICA:</strong> That&#8217;s much harder for me. I think all apps within legal bounds should be allowed to exist and the free market should decide which ones survive. Should there be more regulation of apps? Perhaps. But more at the level of giving people tools to manage their consumption at their own discretion and not outlawing specific apps; that just feels too paternalistic to me.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Interesting. OK, next question. What&#8217;s one thing that used to be very important to you but isn&#8217;t so much anymore?&nbsp;</p>



<p><strong>IMMORLICA:</strong> Freedom. So by that I mean the freedom to do whatever I want, whenever I want, with whomever I want. This feeling that I could go anywhere at any time without any preparation, that I could be the Paul Erdős of the 21st century, traveling from city to city, living out of a suitcase, doing beautiful math just for the art of it. This feeling that I have no responsibilities. Like, I really bought into that in my 20s.&nbsp;</p>



<p><strong>HUIZINGA:</strong> And not so much now?&nbsp;</p>



<p><strong>IMMORLICA:</strong> No.&nbsp;</p>



<p><strong>HUIZINGA:</strong> OK, so what&#8217;s one thing that wasn&#8217;t very important to you but is now?&nbsp;</p>



<p><strong>IMMORLICA:</strong> Now, as Janis Joplin sang, “Freedom is just another word for nothing left to lose.” [LAUGHTER] And so now it&#8217;s important to me to have things to lose—roots, family, friends, pets. I think this is really what gives my life meaning.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Yeah, having Janis Joplin cited in this podcast wasn&#8217;t on my bingo card either, but that&#8217;s great. Well, finally, Nicole, I want to ask you this question based on something we talked about before. Our audience doesn’t know it, but I think it’s funny. What do Norah Jones and oatmeal have in common for you?&nbsp;</p>



<p><strong>IMMORLICA:</strong> Yeah, so I use these in conversation as examples of comfort and nostalgia in the categories of music and food because I think they&#8217;re well-known examples. But for me personally, comfort is the Brahms Cello Sonata in E Minor, which was in fact my high school cello performance piece, and nostalgia is spaghetti with homemade marinara sauce, either my boyfriend&#8217;s version or, in my childhood, my Italian grandma&#8217;s version.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Man! Poetry, art, cooking, music &#8230; who would have expected all of these to come into an economist/computer scientist podcast on the Microsoft Research Podcast. Nicole Immorlica, how fun to have you on the show! Thanks for joining us today on <em>Ideas</em>!&nbsp;</p>



<p><strong>IMMORLICA:</strong> Thank you for having me.&nbsp;</p>



<p>[MUSIC]&nbsp;</p>

				</span>
			</div>
			<button
				class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle"
				aria-expanded="false"
				data-show-less-text="Show less"
				type="button"
				aria-controls="show-more-show-less-toggle-4"
				aria-label="Show more content"
				data-alternate-aria-label="Show less content">
				Show more			</button>
		</div>
	</div>
</div>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/podcast/ideas-economics-and-computation-with-nicole-immorlica/">Ideas: Economics and computation with Nicole Immorlica</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Research Focus: Week of December 2, 2024</title>
		<link>https://www.microsoft.com/en-us/research/blog/research-focus-week-of-december-2-2024/</link>
		
		<dc:creator><![CDATA[Brenda Potts]]></dc:creator>
		<pubDate>Wed, 04 Dec 2024 17:07:27 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1106934</guid>

					<description><![CDATA[<p>Can a new SOS-RMT protocol enable more efficient CL-MPC?; A fair-by-design, cloud-based algorithmic trading platform; LLM2CLIP unlocks richer visual representation; New technique enhances Low-Rank Adaptation’s expressiveness, generalization capabilities.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-december-2-2024/">Research Focus: Week of December 2, 2024</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-pullquote"><blockquote><p>Welcome to Research Focus, a series of blog posts that highlights notable publications, events, code/datasets, new hires and other milestones from across the research community at Microsoft.</p></blockquote></figure>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/RF54-BlogHeroFeature-1400x788-1.jpg" alt="Research Focus: Week of December 2, 2024" class="wp-image-1106946" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/RF54-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/RF54-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/RF54-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/RF54-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/RF54-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/RF54-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/RF54-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/RF54-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/RF54-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/RF54-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained">
<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-e734c6e9609233ab051742bb3beeed63" id="new-research">NEW RESEARCH</h2>
</div>



<div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained">
<h2 class="wp-block-heading" id="heading">Adaptive Security, Erasures, and Network Assumptions in Communication-Local MPC</h2>



<p>n-party Multi-Party Computation (MPC) is a cryptographic protocol technique that allows separate parties to securely compute a function on their joint data while keeping their inputs private. To build such a protocol, most works require all pairs of participating parties to be able to securely and reliably communicate with each other. Recently, the problem of Communication-Local (CL) MPC has been explored where this assumption is modelled more realistically – e.g. by only requiring that participating parties can securely and reliably communicate with a few other participating parties (as for example in networks like blockchains). However, few solutions exist that guarantee adaptive security—resilience to dynamic corruption of parties—and most rely on strong assumptions about party actions.</p>



<p>In a recent paper: <a href="https://www.microsoft.com/en-us/research/publication/adaptive-security-erasures-and-network-assumptions-in-communication-local-mpc/">Adaptive Security, Erasures, and Network Assumptions in Communication-Local MPC</a>, researchers from Microsoft and external collaborators revisit assumptions made in earlier work. The authors conclude that for secure, adaptive CL-MPC, some previously assumed capabilities (like secure erasure and multisend) can be bypassed under certain conditions; however, fully reducing all-to-all to all-to-one communication remains unachievable in CL settings without some minimal assumptions. They propose a new SOS-RMT protocol, enabling more efficient CL-MPC under specific feasibility bounds and additional cryptographic assumptions.</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-4 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--5"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/adaptive-security-erasures-and-network-assumptions-in-communication-local-mpc/">Read the paper</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>
</div>



<div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained">
<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-e734c6e9609233ab051742bb3beeed63" id="new-research">NEW RESEARCH</h2>



<h2 class="wp-block-heading" id="heading">Cuttlefish: A Fair, Predictable Execution Environment for Cloud-hosted Financial Exchanges</h2>



<p>Low-latency algorithmic trading is driving efficiency in modern financial markets by promoting accurate/timely pricing of securities, higher liquidity, and lower trade costs for investors. The goal is to process incoming market data and issue trades as quickly as possible to take advantage of ephemeral market-making and arbitrage opportunities. Interest in cloud-hosted financial exchanges is growing, as they promise a cost-effective platform more accessible to market participants, among other benefits.</p>



<p>Unfortunately, one of the major roadblocks in cloud environments is to ensure equal network and compute despite the unpredictable network latencies as well as non-deterministic computation times.</p>



<p>In a recent preprint: <a href="https://www.microsoft.com/en-us/research/publication/cuttlefish-a-fair-predictable-execution-environment-for-cloud-hosted-financial-exchanges/" target="_blank" rel="noreferrer noopener">Cuttlefish: A Fair, Predictable Execution Environment for Cloud-hosted Financial Exchanges</a>, researchers from Microsoft and external collaborators present a fair-by-design algorithmic trading platform that can run in cloud environments. Cuttlefish aims to apply efficient and robust mapping of real operations to a novel formulation of ‘virtual time’. This allows Cuttlefish to push fairness to the extreme, regardless of the underlying network communication and computation hardware. The researchers’ implementation and evaluation validate the practicality of Cuttlefish and shows its operational efficiency on public cloud platforms. This paper builds on previous work: <a href="https://www.microsoft.com/en-us/research/publication/rethinking-cloud-hosted-financial-exchanges-for-response-time-fairness/" target="_blank" rel="noreferrer noopener">Rethinking Cloud-hosted Financial Exchanges for Response Time Fairness</a> and <a href="https://www.microsoft.com/en-us/research/publication/dbo-fairness-for-cloud-hosted-financial-exchanges/" target="_blank" rel="noreferrer noopener">DBO: Fairness for Cloud-Hosted Financial Exchanges.</a> </p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-5 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--6"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/cuttlefish-a-fair-predictable-execution-environment-for-cloud-hosted-financial-exchanges/">Read the paper</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>
</div>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1085523">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">Microsoft research podcast</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/podcast/abstracts-august-15-2024/" aria-label="Abstracts: August 15, 2024" data-bi-cN="Abstracts: August 15, 2024" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/08/Episode-15_Abstracts_Hero_Feature_No_Text_1400x788.jpg" alt="Stylized microphone and sound waves illustration." />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">Abstracts: August 15, 2024</h2>
				
								<p class="large">Advanced AI may make it easier for bad actors to deceive others online. A multidisciplinary research team is exploring one solution: a credential that allows people to show they’re not bots without sharing identifying information. Shrey Jain and Zoë Hitzig explain.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://www.microsoft.com/en-us/research/podcast/abstracts-august-15-2024/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Listen now" data-bi-cN="Abstracts: August 15, 2024" target="_blank">
							Listen now						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained">
<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-e734c6e9609233ab051742bb3beeed63" id="new-research">NEW RESEARCH</h2>



<h2 class="wp-block-heading" id="heading">LLM2CLIP: Powerful language model unlocks richer visual representation</h2>



<p>CLIP is a prominent multimodal foundational model, aligning visual and textual signals into a shared feature space. It supports various tasks, including zero-shot classification, detection, segmentation, and cross-modal retrieval, significantly influencing the entire multimodal domain. As a feature extractor, it has become dominant in cross-modal representation tasks such as image understanding, video understanding, and text-to-image/video generation. However, rapid advancements in large language models (LLMs) are continually pushing the boundaries of language comprehension and generation. Can the capabilities of LLMs be harnessed to further improve multimodal representation learning?</p>



<p>In a recent article: <a href="https://www.microsoft.com/en-us/research/publication/llm2clip-powerful-language-model-unlock-richer-visual-representation/">LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation</a>, researchers from Microsoft and external collaborators propose LLM2CLIP, a novel approach to unlock CLIP&#8217;s potential, focusing on fundamental optimizations of promising foundation models. By fine-tuning the LLM in the caption space with contrastive learning, they extract its textual capabilities into the output embeddings, significantly improving the output layer&#8217;s textual discriminability. The researchers then design a training process where the fine-tuned LLM acts as a powerful teacher for CLIP&#8217;s visual encoder. The LLM&#8217;s presence allows them to incorporate longer and more complex captions without being restricted by CLIP&#8217;s text encoder&#8217;s context window and ability limitations. Their experiments demonstrate that this approach brings substantial improvements in cross-modal tasks.</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-6 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--7"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/llm2clip-powerful-language-model-unlock-richer-visual-representation/">Read the paper</a></div>



<div class="wp-block-button is-style-outline is-style-outline--8"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://aka.ms/llm2clip" target="_blank" rel="noreferrer noopener">Learn more about LLM2CLIP</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>
</div>



<div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained">
<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-e734c6e9609233ab051742bb3beeed63" id="new-research">NEW RESEARCH</h2>



<h2 class="wp-block-heading" id="heading">LORASC: Expressive and Generalizable Low-rank Adaptation for Large Models via Slow Cascaded Learning</h2>



<p>Foundation models, which are large-scale models pre-trained on extensive datasets and subsequently adapted for specific downstream tasks, have become integral to contemporary machine learning frameworks. Fine-tuning these models is essential, yet full parameter fine-tuning often encounters significant memory and computational bottlenecks. Parameter-efficient finetuning (PEFT) techniques aim to minimize the number of trainable parameters to reduce training costs and improve training stability. Among these techniques, Low-Rank Adaptation (LoRA) is highly efficient, although it has limitations in terms of expressiveness and generalization have been noted.</p>



<p>In a recent paper: <a href="https://www.microsoft.com/en-us/research/publication/lorasc-expressive-and-generalizable-low-rank-adaptation-for-large-models-via-slow-cascaded-learning/">LORASC: Expressive and Generalizable Low-rank Adaptation for Large Models via Slow Cascaded Learning</a>, researchers from Microsoft and external collaborators present an innovative technique designed to enhance LoRA’s expressiveness and generalization capabilities while preserving its training efficiency. Their cascaded learning strategy enables a mixture-of-low-rank adaptation, thereby increasing the model’s ability to capture complex patterns. They also introduce a slow-fast update mechanism and cascading noisy tuning to bolster generalization. Their extensive experiments on various language and vision datasets, as well as robustness benchmarks, show that the proposed method significantly outperforms existing baselines, while also mitigating overfitting, enhancing model stability, and improving out-of-distribution (OOD) robustness.</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-7 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--9"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/lorasc-expressive-and-generalizable-low-rank-adaptation-for-large-models-via-slow-cascaded-learning/">Read the paper</a></div>
</div>
</div>



<div style="padding-bottom:64px; padding-top:64px" class="wp-block-msr-immersive-section alignfull row has-background has-lighter-gray-background-color has-text-color has-black-color wp-block-msr-immersive-section">
	
	<div class="container">
		<div class="wp-block-msr-immersive-section__inner">
			<div class="wp-block-msr-cards msr-cards msr-cards--default mt-4 has-text-align-left" data-bi-aN="microsoft-research-in-the-news">
	<div class="msr-cards__inner">
					<div class="heading-wrapper">
				<h2 class="mb-5 ">Microsoft Research in the news</h2>
			</div>
		
		<div class="row row-cols-1 row-cols-sm-2 row-cols-lg-3">
	<div class="msr-cards__card msr-cards__card--default col">
	<div class="card has-spectrum-border-top__hover material-card h-100 p-0" data-mount="click-group">

		
		<div class="card-body bg-white p-4 pt-3">
							<h3 class="h5">
					<a
						class="text-decoration-none text-black"
						data-bi-cN="Can AI spot the next bomb cyclone far in advance? Microsoft hopes so"
						href="https://www.seattletimes.com/business/can-ai-spot-the-next-bomb-cyclone-far-in-advance-microsoft-hopes-so/"
					>
						<span>Can AI spot the next bomb cyclone far in advance? Microsoft hopes so</span>&nbsp;<span class="glyph-prepend glyph-prepend-small glyph-prepend-chevron-right" aria-hidden="true"></span>
					</a>
				</h3>
										<div class="card__description">
					<p>Seattle Times | November 23, 2024</p><p>Microsoft claims that Aurora, a deep-learning model that’s constantly being trained, can produce weather forecasts much faster than — and with accuracy that meets or exceeds — traditional forecasting models.</p>				</div>
			
					</div>
	</div>
</div>
<div class="msr-cards__card msr-cards__card--default col">
	<div class="card has-spectrum-border-top__hover material-card h-100 p-0" data-mount="click-group">

		
		<div class="card-body bg-white p-4 pt-3">
							<h3 class="h5">
					<a
						class="text-decoration-none text-black"
						data-bi-cN="How Microsoft's next-gen BitNet architecture is turbocharging LLM efficiency "
						href="https://venturebeat.com/ai/how-microsofts-next-gen-bitnet-architecture-is-turbocharging-llm-efficiency/"
					>
						<span>How Microsoft's next-gen BitNet architecture is turbocharging LLM efficiency </span>&nbsp;<span class="glyph-prepend glyph-prepend-small glyph-prepend-chevron-right" aria-hidden="true"></span>
					</a>
				</h3>
										<div class="card__description">
					<p>VentureBeat | November 13, 2024</p><p>One-bit large language models (LLMs) have emerged as a promising approach to making generative AI more accessible and affordable. In a new paper, Microsoft researchers introduce Binet a4.8, a new technique that further improves the efficiency of one-bit LLMs without sacrificing their performance.</p>				</div>
			
					</div>
	</div>
</div>
<div class="msr-cards__card msr-cards__card--default col">
	<div class="card has-spectrum-border-top__hover material-card h-100 p-0" data-mount="click-group">

		
		<div class="card-body bg-white p-4 pt-3">
							<h3 class="h5">
					<a
						class="text-decoration-none text-black"
						data-bi-cN="2024 Ellison Cliffe Lecture: AI in science and medicine with Christopher Bishop"
						href="https://www.youtube.com/watch?v=lB3K4pk5jN4"
					>
						<span>2024 Ellison Cliffe Lecture: AI in science and medicine with Christopher Bishop</span>&nbsp;<span class="glyph-prepend glyph-prepend-small glyph-prepend-chevron-right" aria-hidden="true"></span>
					</a>
				</h3>
										<div class="card__description">
					<p>Royal Society of Medicine | November 13, 2024</p><p>Christopher Bishop, Technical Fellow and Director of Microsoft Research AI for Science, discusses the extraordinary advances in the deep learning technology that underpins the AI revolution, including crucial progress in the fields of scientific discovery and medicine. This recent speech at the Royal Society of Medicine includes current examples of AI’s impact in materials design, drug discovery, and healthcare.</p>				</div>
			
					</div>
	</div>
</div>
</div>

					<div class="justify-content-center text-center mb-4">
				<a
					href="https://www.microsoft.com/en-us/research/news-and-awards/"
					class="btn btn-outline-primary glyph-append glyph-append-small glyph-append-chevron-right msr-cards__cta"
					data-bi-cN="View more news and awards"
					data-bi-type="button"
				>
					View more news and awards				</a>
			</div>
			</div>
</div>		</div>
	</div>

	</div>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-december-2-2024/">Research Focus: Week of December 2, 2024</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>MarS: A unified financial market simulation engine in the era of generative foundation models</title>
		<link>https://www.microsoft.com/en-us/research/blog/mars-a-unified-financial-market-simulation-engine-in-the-era-of-generative-foundation-models/</link>
		
		<dc:creator><![CDATA[Brenda Potts]]></dc:creator>
		<pubDate>Wed, 04 Dec 2024 17:01:25 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1106997</guid>

					<description><![CDATA[<p>Microsoft Research presents a new large market model and Financial Market Simulation Engine (MarS) to help empower financial researchers with enhanced efficiency and more accurate insights for downstream tasks in financial markets.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/mars-a-unified-financial-market-simulation-engine-in-the-era-of-generative-foundation-models/">MarS: A unified financial market simulation engine in the era of generative foundation models</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS-BlogHeroFeature-1400x788-1.jpg" alt="MarS illustration with document workflow and chatbot icons on a purple gradient background" class="wp-image-1107222" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<h2 class="wp-block-heading" id="introduction">Introduction</h2>



<p>Generative foundation models have transformed various domains, creating new paradigms for content generation. Integrating these models with domain-specific data enables industry-specific applications. Microsoft Research has used this approach to develop the large market model (LMM) and the Financial Market Simulation Engine (MarS) for the financial domain. These innovations have the potential to empower financial researchers to customize generative models for diverse scenarios, establishing a new paradigm for applying generative models to downstream tasks in financial markets. This integration may provide enhanced efficiency, more accurate insights, and significant advancements in the financial domain.&nbsp;</p>



<h3 class="wp-block-heading" id="applying-generative-models-to-financial-markets">Applying generative models to financial markets</h3>



<p>In recent years, generative foundation models have achieved notable success in fields like natural language processing and media generation. Their rise has sparked a new wave of research and industrial adoption, reshaping production processes across industries. These models excel due to three essential elements: a large volume of high-quality training data; effective tokenization and serialization of core information (such as semantic information in text); and an auto-regressive training approach that models data comprehensively, enabling implicit reasoning.&nbsp;</p>



<p>Building on years of AI applications across industries, Microsoft researchers recognized that combining generative models with domain-specific data could lead to impactful solutions, particularly in finance. The financial market is a prime example, notably for its vast amount of order data, which are characterized by three key features:&nbsp;</p>



<ul class="wp-block-list">
<li><strong>Fine granularity</strong>: Orders, as the atomic data in the financial market, provide a comprehensive and detailed representation of the real market. Combined with matching rules, one can reproduce the entire market operation process.&nbsp;</li>



<li><strong>Large scale</strong>: Electronic trading has resulted in the accumulation of massive trade-order data across global exchanges</li>



<li><strong>Well-structured</strong>: The structured nature of order data makes it ideal for tokenization and sequential modeling</li>
</ul>



<div class="annotations " data-bi-aN="margin-callout">
	<ul class="annotations__list card depth-16 bg-body p-4 annotations__list--right">
		<li class="annotations__list-item">
						<span class="annotations__type d-block text-uppercase font-weight-semibold text-neutral-300 small">Download</span>
			<a href="https://github.com/microsoft/MarS" target="_self" class="annotations__link font-weight-semibold text-decoration-none" data-bi-type="annotated-link" aria-label="MarS" data-bi-aN="margin-callout" data-bi-cN="MarS">
				MarS&nbsp;<span class="glyph-append glyph-append-share glyph-append-xsmall"></span>
			</a>
					</li>
	</ul>
</div>



<p>These characteristics position order flow data as a critical foundation for generative modeling in financial markets. To this end, Microsoft Research developed the LMM and the MarS, which financial researchers can use to customize generative models for various applications, thus fostering a new paradigm of generative solutions for all downstream tasks in finance. This has the potential to advance efficiency and insight generation in the financial industry.&nbsp;</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="850" height="278" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-1.jpg" alt="Figure 1: Illustration of Stock Market and Orders. On the left, a document icon shows order details. An arrow points to the right where multiple icons (robots and human figures) interact with charts and graphs representing market data." class="wp-image-1107234" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-1.jpg 850w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-1-300x98.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-1-768x251.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-1-240x78.jpg 240w" sizes="auto, (max-width: 850px) 100vw, 850px" /><figcaption class="wp-element-caption">Figure 1: Illustration of stock market and orders</figcaption></figure>



<h3 class="wp-block-heading" id="tokenization-of-order-flow-information">Tokenization of order flow information</h3>



<p>Order flow data is vital for generative models in finance, reflecting real-time interactions among market participants. It offers two types of value:&nbsp;</p>



<ul class="wp-block-list">
<li><strong>Fine-grained market feedback</strong>: Each order, especially large ones, may influence others&#8217; decisions, providing a micro-level view of pricing behavior.&nbsp;</li>



<li><strong>Macroscopic market dynamics</strong>: Collective interactions shape trading dynamics over time, capturing the evolution and resolution of conflicts between market forces.&nbsp;</li>
</ul>



<p>Researchers at Microsoft developed LMM by modeling both individual orders and entire order sets over time. This two-tiered approach captures both fine-grained feedback and macro-level dynamics of competition. Figure 2 shows the tokenization techniques for these models, enabling high-fidelity simulations of complex market dynamics.&nbsp;</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="620" height="300" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-2.jpg" alt="Figure 2: Illustration of Tokenization for Individual Orders (Top) and Batch Orders (Bottom) . At the top left, a green document labeled 'Type Price Volume Interval' is connected by dotted lines to another document icon. To the right, a bar chart with red and green bars shows volume on the y-axis and numbers on the x-axis. Below, an arrow points from an 'Order Batch' section with three documents to three grids." class="wp-image-1107249" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-2.jpg 620w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-2-300x145.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-2-240x116.jpg 240w" sizes="auto, (max-width: 620px) 100vw, 620px" /><figcaption class="wp-element-caption">Figure 2: Tokenization for individual orders (top) and batch orders (bottom)&nbsp;</figcaption></figure>



<h3 class="wp-block-heading" id="expansion-law-of-large-market-model-unlocking-the-potential-of-financial-data">Expansion law of large market model: Unlocking the potential of financial data&nbsp;</h3>



<p>The effectiveness of generative models improves significantly with larger training datasets and model parameters. Researchers at Microsoft used two tokenization strategies to design models based on the Transformer architecture, testing them across varying data scales. Figure 3 illustrates the scaling behavior of both the order and order batch models, highlighting insights from historical trading data. This integration enhances the model&#8217;s ability to generate order flows with a deep understanding of market intricacies, enabling more accurate time-series modeling.&nbsp;</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="850" height="303" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-3.jpg" alt="Figure 3: Two line graphs comparing validation loss against the number of training tokens for different model sizes. The left graph, titled 'Order Model,' shows curves for model sizes ranging from 2M to 1.02B, with validation loss decreasing as the number of training tokens increases. The right graph, titled 'Order-Batch Model,' displays curves for model sizes ranging from 150M to 3B, also showing a decrease in validation loss with an increase in training tokens." class="wp-image-1107237" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-3.jpg 850w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-3-300x107.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-3-768x274.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-3-240x86.jpg 240w" sizes="auto, (max-width: 850px) 100vw, 850px" /><figcaption class="wp-element-caption">Figure 3: Scaling curves of order and batch order models under different parameter sizes&nbsp;</figcaption></figure>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="931956">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">Spotlight: On-demand video</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/blog/ai-explainer-foundation-models-and-the-next-era-of-ai/" aria-label="AI Explainer: Foundation models ​and the next era of AI" data-bi-cN="AI Explainer: Foundation models ​and the next era of AI" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/03/AIEx01_blog_hero_1400x788.png" alt="a screenshot of a computer screen shot of a man" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">AI Explainer: Foundation models ​and the next era of AI</h2>
				
								<p class="large">Explore how the transformer architecture, larger models and more data, and in-context learning have helped advance AI from perception to creation.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://www.microsoft.com/en-us/research/blog/ai-explainer-foundation-models-and-the-next-era-of-ai/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Watch video" data-bi-cN="AI Explainer: Foundation models ​and the next era of AI" target="_blank">
							Watch video						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="mars-based-on-lmm">MarS based on LMM</h2>



<h3 class="wp-block-heading" id="a-customizable-generative-model-for-financial-scenarios">A customizable generative model for financial scenarios</h3>



<p>Generative models, once trained, can be easily adapted for a range of downstream tasks, often outperforming traditional models tailored for specific scenarios. Building on the development of LMM, researchers further analyzed the needs of various financial scenarios and designed MarS as a versatile financial market simulation engine. MarS not only serves as a general-purpose simulation tool but also introduces a novel framework for applying generative models across diverse financial tasks, from market prediction and risk assessment to trading strategy optimization.&nbsp;</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1280" height="720" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-4.png" alt="Figure 4: Diagram of the MarS framework showing data flow and interactions between components like the current market & environment data, order-level historical market data, large marke model, generated order sequences, simulated market trajectories, and applications." class="wp-image-1107231" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-4.png 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-4-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-4-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-4-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-4-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-4-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-4-240x135.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-4-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-4-960x540.png 960w" sizes="auto, (max-width: 1280px) 100vw, 1280px" /><figcaption class="wp-element-caption">Figure 4: Framework of MarS</figcaption></figure>



<h3 class="wp-block-heading" id="constructing-a-unified-paradigm-for-prediction-and-detection-tasks">Constructing a unified paradigm for prediction and detection tasks&nbsp;</h3>



<p>Traditional financial prediction solutions often require the development of specialized algorithms, which must be frequently adjusted, consuming time and resources. LMM’s capacity to model financial markets in depth allows for periodic updates based on the latest data. MarS creates a virtual exchange to match order flows generated by LMM, simulating trades and deriving simulated market trajectories (see the top right of Figure 4). This approach can effectively address common prediction and detection tasks in financial scenarios, introducing innovative solutions within the generative model framework.&nbsp;</p>



<h3 class="wp-block-heading" id="applications-in-prediction-tasks">Applications in prediction tasks</h3>



<p>Prediction tasks, vital in finance, involve estimating future market metrics. Traditional models require modifications with any change in prediction targets. MarS addresses this by continuously generating future order flows from recent data, which are matched in a virtual exchange, allowing for the simulation of potential future market trajectories. This provides a significant advancement in forecasting capabilities.</p>



<p>Figure 5 demonstrates the use of MarS in forecasting stock-price movements, where its performance significantly outperforms traditional benchmark algorithms. Taking the Order Model (1.02B) for instance, its performance exceeds that of DeepLOB by approximately (0.662/0.583−1=13.5%) at a 1-minute horizon and increases to (0.579/0.473−1=22.4%) at a 5-minute horizon This widening performance gap suggests that the Order Model maintains its predictive accuracy more effectively over longer horizons, highlighting its superior generalization capability compared to baseline, especially as the prediction task becomes more challenging over extended timeframes. This provides an attractive solution for prediction tasks in financial markets, while also highlighting LMM’s capability in modeling stock market dynamics.&nbsp;</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="438" height="300" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-5.jpg" alt="Figure 5: Line graph comparing prediction accuracy over time for three models: DeepLOB, Order Model (0.22B), and Order Model (1.02B). Prediction accuracy decreases as time increases from 1 to 5 minutes, with DeepLOB showing the lowest accuracy and Order Model (1.02B) showing the highest." class="wp-image-1107240" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-5.jpg 438w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-5-300x205.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-5-240x164.jpg 240w" sizes="auto, (max-width: 438px) 100vw, 438px" /><figcaption class="wp-element-caption">Figure 5: Predicting stock price trends with MarS</figcaption></figure>



<h3 class="wp-block-heading" id="applications-in-detection-tasks">Applications in detection tasks</h3>



<p>For regulators, detecting systemic risks or market abuse is critical for market stability. LMM models typical market patterns, enabling the identification of anomalies by comparing real market trajectories with those generated by MarS. Figure 6 shows the differences in the spread distribution (i.e., the difference between the best buy and sell prices, which reflects asset liquidity) between simulated and real market trajectories during a confirmed malicious market manipulation incident. This comparison can uncover subtle deviations indicative of unusual activities, offering regulators effective tools for monitoring market integrity.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="850" height="263" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-6.jpg" alt="Figure 6: Three bar graphs comparing the distribution similarity of data across three different periods: pre-manipulation, manipulation period, and post-manipulation. Each graph shows the probability distribution for 2 types of data: Replay and Simulation. The distribution similarity scores are 0.870 for pre-manipulation, 0.835 for the manipulation period, and 0.873 for post-manipulation." class="wp-image-1107243" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-6.jpg 850w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-6-300x93.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-6-768x238.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-6-240x74.jpg 240w" sizes="auto, (max-width: 850px) 100vw, 850px" /><figcaption class="wp-element-caption">Figure 6: Spread correlation between simulated and real market during market manipulation&nbsp;</figcaption></figure>



<h3 class="wp-block-heading" id="defining-new-fintech-scenarios">Defining new FinTech scenarios&nbsp;</h3>



<p>Generative models can create tailored content from simple descriptions. In MarS, a mechanism generates specific order flows from natural language descriptions of market conditions. To address extreme conditions, researchers developed a control signal system using a hierarchical diffusion model to generate high-fidelity signals during rare events, such as stock market crashes and circuit breakers. This capability transforms broad descriptions into precise order flow controls.&nbsp;</p>



<p>By integrating controlled order generation with real-time feedback, MarS creates a unified framework for prediction and detection tasks, redefining financial research, applications, and market understanding. Key applications include &#8220;What If&#8221; analyses and training environments for reinforcement learning algorithms in realistic market conditions.&nbsp;</p>



<h3 class="wp-block-heading" id="what-if-analysis-for-financial-research">“What If” analysis for financial research</h3>



<p>The question &#8220;What would happen if different sizes of trading orders were executed under different market conditions?&#8221; is crucial for understanding market behavior. Traditional methods, relying on real orders, experience, and assumptions, are costly and slow. Generative models provide a breakthrough solution.</p>



<p>Figure 7 illustrates how MarS can simulate market impact: the top left shows how buy orders affect asset price trajectories, while the top right presents market impact curves of different strategies, matching traditional patterns. Researchers also used MarS to generate large-scale simulated data, constructing a market impact model using ordinary differential equations (ODE). The bottom left of Figure 7) shows the derived impact formula, and the bottom right demonstrates its interpretability. These advancements highlight MarS&#8217;s potential to enhance “What If” research through deep market modeling.&nbsp;</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="514" height="450" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-7.jpg" alt="Figure 7: Composite image of four graphs related to sample research results for market impact of orders Using MarS. The top left graph shows mid-price over time with two lines representing simulation and replay actions. The top right graph displays market impact for different agent types over time. The bottom left graph illustrates the auto-correlation of market impact decay for learned ODE, base ODE, and synthetic Seq. The bottom right heatmap shows interaction weights of the learned ODE with various features on the x-axis and log-transformed time on the y-axis." class="wp-image-1107246" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-7.jpg 514w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-7-300x263.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-7-206x180.jpg 206w" sizes="auto, (max-width: 514px) 100vw, 514px" /><figcaption class="wp-element-caption">Figure 7: Sample research results for market impact of orders using MarS&nbsp;</figcaption></figure>



<h3 class="wp-block-heading" id="training-environments-for-reinforcement-learning-in-financial-markets">Training environments for reinforcement learning in financial markets</h3>



<p>Reinforcement learning (RL) algorithms require controlled environments for testing and optimization. Financial market behaviors often manifest through order flow changes, impacting the market. If the simulation cannot reflect these impacts accurately, an RL algorithm may fail in real-world scenarios.</p>



<p>MarS provides high-fidelity generation and real-time feedback, creating a comprehensive environment for RL in finance. Figure 8 shows the training process of trading agents, highlighting significant improvements in performance over time and demonstrating MarS&#8217;s effectiveness as an RL training ground.&nbsp;</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="436" height="320" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-8.jpg" alt="Figure 8: Line graph titled 'Price Advantage' on the y-axis and 'Step' on the x-axis. The graph shows an upward trend as the steps increase." class="wp-image-1107228" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-8.jpg 436w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-8-300x220.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-8-80x60.jpg 80w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MarS_Figure-8-240x176.jpg 240w" sizes="auto, (max-width: 436px) 100vw, 436px" /><figcaption class="wp-element-caption">Figure 8: Performance of reinforcement learning trading agents trained in MarS. During training, the agent&#8217;s performance improved significantly, showcasing MarS&#8217;s ability to aid in developing robust reinforcement learning algorithms for real market conditions.&nbsp;</figcaption></figure>



<p><em><strong>Disclaimer</strong>: The research mentioned in this article, conducted by Microsoft Research, focuses on scientific exploration, aiming to advance knowledge and provide theoretical and technological support for research and applications in the financial field. All studies adhere to Microsoft&#8217;s responsible AI guidelines, ensuring principles such as fairness, inclusiveness, reliability and safety, transparency, privacy, and accountability are maintained. The technologies and methods discussed are still under research and development, not yet forming any commercial products or services, nor constituting any financial solutions. Readers are advised to consult certified financial professionals before making any financial decisions.&nbsp;</em></p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/mars-a-unified-financial-market-simulation-engine-in-the-era-of-generative-foundation-models/">MarS: A unified financial market simulation engine in the era of generative foundation models</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Advances in run-time strategies for next-generation foundation models</title>
		<link>https://www.microsoft.com/en-us/research/blog/advances-in-run-time-strategies-for-next-generation-foundation-models/</link>
		
		<dc:creator><![CDATA[Alyssa Hughes (2ADAPTIVE LLC dba 2A Consulting)]]></dc:creator>
		<pubDate>Wed, 27 Nov 2024 14:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1106850</guid>

					<description><![CDATA[<p>Discover the most effective run-time strategies on the OpenAI o1-preview model, improving accuracy in medical language tasks.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/advances-in-run-time-strategies-for-next-generation-foundation-models/">Advances in run-time strategies for next-generation foundation models</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MedPrompt-BlogHeroFeature-1400x788-1.png" alt="A visual illustration of Medprompt performance on the MedQA benchmark. Moving from left to right on a horizontal line, the illustration shows how different Medprompt components and additive contributions improve accuracy starting with zero-shot at 81.7 accuracy, to random few-shot at 83.9 accuracy, to random few-shot, chain-of-thought at 87.3 accuracy, to kNN, few-shot, chain-of-thought at 88.4 accuracy, to ensemble with choice shuffle at 90.2 accuracy. " class="wp-image-1106865" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MedPrompt-BlogHeroFeature-1400x788-1.png 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MedPrompt-BlogHeroFeature-1400x788-1-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MedPrompt-BlogHeroFeature-1400x788-1-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MedPrompt-BlogHeroFeature-1400x788-1-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MedPrompt-BlogHeroFeature-1400x788-1-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MedPrompt-BlogHeroFeature-1400x788-1-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MedPrompt-BlogHeroFeature-1400x788-1-240x135.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MedPrompt-BlogHeroFeature-1400x788-1-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MedPrompt-BlogHeroFeature-1400x788-1-960x540.png 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MedPrompt-BlogHeroFeature-1400x788-1-1280x720.png 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<p>Groundbreaking advancements in frontier language models are progressing rapidly, paving the way for boosts in accuracy and reliability of generalist models, making them highly effective in specialized domains. As part of our ongoing exploration of foundation model capabilities, we developed <a href="https://www.microsoft.com/en-us/research/blog/the-power-of-prompting/?msockid=06148b71b49b652837109fc1b5b66432" target="_blank" rel="noreferrer noopener">Medprompt</a> last year—a novel approach to maximize model performance on specialized domain and tasks without fine-tuning. By leveraging multiphase prompting, Medprompt optimizes inference by identifying the most effective chain-of-thought (CoT) examples at run time and<em> </em>drawing on multiple calls to refine output. When deployed with GPT-4, Medprompt achieved an impressive 90.2% accuracy on the MedQA benchmark (USMLE-style), outperforming all other methods.&nbsp;</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/llm_timeline1-Fig1-1400x788-1.jpg" alt="A line chart that plots the MedQA test accuracy (y-axis) over time (x-axis).  

Open AI o1-preview model achieves the highest result at 96.0% accuracy followed by Med-Gemini at 91.1%; GPT-4 (Medprompt) at 90.2%; Med PaLM 2 at 86.5; GPT-4 base at 86.1; Med PaLM at 67.2; GPT-3.5 base at 60.2, BioMedLM at 50.3; DRAGON at 47.5; BioLinkBERT at 45.1; PubMedBERT at 38.1.  " class="wp-image-1107318" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/llm_timeline1-Fig1-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/llm_timeline1-Fig1-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/llm_timeline1-Fig1-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/llm_timeline1-Fig1-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/llm_timeline1-Fig1-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/llm_timeline1-Fig1-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/llm_timeline1-Fig1-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/llm_timeline1-Fig1-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/llm_timeline1-Fig1-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/llm_timeline1-Fig1-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">Figure 1. Comparative analyses of performance of multiple models on MedQA.</figcaption></figure>



<p>Less than a year later, our tests show the OpenAI o1-preview demonstrated superior performance over Medprompt, reaching 96% on the same benchmark (Figure 1)—without using sophisticated prompt guidance and control. This advancement is driven by the model’s integration of run-time strategies at its core, enabling state-of-the-art results on medical licensing exams in the United States and Japan, medical subsets of the Massive Multitask Language Understanding (MMLU) benchmark, and nursing exams (NCLEX) as shown in Figure 2.&nbsp;</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/main_radar_70_1011-Fig2-1400x788-1.jpg" alt="A spider web chart plotting the performance of OpenAI o1-preview (0 shot ensemble) compared to GPT-4 (Medprompt) and GPT-4 (5 shot) model performance on medical challenge problems. o1-preview achieves state-of-the-art results on MedQA US (4-option), JMLE-2024, MedMCQA Dev, MMLU Anatomy, MMLU Medical Genetics, MMLU Professional Medicine, MMLU College Biology, and MMLU College Medicine, and NCLEX. GPT-4 (Medprompt) performed better than OpenAI o1-preview (0 shot ensemble) on MMLU Clinical Knowledge" class="wp-image-1107330" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/main_radar_70_1011-Fig2-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/main_radar_70_1011-Fig2-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/main_radar_70_1011-Fig2-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/main_radar_70_1011-Fig2-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/main_radar_70_1011-Fig2-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/main_radar_70_1011-Fig2-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/main_radar_70_1011-Fig2-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/main_radar_70_1011-Fig2-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/main_radar_70_1011-Fig2-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/main_radar_70_1011-Fig2-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">Figure 2. Comparisons on a wide range of medical challenge benchmarks.</figcaption></figure>



<p>These results are notable, prompting us to publish our recent study, findings, and analyses, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/abs/2411.03590" target="_blank" rel="noreferrer noopener"><em>From Medprompt to o1: Exploration of Run-Time Strategies for Medical Challenge Problems and Beyond</em><span class="sr-only"> (opens in new tab)</span></a>. But the numbers are only part of the story. In this blog, we discuss prompting strategies to make the most of o1-preview models and other factors to consider as well as directions forward for run-time strategies.</p>



<h2 class="wp-block-heading" id="is-o1-preview-just-fancy-prompting">Is o1-preview “just” fancy prompting?&nbsp;</h2>



<p>The introduction of the OpenAI o1 model series marks a significant shift from prior GPT models. Unlike GPT, o1 models are trained using reinforcement learning (RL) techniques that enable them to “think” before generating outputs. While Medprompt relies on a cascade of operations with GPT-4 at run time guided by a multistage prompt, the o1 series incorporates this run-time reasoning directly into its RL-based design. The built-in functionality enables the o1 models to significantly outperform even the best results using GPT-4 and Medprompt. The performance gains come with a notable tradeoff: its per-token cost was approximately six times that of GPT-4o at the time of our evaluation. While the results for GPT-4o with Medprompt fall short of o1-preview model performance, the combination offers a more cost-effective alternative. The cost-benefit tradeoffs are highlighted in the following figure, with the x-axis presented on a logarithmic scale.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/cost_frontier1-Fig-3-1400x788-1.jpg" alt="A line chart plotting accuracy on the MedQA Test (y-axis) versus total cost on a logarithmic scale (x-axis). OpenAI o1-preview using 5x, 10x, and 15x Ensemble hover around 1000 total cost. OpenAI o1-preview using Tailored Prompt, Minimal Prompt, Few-shot, kNN Few-shot are around 100 total cost. GPT-4o with Medprompt is below 100; kNN Few-shot CoT, Few-shot CoT, and Few-Shot are at 10; Zero-shot is at 1. GPT-4-Turbo with Medprompt is at 200; kNN Few-shot CoT, Few-shot CoT, and Few-Shot hover near 50, Zero-shot is near 5. " class="wp-image-1107345" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/cost_frontier1-Fig-3-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/cost_frontier1-Fig-3-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/cost_frontier1-Fig-3-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/cost_frontier1-Fig-3-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/cost_frontier1-Fig-3-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/cost_frontier1-Fig-3-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/cost_frontier1-Fig-3-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/cost_frontier1-Fig-3-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/cost_frontier1-Fig-3-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/cost_frontier1-Fig-3-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">Figure 3. Pareto frontier showing accuracy versus total API cost (log scale) on the MedQA benchmark (1273 questions total). o1-preview (Sep 2024) is compared with GPT-4o (Aug 2024) and GPT-4 Turbo (Nov 2023).</figcaption></figure>



<h2 class="wp-block-heading" id="can-we-prompt-engineer-o1-preview">Can we prompt engineer o1-preview?</h2>



<p>The o1-preview model exhibits distinct run-time behaviors compared to the GPT series. While some of our more dynamic prompting strategies performed better than expected with o1-preview models, our most tried-and-true strategy was anything but consistent throughout our evaluation. Figure 4 captures specific performance results for Tailored Prompt, Ensembling, and Few-Shot Prompting on o1-preview. Here’s a summary of our findings:&nbsp;</p>



<ol start="1" class="wp-block-list">
<li><strong>Tailored Prompt</strong>: While minimal prompting—like a brief, one-sentence description followed by a question—offered a strong baseline performance, detailed task descriptions were best for eliciting accurate responses.</li>



<li><strong>Ensembling</strong>: Generating multiple answers per question and using majority voting across different reasoning paths boosted reliability, while shuffling answers in runs produced richer reasoning chains and improved outcomes. Ensembling continues to yield consistent performance improvements.</li>



<li><strong>Few-Shot Prompting</strong>: Guiding the model with a few examples produced inconsistent results and, on average, decreased performance compared with GPT models.</li>
</ol>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1400" height="699" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/experiment_slope_chart_nomedmcqa2-Fig4-1400x788-1.jpg" alt="Three charts show the accuracy of o1-preview when combined with Tailored Prompt, Ensemble, and 5-shot KNN based on an average baseline of medical benchmarks. Tailored Prompts improves accuracy from 94.2 to 94.7; Ensemble (15x) improves accuracy from 94.2 to 95.5; 5-shot KNN decreases accuracy from 94.2 to 93.7.  " class="wp-image-1107585" style="object-fit:cover" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/experiment_slope_chart_nomedmcqa2-Fig4-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/experiment_slope_chart_nomedmcqa2-Fig4-1400x788-1-300x150.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/experiment_slope_chart_nomedmcqa2-Fig4-1400x788-1-1024x511.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/experiment_slope_chart_nomedmcqa2-Fig4-1400x788-1-768x383.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/experiment_slope_chart_nomedmcqa2-Fig4-1400x788-1-240x120.jpg 240w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">Figure 4. Tests of different prompting strategies across benchmark datasets.</figcaption></figure>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="999693">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">Spotlight: Event Series</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300" href="https://researchforum.microsoft.com/?OCID=msr_researchforum_MCR_Blog_Promo" aria-label="Microsoft Research Forum" data-bi-cN="Microsoft Research Forum" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/01/MRF-24_WebImage_1400x788.png" alt="various abstract 3D shapes on a light blue background" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">Microsoft Research Forum</h2>
				
								<p class="large">Join us for a continuous exchange of ideas about research in the era of general AI. Watch the first four episodes on demand.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://researchforum.microsoft.com/?OCID=msr_researchforum_MCR_Blog_Promo" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Register for series" data-bi-cN="Microsoft Research Forum" target="_blank">
							Register for series						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="do-results-stand-in-another-language">Do results stand in another language?&nbsp;</h2>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/jmle-2024-bar1-BlogHeroFeature-1400x788-1.jpg" alt="A chart with two bar charts measuring the accuracy (y-axis) by short and long questions (x-axis) on the Japanese Medical Licensing Examination. The short question bar is slightly higher than the long question ratio for o1-preview (0-shot ensemble). The short question bar is about two points less accurate than the long question bar for o1-preview (0-shot). The short answer bar is a point more accurate than the long question bar for GPT-4o (Medprompt). The short question bar is one point less accurate than the long question bar for GPT-4o (0 shot). " class="wp-image-1107453" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/jmle-2024-bar1-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/jmle-2024-bar1-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/jmle-2024-bar1-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/jmle-2024-bar1-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/jmle-2024-bar1-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/jmle-2024-bar1-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/jmle-2024-bar1-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/jmle-2024-bar1-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/jmle-2024-bar1-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/jmle-2024-bar1-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">Figure 5. JMLE-2024: National medical licensing exam held in Japan (Feb 2024).</figcaption></figure>



<p>We expanded our research to include a new multilingual benchmark based on the Japanese national medical licensing exam.&nbsp;The JMLE (Japanese Medical Licensing Examination) is written in Japanese and administered in February 2024, after the o1-preview model’s knowledge cutoff. <em>Even without translation to English, the o1-preview model achieved a remarkable score of 98.2% accuracy (Figure 5), well above the exam’s minimum passing score of approximately 80%.&nbsp;</em>&nbsp;</p>



<h2 class="wp-block-heading" id="do-reasoning-tokens-improve-performance">Do reasoning tokens improve performance?&nbsp;</h2>



<p>For fun, we conducted tests to determine whether increasing the number of reasoning tokens could improve performance. Our findings showed that by adjusting the prompt, we were able to consistently increase the number of reasoning tokens used by o1-preview, and the increase was directly correlated with improved performance as demonstrated in Figure 6.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/reasoning_token_slope_chart2-Fig6-1400x788-1.jpg" alt="A chart plotting the impact of reasoning tokens on accuracy. JMLE achieved 95.3% accuracy for Quick Response Prompt and 96.7% accuracy for Extended Reasoning Prompt. MMLU achieved 94.9% accuracy for Quick Response Prompt and 94.7% accuracy for Extended Reasoning Prompt. MedQA achieved 94.3% accuracy for Quick Response Prompt and 95.1% accuracy for Extended Reasoning Prompt. USMLE Sample Exam achieved 92.6% accuracy for Quick Response Prompt and 93.1% accuracy for Extended Reasoning Prompt. USMLE Self Assessment achieved 91.8% accuracy for Quick Response Prompt and 92.2% accuracy for Extended Reasoning Prompt. " class="wp-image-1107408" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/reasoning_token_slope_chart2-Fig6-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/reasoning_token_slope_chart2-Fig6-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/reasoning_token_slope_chart2-Fig6-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/reasoning_token_slope_chart2-Fig6-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/reasoning_token_slope_chart2-Fig6-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/reasoning_token_slope_chart2-Fig6-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/reasoning_token_slope_chart2-Fig6-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/reasoning_token_slope_chart2-Fig6-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/reasoning_token_slope_chart2-Fig6-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/reasoning_token_slope_chart2-Fig6-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">Figure 6. The effect of two prompting strategies that elicit variable length reasoning chains across benchmark datasets.</figcaption></figure>



<h2 class="wp-block-heading" id="what-s-the-takeaway">What’s the takeaway?&nbsp;</h2>



<p>Bottom line: There’s a little something for everyone when it comes to run-time strategies. We’re excited by the performance gains from GPT models to o1-preview models. While these improvements are significant, so is the cost. For those needing proven accuracy on a budget, Medprompt leveraging calls to GPT-4 is a viable option for medicine and beyond. We summarize the relative performance of prompting strategies in Figure 7 to determine the best option, or <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/abs/2411.03590" target="_blank" rel="noreferrer noopener">check out the paper for a detailed breakdown of every dataset, experimental configuration, and prompt template<span class="sr-only"> (opens in new tab)</span></a>.&nbsp;</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/performance_heatmap2-Fig7-1400x788-1.jpg" alt="A matrix that shows the relative performance of prompting strategies over baseline medical benchmarks. The top row from left to right are the results for baseline numbers: JMLE = 95.6%; MMLU = 94.6%; MedMCQA = 81.4%; MedQA = 94.9%; USMLE Sample Exam = 94.0%; USMLE Self Assessment = 91.8%. The second row from left to right, 5-shot Random baseline difference: JMLE = +1.2%; MMLU = -1.1%; MedMCQA = 0.0%; MedQA = -1.4%; USMLE Sample Exam = -0.4%; USMLE Self Assessment = -1.0%. The third row from left to right, 5-shot KNN baseline difference: JMLE = +0.6%; MMLU = -0.1%; MedMCQA = +1.2%; MedQA = -2.2%; USMLE Sample Exam = -0.3%; USMLE Self Assessment = -0.6%. The fourth row from left to right, Bootstrap Ensemble (5x) baseline difference: JMLE = +1.5%; MMLU = +0.1%; MedMCQA = +1.3%; MedQA = +0.7%; USMLE Sample Exam = +1.3%; USMLE Self Assessment = +1.0%. The fifth row from left to right, Bootstrap Ensemble (10x) baseline difference: JMLE = +1.4%; MMLU = +0.6%; MedMCQA = +1.5%; MedQA = +0.7%; USMLE Sample Exam = +1.3%; USMLE Self Assessment = +1.1%. The sixth row from left to right, Ensemble (15x) baseline difference: JMLE = +1.5%; MMLU = +0.6%; MedMCQA = +2.0%; MedQA = +1.1%; USMLE Sample Exam = +2.0%; USMLE Self Assessment = +1.3%. The seventh row from left to right, Tailored Prompt baseline difference: JMLE = +1.6%; MMLU = +0.4%; MedMCQA = +0.9%; MedQA = +0.2%; USMLE Sample Exam = +0.0%; USMLE Self Assessment = +0.4%. The eighth row from left to right, Tailored Bootstrap Ensemble (5x) baseline difference: JMLE = +2.2%; MMLU = +0.7%; MedMCQA = +1.8%; MedQA = +0.8%; USMLE Sample Exam = +0.9%; USMLE Self Assessment = +1.1%. The ninth row from left to right, Tailored Bootstrap Ensemble (10x) baseline difference: JMLE = +2.3%; MMLU = +0.7%; MedMCQA = +2.1%; MedQA = +0.9%; USMLE Sample Exam = +0.9%; USMLE Self Assessment = +1.2%. The tenth row from left to right, Tailored Ensemble (15x) baseline difference: JMLE = +2.5%; MMLU = +0.4%; MedMCQA = +2.6%; MedQA = +1.1%; USMLE Sample Exam = +0.9%; USMLE Self Assessment = +1.4%.   " class="wp-image-1107396" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/performance_heatmap2-Fig7-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/performance_heatmap2-Fig7-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/performance_heatmap2-Fig7-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/performance_heatmap2-Fig7-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/performance_heatmap2-Fig7-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/performance_heatmap2-Fig7-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/performance_heatmap2-Fig7-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/performance_heatmap2-Fig7-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/performance_heatmap2-Fig7-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/performance_heatmap2-Fig7-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">Figure 7. Heatmap showing absolute accuracy and relative performance over baseline zero-shot prompt (in parenthesis) across all benchmark datasets.</figcaption></figure>



<h2 class="wp-block-heading" id="anything-more-to-consider">Anything more to consider?</h2>



<p>We highlighted several considerations in the paper that are worth checking out. Here are three opportunities that are top of mind:</p>



<ul class="wp-block-list">
<li><em>Research on run-time strategies</em>. The research community has largely relied on boosting model capabilities with data, compute, and model size, predictably achieving gains by way of scaling laws. A promising new direction is inference-time scaling—the value of investing in additional computation and machinery for guiding inference at run time. We highlight in the paper opportunities to guide run-time allocations to boost efficiency, accuracy, and intellectual capabilities, including meta reasoning and reflection in real time and learning <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://erichorvitz.com/cc_aij_horvitz.pdf">during the “idle” time<span class="sr-only"> (opens in new tab)</span></a> between problem solving. We see a great deal of opportunity for new research and development on real-time and “offline” reasoning, learning, and reflection.</li>



<li><em>Benchmark saturation</em>. With the rapid advancement of state-of-the-art models, many existing medical benchmarks are reaching “saturation,” where models perform extremely well on standing medical competency challenges, considered extremely difficult just a few years ago. Current benchmarks, such as USMLE and JMLE, were designed to assess the performance of medical students and clinicians and are increasingly inadequate for evaluating cutting-edge AI models. To drive understandings of models and guide research, we need to design more challenging medical benchmarks. </li>



<li><em>From benchmarks to clinical applications.</em> We note that, while benchmarks offer valuable insights into performance and accuracy, they often fail to capture the complexities and nuances of real-world clinical decision making and healthcare delivery, more broadly. Conducting clinical trials to rigorously evaluate the impact of AI applications on patient care poses far greater difficulties than benchmarking models against challenge problems drawn from medical competency exams. Yet, studies of AI deployments in realistic clinical settings are essential for understanding model capabilities and for guiding the effective integration of AI into healthcare.</li>
</ul>



<h2 class="wp-block-heading" id="resources">Resources&nbsp;</h2>



<ul class="wp-block-list">
<li><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/abs/2411.03590">From Medprompt to o1: Exploration of Run-Time Strategies for Medical Challenge Problems and Beyond<span data-contrast="auto" xml:lang="EN-US" lang="EN-US" class="TextRun EmptyTextRun SCXW11105993 BCX8" style="-webkit-user-drag: none; -webkit-tap-highlight-color: transparent; margin: 0px; padding: 0px; user-select: text; font-size: 11pt; line-height: 19.6917px; font-family: Aptos, Aptos_EmbeddedFont, Aptos_MSFontService, sans-serif; font-variant-ligatures: none !important;"></span>&nbsp;<span class="sr-only"> (opens in new tab)</span></a></li>



<li><a href="https://www.microsoft.com/en-us/research/publication/can-generalist-foundation-models-outcompete-special-purpose-tuning-case-study-in-medicine/">Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine</a>&nbsp;</li>



<li><a href="https://www.microsoft.com/en-us/research/blog/the-power-of-prompting/">The Power of Prompting</a></li>



<li><a href="https://www.microsoft.com/en-us/research/blog/steering-at-the-frontier-extending-the-power-of-prompting/">Steering at the Frontier: Extending the Power of Prompting</a></li>



<li><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/abs/2303.13375" target="_blank" rel="noreferrer noopener">Capabilities of GPT-4 on Medical Challenge Problems<span class="sr-only"> (opens in new tab)</span></a></li>
</ul>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/advances-in-run-time-strategies-for-next-generation-foundation-models/">Advances in run-time strategies for next-generation foundation models</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Accelerating drug discovery with TamGen: A generative AI approach to target-aware molecule generation</title>
		<link>https://www.microsoft.com/en-us/research/blog/accelerating-drug-discovery-with-tamgen-a-generative-ai-approach-to-target-aware-molecule-generation/</link>
		
		<dc:creator><![CDATA[Alyssa Hughes (2ADAPTIVE LLC dba 2A Consulting)]]></dc:creator>
		<pubDate>Mon, 25 Nov 2024 21:02:26 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1105683</guid>

					<description><![CDATA[<p>TamGen uses generative AI to design new drug candidate compounds to treat TB, going beyond traditional methods by generating novel chemical structures. Learn how a collaboration with the Global Health Drug Discovery Institute is making this possible.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/accelerating-drug-discovery-with-tamgen-a-generative-ai-approach-to-target-aware-molecule-generation/">Accelerating drug discovery with TamGen: A generative AI approach to target-aware molecule generation</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen-BlogHeroFeature-1400x788-1.jpg" alt="TamGen logo on a gradient background." class="wp-image-1105698" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<p>The <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="http://www.ghddi.org/" target="_blank" rel="noreferrer noopener">Global Health Drug Discovery Institute<span class="sr-only"> (opens in new tab)</span></a> (GHDDI) and Microsoft Research have reached a milestone in tuberculosis (TB) drug research with <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.nature.com/articles/s41467-024-53632-4" target="_blank" rel="noreferrer noopener">TamGen<span class="sr-only"> (opens in new tab)</span></a>, an <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/TamGen" target="_blank" rel="noreferrer noopener">open-source<span class="sr-only"> (opens in new tab)</span></a>, transformer-based chemical language model for developing target-specific drug compounds. Working in collaboration, the joint team successfully identified several promising inhibitors for a TB protease, with the most effective compound showing significant bioactivity. Research shows that TamGen can also optimize existing molecules by designing target-aware molecule fragments, potentially enabling the discovery of novel compounds that build on a known molecular core structure.&nbsp;&nbsp;</p>



<h2 class="wp-block-heading" id="generative-ai-helps-overcome-limitations-in-drug-discovery">Generative AI helps overcome limitations in drug discovery</h2>



<p>Generative AI is opening new avenues for scientific exploration by allowing computers to autonomously learn and produce original content. TamGen offers a new approach to drug discovery by applying the principles of generative AI to molecular design. Unlike traditional methods, which depend on systematically screening known compounds—a process that is long, complex, and costly due to its reliance on empirical knowledge and the time-consuming task of exploring a vast chemical library—generative AI provides opportunities for designing entirely new chemical structures.&nbsp;&nbsp;</p>



<p>TamGen goes beyond analyzing existing data by generating chemically diverse compounds that conventional approaches might miss. Figure 1 shows that generative AI expands chemical exploration, allowing for a deeper and more comprehensive search for therapeutic solutions compared to traditional methods.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="2560" height="1441" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGenFig1_1400x788-scaled.jpg" alt="Two funnels that illustrate compared to the traditional screening-based approach, the generative AI-based approach enables the exploration of a broader range of novel compounds. The table below the two funnels has two rows labeled “Valuable cmpd%” and “Novel cmpd” to compare “Screening-based” approach and “Generative-based” approach. The “Screening-based” column shows “Low”, “No”, “High” verse to “Generative-based” shows “High” “Yes”, “Low”" class="wp-image-1106838" style="object-fit:cover" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGenFig1_1400x788-scaled.jpg 2560w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGenFig1_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGenFig1_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGenFig1_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGenFig1_1400x788-1536x865.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGenFig1_1400x788-2048x1153.jpg 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGenFig1_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGenFig1_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGenFig1_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGenFig1_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGenFig1_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGenFig1_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGenFig1_1400x788-1920x1080.jpg 1920w" sizes="auto, (max-width: 2560px) 100vw, 2560px" /><figcaption class="wp-element-caption">Figure 1. Compared with the traditional screening-based approach to drug discovery, a generative AI-based approach enables the discovery of novel compounds. </figcaption></figure>



<h2 class="wp-block-heading" id="tamgen-workflow">TamGen workflow&nbsp;</h2>



<p>TamGen’s workflow uses generative AI to design target-specific chemical compounds. Building on the success of large language models (LLMs), we adapted a similar approach for molecular generation, using a training method like that of GPT models, which involves next-token prediction. Molecules were first converted into a simplified molecular input line entry system (SMILES)—a notation representing molecular structures as symbol sequences, similar to text. We then developed a protein encoder to process information about proteins, including their 3D structure.&nbsp;&nbsp;</p>



<p>A contextual encoder<strong> </strong>combines insights from medical professionals with data on the protein target and existing compounds that have proven to be effective or promising. Using expert knowledge and computational analysis, this encoder guides the compound generator to produce new molecules that are more likely to bind to a given protein. This workflow is illustrated in Figure 2.&nbsp;</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1400" height="1468" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig2-v2.jpg" alt="The protein encoder process proteins information, including 3D information, to help with protein representation. The contextual encoder captures protein-ligand information to generate new molecules bind to proteins. Then the new molecules will be converted into SMILES and pre-trained a GPT-like model to generate molecular compounds." class="wp-image-1106658" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig2-v2.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig2-v2-286x300.jpg 286w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig2-v2-977x1024.jpg 977w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig2-v2-768x805.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig2-v2-172x180.jpg 172w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">Figure 2. TamGen’s workflow&nbsp;</figcaption></figure>



<h2 class="wp-block-heading" id="evaluating-tamgen-computationally">Evaluating TamGen computationally&nbsp;</h2>



<div class="annotations " data-bi-aN="margin-callout">
	<ul class="annotations__list card depth-16 bg-body p-4 annotations__list--right">
		<li class="annotations__list-item">
						<span class="annotations__type d-block text-uppercase font-weight-semibold text-neutral-300 small">Download</span>
			<a href="https://github.com/microsoft/TamGen" target="_self" class="annotations__link font-weight-semibold text-decoration-none" data-bi-type="annotated-link" aria-label="TamGen" data-bi-aN="margin-callout" data-bi-cN="TamGen">
				TamGen&nbsp;<span class="glyph-append glyph-append-share glyph-append-xsmall"></span>
			</a>
					</li>
	</ul>
</div>



<p>To evaluate TamGen’s performance, we compared it to five other common methods used to create 3D shapes of molecules intended to bind to certain proteins. We evaluated these methods using the CrossDocked benchmark, a dataset used in AI research to assess the quality of molecule generation conditioned on a target protein.</p>



<p>Evaluation metrics:&nbsp;</p>



<ul class="wp-block-list">
<li><strong>Docking score</strong>: Measures how well a molecule binds to a target protein. </li>
</ul>



<ul class="wp-block-list">
<li><strong>Quantitative estimate of drug-likeness (QED)</strong>: Assesses how good a candidate a molecule is for a drug. </li>
</ul>



<ul class="wp-block-list">
<li><strong>Synthesis accessibility score (SAS)</strong>:<strong> </strong>Measures how easy or difficult it is to synthesize a particular chemical compound in a lab. </li>
</ul>



<ul class="wp-block-list">
<li><strong>Ro5 (Lipinski&#8217;s rule of five)</strong>: Determines how likely a compound can be developed into an oral drug.  </li>
</ul>



<ul class="wp-block-list">
<li><strong>LogP</strong>: Tests a compound&#8217;s ability to move between water and fats. </li>
</ul>



<ul class="wp-block-list">
<li><strong>Diversity</strong>: Measures the range of different molecular structures and properties in a collection of compounds.  </li>
</ul>



<p>The findings, illustrated in Figure 3, show TamGen’s overall performance. While other methods may produce compounds that bind more strongly, they often include multiple interconnected ring structures. Research indicates that more of these structures can lower synthesis accessibility (SAS) and increase cellular toxicity, making these compounds harder to develop. We believe that molecular pretraining of the model contributed to the overall effectiveness of the compounds TamGen generated.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1400" height="1074" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig3-v2.jpg" alt="The figure uses the shade of the color to illustrate the performance of “Docking”, “QED”, “Lipinski”, “SAS”, “logP”, “Diversity” between TamGen 5 other methods including “Pocket2Mol”, “ResGen”, “TargeDiff”, “3D-AR”, “LiGAN”. The results show that TamGen achieved the best overall performance." class="wp-image-1106661" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig3-v2.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig3-v2-300x230.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig3-v2-1024x786.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig3-v2-768x589.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig3-v2-80x60.jpg 80w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig3-v2-235x180.jpg 235w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">Figure 3. Results from TamGen’s computational performance verification</figcaption></figure>



<h2 class="wp-block-heading" id="experimental-lab-verification">Experimental lab verification&nbsp;</h2>



<p>To ensure real-world applicability, we also validated our findings in a hands-on lab environment. Here, we focused on the ClpP protease in <em>Mycobacterium tuberculosis</em> as the target because it plays a significant role in the bacterium’s survival under stress conditions. We proposed the Design-Refine-Test pipeline to effectively identify molecular compounds for TB drug discovery.</p>



<p><strong>Design stage</strong>: We began by using TamGen to analyze the <em>binding pocket</em> of the protease, where molecules can attach and influence its function. TamGen generated about 2,600 potential compounds that could fit into this pocket. We assessed these compounds based on how well they could attach to the protease and their predicted biological effects, narrowing it down to four promising candidates. </p>



<p><strong>Refine stage</strong>: Next, we entered the four compounds into TamGen, along with three molecular fragments that had been validated in previous lab experiments. This generated a total of 8,600 new compounds, which we screened again using the same criteria, eventually narrowing the selection to 296 compounds.</p>



<p><strong>Test stage</strong>: Because synthesizing all 296 compounds wasn’t feasible, we identified similar compounds available in commercial libraries and tested their initial activity against TB. Five compounds showed promising results. We then synthesized one of the originals and two variants of another. Additionally, we categorized the generated compounds into clusters, selected the top 10% from each cluster based on docking scores, and after manual review, synthesized eight more compounds. </p>



<p>The team from Microsoft Research generated the compounds by TamGen, and the GHDDI team conducted binding analysis, structure–activity relationship studies, and lab experiments to verify the compounds’ inhibitory effect on the ClpP protease, measuring their capacity to interfere with or reduce its activity. Lower IC50 values signify greater potency. Out of the 16 compounds tested, 14 showed strong inhibitory activity measuring under 40 µM, indicating high potential. The most effective compound had a measured IC50 value of 1.88 µM.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1400" height="677" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig4-v2.jpg" alt="The figure shows step by step the process and results, as the context described, of how TamGen analyzes the binding pocket of the protease on Design stage, Refine stage and Test stage." class="wp-image-1106655" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig4-v2.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig4-v2-300x145.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig4-v2-1024x495.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig4-v2-768x371.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig4-v2-240x116.jpg 240w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">Figure 4. The hands-on lab verification process  </figcaption></figure>



<h2 class="wp-block-heading" id="from-molecule-to-fragment-generation">From molecule to fragment generation&nbsp;</h2>



<p>In addition to generating new molecules, TamGen can optimize existing ones by designing smaller molecular fragments. In this fragment generation process, TamGen builds on a given protein target and a molecular core structure to design new compounds around that core. By incorporating information about the target protein, it generates fragments that are highly specific to the target. This approach moves beyond traditional methods that rely on pre-existing databases, which often limit both novelty and effectiveness of molecular fragments.</p>



<p>For fragment generation, we adjusted the input to TamGen’s compound generator. We modified the SMILES string to ensure it ended at the desired growth site. This was done by specifying the fragment we wanted to retain and its connection point for further growth. The tailored SMILES string was then fed into the compound generator to extend the molecule.&nbsp;</p>



<p>We evaluated this method by targeting the ClpP protease for TB, achieving a more than tenfold improvement in the binding affinity of the generated compound compared to the original. Some compounds also demonstrated slow binding, indicating potential for prolonged action and improved selectivity for the target protein.</p>



<h2 class="wp-block-heading" id="ai-s-potential-in-drug-discovery">AI’s potential in drug discovery&nbsp;</h2>



<p>TamGen showcases the transformative potential of generative AI in drug design, combining advanced molecular modeling with researcher-AI collaboration. Tasks that once took years can now be accomplished in a fraction of the time. This research underscores AI&#8217;s expanding role in drug discovery and its promise for developing effective treatments against persistent infectious diseases like TB.&nbsp;</p>



<p>Looking ahead, we plan to integrate advanced techniques into TamGen, including diffusion models for generating 3D structures, reinforcement learning to apply physical constraints, and molecular dynamics simulations to capture proteins&#8217; shifting shapes. These enhancements aim to improve how well generated compounds bind to target proteins, increase their ability to be synthesized, and strengthen other critical drug properties.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/accelerating-drug-discovery-with-tamgen-a-generative-ai-approach-to-target-aware-molecule-generation/">Accelerating drug discovery with TamGen: A generative AI approach to target-aware molecule generation</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>LazyGraphRAG: Setting a new standard for quality and cost</title>
		<link>https://www.microsoft.com/en-us/research/blog/lazygraphrag-setting-a-new-standard-for-quality-and-cost/</link>
		
		<dc:creator><![CDATA[Brenda Potts]]></dc:creator>
		<pubDate>Mon, 25 Nov 2024 17:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1106013</guid>

					<description><![CDATA[<p>Introducing a new approach to graph-enabled RAG. LazyGraphRAG needs no prior summarization of source data, avoiding prohibitive up-front indexing costs. It’s inherently scalable in cost and quality across multiple methods and search mechanisms.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/lazygraphrag-setting-a-new-standard-for-quality-and-cost/">LazyGraphRAG: Setting a new standard for quality and cost</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-large"><img loading="lazy" decoding="async" width="1024" height="576" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-BlogHeroFeature-1400x788-1-1024x576.png" alt="A white geometric network icon with interconnected nodes and lines is centered on a gradient background transitioning from blue on the left to pink on the right." class="wp-image-1106046" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-BlogHeroFeature-1400x788-1-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-BlogHeroFeature-1400x788-1-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-BlogHeroFeature-1400x788-1-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-BlogHeroFeature-1400x788-1-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-BlogHeroFeature-1400x788-1-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-BlogHeroFeature-1400x788-1-240x135.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-BlogHeroFeature-1400x788-1-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-BlogHeroFeature-1400x788-1-960x540.png 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-BlogHeroFeature-1400x788-1-1280x720.png 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-BlogHeroFeature-1400x788-1.png 1400w" sizes="auto, (max-width: 1024px) 100vw, 1024px" /></figure>



<h2 class="wp-block-heading" id="affordable-graphrag-for-every-use-case">Affordable GraphRAG for every use case</h2>



<p>The <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://aka.ms/graphrag" target="_blank" rel="noreferrer noopener">GraphRAG project<span class="sr-only"> (opens in new tab)</span></a> aims to expand the class of questions that AI systems can answer over private datasets by leveraging the implicit relationships within unstructured text.&nbsp;</p>



<p>A key advantage of GraphRAG over conventional vector RAG (or “semantic search”) is its ability to answer <em>global</em> <em>queries</em> that address the entire dataset, such as “what are the main themes in the data?”, or “what are the most important implications for X?”. Conversely, vector RAG excels for <em>local</em> <em>queries</em> where the answer resembles the query and can be found within specific text regions, as is typically the case for “who”, “what”, “when”, and “where” questions.&nbsp;</p>



<p>In recent blog posts, we have shared two new query mechanisms that exploit the rich, summary-based data index created by GraphRAG to improve <a href="https://www.microsoft.com/en-us/research/blog/introducing-drift-search-combining-global-and-local-search-methods-to-improve-quality-and-efficiency/" target="_blank" rel="noreferrer noopener">local search performance</a> and <a href="https://www.microsoft.com/en-us/research/blog/graphrag-improving-global-search-via-dynamic-community-selection/" target="_blank" rel="noreferrer noopener">global search costs</a>, respectively.&nbsp;</p>



<p>In this blog post, we introduce a radically different approach to graph-enabled RAG that requires no prior summarization of the source data, avoiding the up-front indexing costs that may be prohibitive for some users and use cases. We call this approach “LazyGraphRAG”.&nbsp;</p>



<p>A key advantage of LazyGraphRAG is its inherent scalability in terms of both cost and quality. Across a range of competing methods (standard vector RAG, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/profintegra/raptor-rag" target="_blank" rel="noreferrer noopener">RAPTOR<span class="sr-only"> (opens in new tab)</span></a>, and GraphRAG <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://microsoft.github.io/graphrag/query/local_search/" target="_blank" rel="noreferrer noopener">local<span class="sr-only"> (opens in new tab)</span></a>, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://microsoft.github.io/graphrag/query/global_search/" target="_blank" rel="noreferrer noopener">global<span class="sr-only"> (opens in new tab)</span></a>, and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://microsoft.github.io/graphrag/query/drift_search/" target="_blank" rel="noreferrer noopener">DRIFT<span class="sr-only"> (opens in new tab)</span></a> search mechanisms), LazyGraphRAG shows strong performance across the cost-quality spectrum as follows:&nbsp;</p>



<ul class="wp-block-list">
<li>LazyGraphRAG data indexing costs are identical to vector RAG and 0.1% of the costs of full GraphRAG.&nbsp;</li>



<li>For comparable query costs to vector RAG, LazyGraphRAG outperforms all competing methods on local queries, including long-context vector RAG and GraphRAG <a href="https://www.microsoft.com/en-us/research/blog/introducing-drift-search-combining-global-and-local-search-methods-to-improve-quality-and-efficiency/" target="_blank" rel="noreferrer noopener">DRIFT</a> search (our recently introduced RAG approach shown to outperform vector RAG) as well as GraphRAG local search.&nbsp;</li>



<li>The same LazyGraphRAG configuration also shows comparable answer quality to GraphRAG Global Search for global queries, but <em>more than 700 times lower query cost</em>.</li>



<li>For 4% of the query<em> </em>cost of GraphRAG global search, LazyGraphRAG <em>significantly outperforms all competing methods</em> on both local and global query types, including GraphRAG global search at the C2 level (the third level of the community hierarchy recommended for most applications).</li>
</ul>



<p>LazyGraphRAG is coming soon to our open-source <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/graphrag" target="_blank" rel="noreferrer noopener">GraphRAG library<span class="sr-only"> (opens in new tab)</span></a>, providing a unified query interface for both local and global queries over a lightweight data index that is comparable in cost to standard vector RAG.&nbsp;</p>



<h2 class="wp-block-heading" id="blending-vector-rag-and-graph-rag-with-deferred-llm-use">Blending vector RAG and Graph RAG with deferred LLM use</h2>



<p>LazyGraphRAG aims to blend the advantages of vector RAG and Graph RAG while overcoming their respective limitations:&nbsp;</p>



<ul class="wp-block-list">
<li>Vector RAG is a form of <em>best-first</em> <em>search</em> that uses the similarity with the query to select the best-matching source text chunks. However, it has no sense of the breadth of the dataset to consider for global queries.&nbsp;</li>



<li>GraphRAG global search is a form of <em>breadth-first search</em> that uses the community structure of source text entities to ensure that queries are answered considering the full breadth of the dataset. However, it has no sense of the best communities to consider for local queries.</li>
</ul>



<p>LazyGraphRAG combines best-first and breadth-first search dynamics in an iterative deepening manner (Table 1). Compared to the global search mechanism of full GraphRAG, this approach is “lazy” in ways that defer LLM use and dramatically increase the efficiency of answer generation. Overall performance can be scaled via a single main parameter – the <em>relevance test budget </em>– that controls the cost-quality trade-off in a consistent manner.</p>



<figure class="wp-block-table"><table><thead><tr><th></th><th>GraphRAG</th><th>LazyGraphRAG</th></tr></thead><tbody><tr><td>Build index</td><td>Uses an LLM to extract and describe entities and their relationships, b) uses an LLM to summarize all observations of each entity and relationship, c) uses graph statistics to optimize the entity graph and extract hierarchical community structure</td><td>Uses NLP noun phrase extraction to extract concepts and their co-occurrences, b) uses graph statistics to optimize the concept graph and extract hierarchical community structure</td></tr><tr><td>Summarize index</td><td>Uses an LLM to summarize entities and relationships in each community</td><td>None – the “lazy” approach defers all LLM use until query time</td></tr><tr><td>Refine query</td><td>None – the original query is used throughout</td><td>Uses an LLM to a) identify relevant subqueries and recombine them into a single expanded query, b) refine subqueries with matching concepts from the concept graph</td></tr><tr><td>Match query</td><td>None – all queries are answered using all community summaries (<em>breadth first</em>)&nbsp;</td><td>For each of <em>q </em>subqueries [3-5]: <br>&#8211; Uses text chunk embeddings and chunk-community relationships to first rank text chunks by similarity to the query, then rank communities by the rank of their top-<em>k</em> text chunks (<em>best first</em>) <br>&#8211; Uses an LLM-based sentence-level relevance assessor to rate the relevance of the top-<em>k</em> untested text chunks from communities in rank order (<em>breadth first</em>) <br>&#8211; Recurses into relevant sub-communities after <em>z </em>successive communities yield zero relevant text chunks  (<em>iterative deepening</em>)<br>&#8211; Terminates when no relevant communities remain or <em>relevance test budget</em> / <em>q</em> is reached</td></tr><tr><td>Map answers</td><td>Uses an LLM to answer the original query over random batches of community summaries in parallel</td><td>For each of <em>q</em> subqueries [3-5]:&nbsp;<br>&#8211; Builds a subgraph of concepts from the relevant text chunks&nbsp;<br>&#8211; Uses the community assignments of concepts to group related chunks together&nbsp;<br>&#8211; Uses an LLM to extract subquery-relevant claims from groups of related chunks as a way of focusing on relevant content only&nbsp;<br>&#8211; Ranks and filters extracted claims to fit a pre-defined context window size&nbsp;</td></tr><tr><td>Reduce answers</td><td>Uses an LLM to answer the original query using the mapped answers</td><td>Uses an LLM to answer the expanded query using the extracted map claims</td></tr></tbody></table></figure>



<h2 class="wp-block-heading" id="lazygraphrag-answer-quality-is-state-of-the-art">LazyGraphRAG answer quality is state-of-the-art</h2>



<p>We compared LazyGraphRAG at varying levels of relevance test budget to a range of competing methods, as follows:&nbsp;</p>



<ul class="wp-block-list">
<li>Dataset: 5,590 AP news articles (used with license)</li>



<li>Queries: 100 synthetic queries (50 local and 50 global), generated using a new method to be described in a future blog post</li>



<li>Metrics: Comprehensiveness, Diversity, Empowerment (as described <a href="https://www.microsoft.com/en-us/research/publication/from-local-to-global-a-graph-rag-approach-to-query-focused-summarization/">here</a>, with an LLM used to compare pairs of answers head-to-head on each metric)</li>



<li>Conditions: Includes LazyGraphRAG with three relevance test budget settings, in addition to eight competing conditions from GraphRAG and literature (Table 2).</li>
</ul>



<figure class="wp-block-table"><table><thead><tr><th>Condition</th><th>Description</th></tr></thead><tbody><tr><td>Z100_Lite</td><td>LazyGraphRAG with a relevance test budget of 100 and using a low-cost LLM model at all steps&nbsp;</td></tr><tr><td>Z500</td><td>LazyGraphRAG with a relevance test budget of 500, using a low-cost LLM for relevance tests and a more advanced (higher cost) LLM for query refinement and map/reduce answer generation&nbsp;</td></tr><tr><td>Z1500</td><td>LazyGraphRAG with a relevance test budget of 1,500, using a low-cost LLM for relevance tests and a more advanced (higher cost) LLM for query refinement and map/reduce answer generation&nbsp;</td></tr><tr><td>C1</td><td>GraphRAG Global Search at community level 1&nbsp;</td></tr><tr><td>C2</td><td>GraphRAG Global Search at community level 2</td></tr><tr><td>C3_Dynamic</td><td>GraphRAG Global Search at community level 3 using dynamic community selection (a cost saving solution introduced in our <a href="https://www.microsoft.com/en-us/research/blog/graphrag-improving-global-search-via-dynamic-community-selection/" target="_blank" rel="noreferrer noopener">previous blog post</a>)</td></tr><tr><td>LS</td><td>GraphRAG Local Search</td></tr><tr><td>DRIFT</td><td>GraphRAG DRIFT Search</td></tr><tr><td>SS_8K</td><td>Semantic Search with vector RAG retrieving 8k tokens of input&nbsp;</td></tr><tr><td>SS_64K</td><td>Semantic Search with vector RAG retrieving 64k tokens of input (included to examine the effect of long-context window on vector RAG)&nbsp;</td></tr><tr><td>RAPTOR</td><td>A hierarchical clustering-based approach to RAG that shares some similar features to LazyGraphRAG&nbsp;</td></tr></tbody></table></figure>



<p>Figure 1 shows LazyGraphRAG win rates against each of the eight competing conditions. At the lowest budget level of 100 relevance tests using a low-cost LLM model&nbsp;(same cost as SS_8K), LazyGraphRAG significantly outperforms all conditions on local and global queries, except for GraphRAG global search conditions for global queries. At an increased budget of 500 relevance tests using a more advanced LLM model (4% of the query cost of C2), LazyGraphRAG significantly outperforms all conditions on both local and global queries. LazyGraphRAG’s win rates continue to increase as the relevance test budget increases to 1,500, demonstrating its scalability in terms of cost vs. quality.&nbsp;</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1400" height="2100" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-blog_fig1.png" alt="Figure 1. Clustered bar charts showing win rates of LazyGraphRAG with relevance test budgets of (100, 500, 1,500) over competing conditions for each combination of (local, global) queries and (comprehensiveness, diversity, empowerment) metrics." class="wp-image-1106040" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-blog_fig1.png 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-blog_fig1-200x300.png 200w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-blog_fig1-683x1024.png 683w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-blog_fig1-768x1152.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-blog_fig1-1024x1536.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-blog_fig1-1365x2048.png 1365w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-blog_fig1-120x180.png 120w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">Figure 1. Clustered bar charts showing win rates of LazyGraphRAG with relevance test budgets of (100, 500, 1,500) over competing conditions for each combination of (local, global) queries and (comprehensiveness, diversity, empowerment) metrics</figcaption></figure>



<p>An example comparison is shown in Table 3.</p>



<p><em>Query: Describe what factors individuals in the United States should consider when selecting a health insurance plan during the open enrollment period for 2024 health insurance plans.</em></p>



<figure class="wp-block-table"><table class="has-fixed-layout"><thead><tr><th>Vector RAG (64K context)</th><th>LazyGraphRAG</th></tr></thead><tbody><tr><td># Factors to Consider When Selecting a Health Insurance Plan During Open Enrollment for 2024&nbsp;<br>&nbsp;<br>Selecting a health insurance plan during the open enrollment period can be a complex process, but understanding the key factors to consider can help individuals make informed decisions. Here are some important aspects to keep in mind:&nbsp;<br>&nbsp;<br>## Coverage and Networks&nbsp;<br>&nbsp;<br>One of the primary considerations is the coverage network of the plan. Insurers form coverage networks of hospitals and doctors, and it is crucial to ensure that your preferred healthcare providers are included in the network of the plan you are considering. Many marketplace plans, such as Health Maintenance Organizations (HMOs) and Exclusive Provider Organizations (EPOs), do not cover claims for most non-emergency care outside their networks [Data: Sources (47419, 47421, 47420, 47410, 47408)].&nbsp;<br>&nbsp;<br>## Costs: Premiums, Deductibles, and Out-of-Pocket Maximums&nbsp;<br>&nbsp;<br>Understanding the costs associated with a health insurance plan is essential. This includes the monthly premiums, deductibles, copays, and coinsurance. Plans are typically categorized into different levels such as platinum, gold, silver, and bronze, with bronze plans generally having lower premiums but higher out-of-pocket costs [Data: Sources (47411, 47412, 47416, 47414, 47418)]. Additionally, it is important to consider the out-of-pocket maximum, which is the most you will have to pay in a year for covered services before the insurance starts covering all costs [Data: Sources (47416, 47415)].&nbsp;<br>&nbsp;<br>## Tax Credits and Cost-Sharing Reductions&nbsp;<br>&nbsp;<br>Individuals with lower incomes may qualify for income-based tax credits to help reduce the cost of premiums. It is important to accurately estimate your income for the coming year when applying for these credits to avoid having to repay any excess credits received [Data: Sources (47412, 47413)]. Additionally, those with incomes below 250% of the federal poverty level may qualify for cost-sharing reductions if they select a silver plan, which can lower deductibles and out-of-pocket maximums [Data: Sources (47417, 47418)].&nbsp;<br>&nbsp;<br>## Special Considerations for Medicaid and Medicare&nbsp;<br>&nbsp;<br>For those transitioning from Medicaid, it is important to understand that marketplace plans may come with higher premiums and copays compared to Medicaid. Additionally, many people who have been removed from Medicaid may be eligible to enroll in marketplace plans through an extended enrollment period [Data: Sources (6284, 6285, 6286, 6287)]. For seniors, the Medicare open enrollment period allows for switching between traditional Medicare and Medicare Advantage plans, with considerations for prescription drug coverage and additional benefits like dental and vision [Data: Sources (72060, 72061, 72062, 72063, 72064)].&nbsp;<br>&nbsp;<br>## Assistance and Resources&nbsp;<br>&nbsp;<br>Navigating the complexities of health insurance can be challenging, and seeking assistance from insurance agents or government-funded navigators can be beneficial. These professionals can help individuals understand their options and make informed decisions without recommending specific plans [Data: Sources (47424, 47425, 47426)].&nbsp;<br>&nbsp;<br>## Conclusion&nbsp;<br>&nbsp;<br>Selecting the right health insurance plan requires careful consideration of various factors including coverage networks, costs, eligibility for tax credits and cost-sharing reductions, and available assistance. By thoroughly evaluating these aspects, individuals can choose a plan that best meets their healthcare needs and financial situation for the upcoming year.</td><td>### Types of Health Insurance Plans&nbsp;<br>&nbsp;<br>When selecting a health insurance plan during the open enrollment period for 2024, individuals in the United States have several options to consider:&nbsp;<br>&nbsp;<br>1. **Marketplace Plans**: These are available under the Affordable Care Act (ACA) and are categorized into platinum, gold, silver, and bronze levels. Bronze plans generally have lower premiums but higher out-of-pocket costs, while platinum plans have higher premiums but lower out-of-pocket costs [Data: Sources (47411, 47420, 47421)].&nbsp;<br>&nbsp;<br>2. **Low-Cost Marketplace Plans**: These often come with high deductibles, requiring patients to pay thousands of dollars before most coverage kicks in. They also have annual out-of-pocket maximums that can exceed $9,000 for individuals and $18,000 for families [Data: Sources (47415, 47416, 47414)].&nbsp;<br>&nbsp;<br>3. **Exclusive Provider Organizations (EPOs) and Health Maintenance Organizations (HMOs)**: EPOs and HMOs generally restrict patients to a network of doctors and require a primary care doctor to direct care. They tend to be cheaper but lack out-of-network flexibility [Data: Sources (47420, 47421, 43218, 43217)].&nbsp;<br>&nbsp;<br>4. **Preferred Provider Organizations (PPOs)**: These plans allow for out-of-network services but at a higher cost. They offer more flexibility compared to HMOs and EPOs [Data: Sources (43217)].&nbsp;<br>&nbsp;<br>5. **High-Deductible Health Plans (HDHPs)**: Defined as plans with a deductible of at least $1,600 for individual coverage or $3,200 for family coverage, with out-of-pocket maximums of no more than $8,050 or $16,100, respectively. HDHPs usually have lower premiums, and sometimes companies contribute to a health savings account (HSA) to help cover the deductible [Data: Sources (43227, 43226)].&nbsp;<br>&nbsp;<br>6. **Medicare Advantage**: These are privately run versions of the federal government&#8217;s Medicare program, mostly for people aged 65 and over. They often include prescription drug coverage and may offer additional benefits like dental or vision coverage not provided by traditional Medicare [Data: Sources (72063, 72061, 72060, 72062)].&nbsp;<br>&nbsp;<br>7. **Short-Term Health Insurance Plans**: These plans are limited to three months and can only be renewed for a maximum of four months under new rules. They are intended for temporary coverage but often lack comprehensive benefits [Data: Sources (97999, 97995, 97996, 97997)].&nbsp;<br>&nbsp;<br>### Cost Factors: Premiums, Deductibles, Co-pays, and Out-of-Pocket Maximums&nbsp;<br>&nbsp;<br>The overall cost of health insurance plans in 2024 is influenced by several factors:&nbsp;<br>&nbsp;<br>&#8211; **Premiums**: This is the set monthly cost you pay for your health insurance plan. Premiums have been rising, with a notable increase of 7% for both family and single plans in 2023, partly due to inflation [Data: Sources (83383, 83382, 83384, 83385, 83381, +more)].&nbsp;<br>&nbsp;<br>&#8211; **Deductibles**: The amount you pay out-of-pocket for health care services before your insurance starts to pay. For HDHPs, the deductible is at least $1,600 for individual coverage or $3,200 for family coverage [Data: Sources (43226, 43225)].&nbsp;<br>&nbsp;<br>&#8211; **Co-pays and Co-insurance**: These are the costs you pay each time you receive a medical service. Co-pays are fixed amounts, while co-insurance is a percentage of the service cost.&nbsp;<br>&nbsp;<br>&#8211; **Out-of-Pocket Maximums**: This is the maximum amount you will pay for covered services in a year. For example, HDHPs have out-of-pocket maximums of no more than $8,050 for individual coverage or $16,100 for family coverage [Data: Sources (43227, 43226)].&nbsp;<br>&nbsp;<br>### Provider Networks: In-Network vs. Out-of-Network&nbsp;<br>&nbsp;<br>The network of healthcare providers is a crucial factor in selecting a health insurance plan:&nbsp;<br>&nbsp;<br>&#8211; **In-Network Providers**: These are doctors and hospitals that have agreements with your insurance plan to provide services at lower rates. Ensuring your preferred doctors and specialists are in-network can save you significant costs [Data: Sources (43216, 47419)].&nbsp;<br>&nbsp;<br>&#8211; **Out-of-Network Providers**: Services from these providers are usually more expensive and may not be covered at all, except in emergencies. PPO plans offer some out-of-network coverage but at a higher cost, while HMOs and EPOs generally do not cover non-emergency out-of-network care [Data: Sources (43217, 47421)].&nbsp;<br>&nbsp;<br>### Specific Medical Needs and Services&nbsp;<br>&nbsp;<br>When selecting a health insurance plan, individuals should consider their specific medical needs:&nbsp;<br>&nbsp;<br>&#8211; **Prescription Drugs**: Ensure that your medications are covered by the plan&#8217;s formulary, as drug coverage can change annually [Data: Sources (43220, 43218, 43219)].&nbsp;<br>&nbsp;<br>&#8211; **Mental Health Services**: Coverage for mental health treatments is essential, especially with new rules pushing insurers to increase their coverage of these services [Data: Sources (97031, 97028, 97027, 97030, 97033, +more)].&nbsp;<br>&nbsp;<br>&#8211; **Chronic Conditions**: Plans should cover ongoing treatments and medications for chronic conditions. Medicare Supplement Insurance (Medigap) can help cover gaps in Medicare for chronic disease management [Data: Sources (93367, 93368)].&nbsp;<br>&nbsp;<br>&#8211; **Preventive Care**: Coverage for preventive services like cancer screenings and HIV prevention is mandated under the ACA, though its future is uncertain due to ongoing legal battles [Data: Sources (71106, 71109, 71098, 71099, 71100, +more)].&nbsp;<br>&nbsp;<br>### Key Dates and Steps for Open Enrollment&nbsp;<br>&nbsp;<br>The open enrollment period for 2024 health insurance plans involves several key dates and steps:&nbsp;<br>&nbsp;<br>&#8211; **Marketplace Plans**: Open enrollment starts on November 1, 2023, and runs through mid-December in most states, ending on January 16, 2024 [Data: Sources (47419, 47411, 47416, 47421, 47409, +more)].&nbsp;<br>&nbsp;<br>&#8211; **Medicare**: Open enrollment for Medicare runs from October 15, 2023, to December 7, 2023. During this period, individuals can choose between traditional Medicare, Medicare Advantage plans, and prescription drug plans [Data: Sources (72061, 72063, 72060, 72062)].&nbsp;<br>&nbsp;<br>&#8211; **Special Enrollment Periods**: Individuals who lose coverage due to life events like job loss or moving may qualify for special enrollment periods. For example, those removed from Medicaid may enroll in marketplace plans through July 2024 [Data: Sources (6288, 6289)].&nbsp;<br>&nbsp;<br>By considering these factors, individuals can make informed decisions about their health insurance coverage for 2024, ensuring they select plans that best meet their medical needs and financial situations.</td></tr></tbody></table></figure>



<h2 class="wp-block-heading" id="looking-forward">Looking forward</h2>



<p>LazyGraphRAG shows that it is possible for a single, flexible query mechanism to substantially outperform a diverse range of specialized query mechanisms across the local-global query spectrum, and to do so without the up-front costs of LLM data summarization. Its very fast and almost-free indexing make LazyGraphRAG ideal for one-off queries, exploratory analysis, and streaming data use cases, while its ability to smoothly increase answer quality with increasing relevance test budget makes it a valuable tool for benchmarking RAG approaches in general (e.g., “RAG approach X beats LazyGraphRAG with budget Y for task Z”).&nbsp;</p>



<p>Does this mean that all graph-enabled RAG should be lazy? We believe the answer is no, for three reasons:&nbsp;</p>



<ol class="wp-block-list">
<li>A GraphRAG data index of entity, relationship, and community summaries has use value beyond question answering (e.g., reading and sharing as reports).&nbsp;</li>



<li>A GraphRAG data index of entity, relationship, and community summaries, combined with a LazyGraphRAG-like search mechanism, is likely to achieve better results than LazyGraphRAG alone.&nbsp;</li>



<li>A new kind of GraphRAG data index designed to support a LazyGraphRAG-like search mechanism (e.g., through pre-emptive claim and topic extraction) is likely to achieve the best possible results.&nbsp;</li>
</ol>



<p>We will be exploring these directions in the coming period, with all advances (including LazyGraphRAG itself) released via the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/graphrag" target="_blank" rel="noreferrer noopener">GraphRAG GitHub repository<span class="sr-only"> (opens in new tab)</span></a>. Stay tuned!&nbsp;</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/lazygraphrag-setting-a-new-standard-for-quality-and-cost/">LazyGraphRAG: Setting a new standard for quality and cost</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Ideas: The journey to DNA data storage</title>
		<link>https://www.microsoft.com/en-us/research/podcast/ideas-the-journey-to-dna-data-storage/</link>
		
		<dc:creator><![CDATA[Alyssa Hughes (2ADAPTIVE LLC dba 2A Consulting)]]></dc:creator>
		<pubDate>Tue, 19 Nov 2024 14:00:00 +0000</pubDate>
				<category><![CDATA[Microsoft Research Podcast]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1103874</guid>

					<description><![CDATA[<p>Research manager Karin Strauss and members of the DNA Data Storage Project reflect on the path to developing a synthetic DNA–based system for archival data storage, including the recent open-source release of its most powerful algorithm for DNA error correction.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/podcast/ideas-the-journey-to-dna-data-storage/">Ideas: The journey to DNA data storage</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1401" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Karin-Jake-Bichlien-Sergey_Ideas_Hero_Feature_No_Text_1400x788.jpg" alt="Outlined illustrations of Karin Strauss, Jake Smith, Bichlien Nguyen, and Sergey Yekhanin for the Microsoft Research Podcast, Ideas series." class="wp-image-1103877" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Karin-Jake-Bichlien-Sergey_Ideas_Hero_Feature_No_Text_1400x788.jpg 1401w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Karin-Jake-Bichlien-Sergey_Ideas_Hero_Feature_No_Text_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Karin-Jake-Bichlien-Sergey_Ideas_Hero_Feature_No_Text_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Karin-Jake-Bichlien-Sergey_Ideas_Hero_Feature_No_Text_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Karin-Jake-Bichlien-Sergey_Ideas_Hero_Feature_No_Text_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Karin-Jake-Bichlien-Sergey_Ideas_Hero_Feature_No_Text_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Karin-Jake-Bichlien-Sergey_Ideas_Hero_Feature_No_Text_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Karin-Jake-Bichlien-Sergey_Ideas_Hero_Feature_No_Text_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Karin-Jake-Bichlien-Sergey_Ideas_Hero_Feature_No_Text_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Karin-Jake-Bichlien-Sergey_Ideas_Hero_Feature_No_Text_1400x788-1280x720.jpg 1280w" sizes="auto, (max-width: 1401px) 100vw, 1401px" /></figure>


<div class="wp-block-msr-podcast-container my-4">
	<iframe loading="lazy" src="https://player.blubrry.com/?podcast_id=138524698&modern=1" class="podcast-player" frameborder="0" height="164px" width="100%" scrolling="no" title="Podcast Player"></iframe>
</div>



<p>Behind every emerging technology is a great idea propelling it forward. In the Microsoft Research Podcast series <em>Ideas</em>, members of the research community at Microsoft discuss the beliefs that animate their research, the experiences and thinkers that inform it, and the positive human impact it targets.</p>



<p>Accommodating the increasing amounts of digital data the world is producing requires out-of-the-box thinking. In this episode, guest host <a href="https://www.microsoft.com/en-us/research/people/kstrauss/?msockid=35739e94ab6c69d41b738b93aa076831">Karin Strauss</a>, an innovation strategist and senior principal research manager at Microsoft, brings together members of her team to explore a more sustainable, more cost-effective alternative for archival data storage: synthetic DNA. Strauss, Principal Researcher <a href="https://www.microsoft.com/en-us/research/people/bnguy/?msockid=35739e94ab6c69d41b738b93aa076831">Bichlien Nguyen</a>, Senior Researcher <a href="https://www.microsoft.com/en-us/research/people/jakesmith/?msockid=35739e94ab6c69d41b738b93aa076831">Jake Smith</a>, and Partner Research Manager <a href="https://www.microsoft.com/en-us/research/people/yekhanin/?msockid=35739e94ab6c69d41b738b93aa076831">Sergey Yekhanin</a> discuss how Microsoft Research’s contributions have helped bring “science fiction,” as Strauss describes it, closer to reality, including its role in establishing the DNA Data Storage Alliance to foster collaboration in developing the technology and to establish specifications for interoperability. They also talk about the scope of collaboration with other fields, such as the life sciences and electrical and mechanical engineering, and the coding theory behind the project, including the group’s most powerful algorithm for DNA error correction, Trellis BMA, which is now open source.&nbsp;</p>



<div class="wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-fill-github"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://github.com/microsoft/TrellisBMA">Get the Trellis BMA code</a></div>
</div>



<div class="wp-block-group msr-pattern-link-list is-layout-flow wp-block-group-is-layout-flow">
<hr class="wp-block-separator has-alpha-channel-opacity"/>



<h2 class="wp-block-heading h5" id="learn-more-1">Learn more:</h2>



<ul class="wp-block-list list-unstyled">
<li><a href="https://www.microsoft.com/en-us/research/publication/trellis-bma-coded-trace-reconstruction-on-ids-channels-for-dna-storage/">Trellis BMA: coded trace reconstruction on IDS channels for DNA storage<br></a>Publication, July 2021</li>



<li><a href="https://www.microsoft.com/en-us/research/publication/evaluating-the-risk-of-data-loss-due-to-particle-radiation-damage-in-a-dna-data-storage-system/">Evaluating the risk of data loss due to particle radiation damage in a DNA data storage system | Nature Communications<br></a>Publication, September 2024</li>



<li><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://dnastoragealliance.org/" target="_blank" rel="noreferrer noopener">DNA Data Storage Alliance<span class="sr-only"> (opens in new tab)</span></a><br>Alliance homepage</li>



<li><a href="https://www.microsoft.com/en-us/research/publication/architecting-datacenters-for-sustainability-greener-data-storage-using-synthetic-dna/">Architecting Datacenters for Sustainability: Greener Data Storage using Synthetic DNA</a><br>Publication, September 2021</li>



<li><a href="https://www.microsoft.com/en-us/research/video/microsoft-and-uw-demonstrate-first-fully-automated-dna-data-storage/">Microsoft and UW demonstrate first fully automated DNA data storage</a><br>Video, March 2019</li>



<li><a href="https://www.microsoft.com/en-us/research/podcast/storing-digital-data-in-synthetic-dna-with-dr-karin-strauss/">Storing digital data in synthetic DNA with Dr. Karin Strauss</a><br>Microsoft Research Podcast, October 2018</li>



<li><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.youtube.com/watch?v=mILu11yqtG8" target="_blank" rel="noreferrer noopener">Microsoft and University of Washington DNA Storage Research Project<span class="sr-only"> (opens in new tab)</span></a><br>Video, July 2016</li>
</ul>
</div>



<section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast">
	<div class="subscribe-to-podcast__inner border-top border-bottom border-width-2">
		<h2 class="h5 subscribe-to-podcast__heading">
			Subscribe to the <a href="https://www.microsoft.com/en-us/research/podcast">Microsoft Research Podcast</a>:		</h2>
		<ul class="subscribe-to-podcast__list list-unstyled">
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://itunes.apple.com/us/podcast/microsoft-research-a-podcast/id1318021537?mt=2" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="black" viewBox="0 0 32 32">  <path d="M7.12 0c-3.937-0.011-7.131 3.183-7.12 7.12v17.76c-0.011 3.937 3.183 7.131 7.12 7.12h17.76c3.937 0.011 7.131-3.183 7.12-7.12v-17.76c0.011-3.937-3.183-7.131-7.12-7.12zM15.817 3.421c3.115 0 5.932 1.204 8.079 3.453 1.631 1.693 2.547 3.489 3.016 5.855 0.161 0.787 0.161 2.932 0.009 3.817-0.5 2.817-2.041 5.339-4.317 7.063-0.812 0.615-2.797 1.683-3.115 1.683-0.12 0-0.129-0.12-0.077-0.615 0.099-0.792 0.192-0.953 0.64-1.141 0.713-0.296 1.932-1.167 2.677-1.911 1.301-1.303 2.229-2.932 2.677-4.719 0.281-1.1 0.244-3.543-0.063-4.672-0.969-3.595-3.907-6.385-7.5-7.136-1.041-0.213-2.943-0.213-4 0-3.636 0.751-6.647 3.683-7.563 7.371-0.245 1.004-0.245 3.448 0 4.448 0.609 2.443 2.188 4.681 4.255 6.015 0.407 0.271 0.896 0.547 1.1 0.631 0.447 0.192 0.547 0.355 0.629 1.14 0.052 0.485 0.041 0.62-0.072 0.62-0.073 0-0.62-0.235-1.199-0.511l-0.052-0.041c-3.297-1.62-5.407-4.364-6.177-8.016-0.187-0.943-0.224-3.187-0.036-4.052 0.479-2.323 1.396-4.135 2.921-5.739 2.199-2.319 5.027-3.543 8.172-3.543zM16 7.172c0.541 0.005 1.068 0.052 1.473 0.14 3.715 0.828 6.344 4.543 5.833 8.229-0.203 1.489-0.713 2.709-1.619 3.844-0.448 0.573-1.537 1.532-1.729 1.532-0.032 0-0.063-0.365-0.063-0.803v-0.808l0.552-0.661c2.093-2.505 1.943-6.005-0.339-8.296-0.885-0.896-1.912-1.423-3.235-1.661-0.853-0.161-1.031-0.161-1.927-0.011-1.364 0.219-2.417 0.744-3.355 1.672-2.291 2.271-2.443 5.791-0.348 8.296l0.552 0.661v0.813c0 0.448-0.037 0.807-0.084 0.807-0.036 0-0.349-0.213-0.683-0.479l-0.047-0.016c-1.109-0.885-2.088-2.453-2.495-3.995-0.244-0.932-0.244-2.697 0.011-3.625 0.672-2.505 2.521-4.448 5.079-5.359 0.547-0.193 1.509-0.297 2.416-0.281zM15.823 11.156c0.417 0 0.828 0.084 1.131 0.24 0.645 0.339 1.183 0.989 1.385 1.677 0.62 2.104-1.609 3.948-3.631 3.005h-0.015c-0.953-0.443-1.464-1.276-1.475-2.36 0-0.979 0.541-1.828 1.484-2.328 0.297-0.156 0.709-0.235 1.125-0.235zM15.812 17.464c1.319-0.005 2.271 0.463 2.625 1.291 0.265 0.62 0.167 2.573-0.292 5.735-0.307 2.208-0.479 2.765-0.905 3.141-0.589 0.52-1.417 0.667-2.209 0.385h-0.004c-0.953-0.344-1.157-0.808-1.553-3.527-0.452-3.161-0.552-5.115-0.285-5.735 0.348-0.823 1.296-1.285 2.624-1.291z"/></svg>
						<span class="subscribe-to-podcast__link-text">Apple Podcasts</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribebyemail.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.4 6a2.392 2.392 0 00-2.372 2.119L16 15.6l11.972-7.481A2.392 2.392 0 0025.6 6H6.4zM4 10.502V22.8a2.4 2.4 0 002.4 2.4h19.2a2.4 2.4 0 002.4-2.4V10.502l-11.365 7.102a1.2 1.2 0 01-1.27 0L4 10.502z"/></svg>
						<span class="subscribe-to-podcast__link-text">Email</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribeonandroid.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M12.414 4.02c-.062.012-.126.023-.18.06a.489.489 0 00-.12.675L13.149 6.3c-1.6.847-2.792 2.255-3.18 3.944h13.257c-.388-1.69-1.58-3.097-3.179-3.944l1.035-1.545a.489.489 0 00-.12-.675.492.492 0 00-.675.135l-1.14 1.68a7.423 7.423 0 00-2.55-.45c-.899 0-1.758.161-2.549.45l-1.14-1.68a.482.482 0 00-.494-.195zm1.545 3.824a.72.72 0 110 1.44.72.72 0 010-1.44zm5.278 0a.719.719 0 110 1.44.719.719 0 110-1.44zM8.44 11.204A1.44 1.44 0 007 12.644v6.718c0 .795.645 1.44 1.44 1.44.168 0 .33-.036.48-.09v-9.418a1.406 1.406 0 00-.48-.09zm1.44 0V21.76c0 .793.646 1.44 1.44 1.44h10.557c.793 0 1.44-.647 1.44-1.44V11.204H9.878zm14.876 0c-.169 0-.33.035-.48.09v9.418c.15.052.311.09.48.09a1.44 1.44 0 001.44-1.44v-6.719a1.44 1.44 0 00-1.44-1.44zM11.8 24.16v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84zm5.759 0v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84z"/></svg>
						<span class="subscribe-to-podcast__link-text">Android</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://open.spotify.com/show/4ndjUXyL0hH1FXHgwIiTWU" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M16 4C9.383 4 4 9.383 4 16s5.383 12 12 12 12-5.383 12-12S22.617 4 16 4zm5.08 17.394a.781.781 0 01-1.086.217c-1.29-.86-3.477-1.434-5.303-1.434-1.937.002-3.389.477-3.403.482a.782.782 0 11-.494-1.484c.068-.023 1.71-.56 3.897-.562 1.826 0 4.365.492 6.171 1.696.36.24.457.725.217 1.085zm1.56-3.202a.895.895 0 01-1.234.286c-2.338-1.457-4.742-1.766-6.812-1.747-2.338.02-4.207.466-4.239.476a.895.895 0 11-.488-1.723c.145-.041 2.01-.5 4.564-.521 2.329-.02 5.23.318 7.923 1.995.419.26.547.814.286 1.234zm1.556-3.745a1.043 1.043 0 01-1.428.371c-2.725-1.6-6.039-1.94-8.339-1.942h-.033c-2.781 0-4.923.489-4.944.494a1.044 1.044 0 01-.474-2.031c.096-.023 2.385-.55 5.418-.55h.036c2.558.004 6.264.393 9.393 2.23.497.292.663.931.371 1.428z"/></svg>
						<span class="subscribe-to-podcast__link-text">Spotify</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.667 4a2.676 2.676 0 00-2.612 2.13v.003c-.036.172-.055.35-.055.534v18.666c0 .183.019.362.055.534v.003a2.676 2.676 0 002.076 2.075h.002c.172.036.35.055.534.055h18.666A2.676 2.676 0 0028 25.333V6.667a2.676 2.676 0 00-2.13-2.612h-.003A2.623 2.623 0 0025.333 4H6.667zM8 8h1.333C17.42 8 24 14.58 24 22.667V24h-2.667v-1.333c0-6.618-5.382-12-12-12H8V8zm0 5.333h1.333c5.146 0 9.334 4.188 9.334 9.334V24H16v-1.333A6.674 6.674 0 009.333 16H8v-2.667zM10 20a2 2 0 11-.001 4.001A2 2 0 0110 20z"/></svg>
						<span class="subscribe-to-podcast__link-text">RSS Feed</span>
					</a>
				</li>
					</ul>
	</div>
</section>


<div class="wp-block-msr-show-more">
	<div class="bg-neutral-100 p-5">
		<div class="show-more-show-less" data-mount="show-more-show-less">
			<div>
				<span>
					

<h2 class="wp-block-heading" id="transcript-1">Transcript</h2>



<p>[TEASER]</p>



<p>[MUSIC PLAYS UNDER DIALOGUE]</p>



<p><strong>JAKE SMITH: </strong>This really starts from the fundamental data production–data storage gap, where we produce way more data nowadays than we could ever have imagined years ago. And it&#8217;s more than we can practically store in magnetic media. And so we really need a denser medium on the other side to contain that. DNA is extremely dense. It holds far, far more information per unit volume, per unit mass than any storage media that we have available today. This, along with the fact that DNA is itself a relatively rugged molecule—it lives in our body; it lives outside our body for thousands and thousands of years if we, you know, leave it alone to do its thing—makes it a very attractive media. </p>



<p><strong>BICHLIEN NGUYEN:</strong> It&#8217;s such a futuristic technology, right? When you begin to work on the tech, you realize how many disciplines and domains you actually have to reach in and leverage. It&#8217;s really interesting, this multidisciplinarity, because we&#8217;re, in a way, bridging software with wetware with hardware. And so you, kind of, need <em>all</em> the different disciplines to actually get you to where you need to go.&nbsp;</p>



<p><strong>SERGEY YEKHANIN:</strong> We all work for Microsoft; we are all Microsoft researchers. Microsoft isn’t a startup. But that team, the team that drove the DNA Data Storage Project, it did feel like a startup, and it was something unusual and exciting for me.</p>



<p><strong>SERIES INTRO: </strong>You’re listening to <em>Ideas</em>, a Microsoft Research Podcast that dives deep into the world of technology research and the profound questions behind the code. In this series, we’ll explore the technologies that are shaping our future and the big ideas that propel them forward.</p>



				</span>
				<span id="show-more-show-less-toggle-10" class="show-more-show-less-toggleable-content">
					



<p>[MUSIC FADES]</p>



<p><strong>GUEST HOST KARIN STRAUSS:</strong> I&#8217;m your guest host Karin Strauss, a senior principal research manager at Microsoft. For nearly a decade, my colleagues and I—along with a fantastic and talented group of collaborators from academia and industry—have been working together to help close the data creation–data storage gap. We&#8217;re producing far more digital information than we can possibly store. One solution we&#8217;ve explored uses synthetic DNA as a medium, and over the years, we&#8217;ve contributed to steady and promising progress in the area. We&#8217;ve helped push the boundaries of how much DNA writer can simultaneously store, shown that full automation is possible, and helped create an ecosystem for the commercial success of DNA data storage. And just this week, we&#8217;ve made one of our most advanced tools for encoding and decoding data in DNA open source. Joining me today to discuss the state of DNA data storage and some of our contributions are several members of the DNA Data Storage Project at Microsoft Research: Principal Researcher Bichlien Nguyen, Senior Researcher Jake Smith, and Partner Research Manager Sergey Yekhanin. Bichlien, Jake, and Sergey, welcome to the podcast.<s></s></p>



<p><strong>BICHLIEN NGUYEN:</strong> Thanks for having us, Karin.</p>



<p><strong>SERGEY YEKHANIN:</strong> Thank you so much.</p>



<p><strong>JAKE SMITH: </strong>Yes, thank you.</p>



<p><strong>STRAUSS:</strong> So before getting into the details of DNA data storage and our work, I&#8217;d like to talk about the big idea behind the work and how we all got here. I&#8217;ve often described the DNA Data Storage Project as turning science fiction into reality. When we started the project in 2015, though, the idea of using DNA for archival storage was already out there and had been for over five decades. Still, when I talked about the work in the area, people were pretty skeptical in the beginning, and I heard things like, “Wow, why are you thinking about that? It&#8217;s so far off.” So, first, please share a bit of your research backgrounds and then how you came to work on this project. Where did you first encounter this idea, what do you remember about your initial impressions—<em>or</em> the impressions of others—and what made you want to get involved? Sergey, why don’t you start.</p>



<p><strong>YEKHANIN:</strong> Thanks so much. So I’m a coding theorist by training, so, like, my core areas of research have been error-correcting codes and also computational complexity theory. So I joined the project probably, like, within half a year of the time that it was born, and thanks, Karin, for inviting me to join. So, like, that was roughly the time when I moved from a different lab, from the Silicon Valley lab in California to the Redmond lab, and actually, it just so happened that at that moment, I was thinking about what to do next. Like, in California, I was mostly working on coding for distributed storage, and when I joined here, that effort kept going. But I had some free cycles, and that was the moment when Karin came just to my office and told me about the project. So, indeed, initially, it did feel a lot like science fiction. Because, I mean, we are used to coding for digital storage media, like for magnetic storage media, and here, like, this is biology, and, like, why exactly these kind of molecules? There are so many different molecules. Like, why that? But honestly, like, I didn&#8217;t try to pretend to be a biologist and make conclusions about whether this is the right medium or the wrong medium. So I tried to look into these kinds of questions from a technical standpoint, and there was a lot of, kind of, deep, interesting coding questions, and that was the main attraction for me. At the same time, I wasn’t convinced that we will get as far as we actually got, and I wasn&#8217;t immediately convinced about the future of the field, but, kind of, just the depth and the richness of the, what I’ll call, technical problems, that&#8217;s what made it appealing for me, and I, kind of, enthusiastically joined. And also, I guess, the culture of the team. So, like, it did feel like a startup. Like, we all work for Microsoft; we’re all Microsoft researchers. Microsoft isn’t a startup. But that team, the team that drove the DNA Data Storage Project, it did feel like a startup, and it was something unusual and exciting for me.</p>



<p><strong>NGUYEN: </strong>Oh, I love that, Sergey. So my background is in organic chemistry, and Karin had reached out to me, and I interviewed not knowing what Karin wanted. Actually … so I took the job kind of blind because I was like, “Hmm, Microsoft Research? … DNA biotech? &#8230;” I was very, very curious, and then when she told me that this project was about DNA data storage, I was like, this is a crazy, crazy idea. I definitely was not sold on it, but I was like, well, look, I get to meet and work with so many interesting people from different backgrounds that, one, even if it doesn&#8217;t work out, I’m going to learn something, and, two, I think it <em>could</em> work, like it could work. And so I think that&#8217;s really what motivated me to join.</p>



<p><strong>SMITH: </strong>The first thing that you think when you hear about we&#8217;re going to take what is our hard drive and we&#8217;re going to turn that into DNA is that this is nuts. But, you know, it didn&#8217;t take very long after that. I come from a chemistry, biotech-type background where I&#8217;ve been working on designing drugs, and there, DNA is this thing off in the nethers, you know. You look at it every now and then to see what information it can tell you about, you know, what maybe your drug might be hitting on the target side, and it&#8217;s, you know, that connection—that the DNA <em>contains</em> the information in the living systems, the DNA <em>contains</em> the information in our assays, and why could the DNA not contain the information that we, you know, think more about every day, that information that lives in our computers—as an extremely cool idea.</p>



<p><strong>STRAUSS:</strong> Through our work, we&#8217;ve had years to wrap our heads around DNA data storage. But, Jake, could you tell us a little bit about how DNA data storage works and why we&#8217;re interested in looking into the technology?</p>



<p><strong>SMITH: </strong>So you mentioned it earlier, Karin, that this really starts from the fundamental data production–data storage gap, where we produce way more data nowadays than we could ever have imagined years ago. And it&#8217;s more than we can practically store in magnetic media. This is a problem because, you know, we have data; we have recognized the <em>value</em> of data with the rise of large language models and these other big generative models. The data that we do produce, our video has gone from, you know, substantially small, down at 480 resolution, all the way up to things at 8K resolution that now take orders of magnitude more storage. And so we really need a denser medium on the other side to contain that. DNA is extremely dense. It holds far, far more information per unit volume, per unit mass than any storage media that we have available today. This, along with the fact that DNA is itself a relatively rugged molecule—it lives in our body; it lives outside our body for thousands and thousands of years if we, you know, leave it alone to do its thing—makes it a very attractive media, particularly compared to the traditional magnetic media, which has lower density and a much shorter lifetime on the, you know, scale of decades at most.</p>



<p>So how does DNA data storage actually work? Well, at a very high level, we start out in the digital domain, where we have our information represented as ones and zeros, and we need to convert that into a series of A&#8217;s, C&#8217;s, T&#8217;s, and G&#8217;s that we could then actually produce, and this is really the domain of Sergey. He&#8217;ll tell us much more about how this works later on. For now, let&#8217;s just assume we&#8217;ve done this. And now our information, you know, lives in the DNA base domain. It&#8217;s still in the digital world. It&#8217;s just represented as A’s, C’s, T’s, and G’s, and we now need to make this <em>physical</em> so that we can store it. This is accomplished through large-scale DNA synthesis. Once the DNA has been synthesized with the sequences that we specified, we need to store it. There&#8217;s a lot of ways we can think about storing it. Bichlien’s done great work looking at DNA encapsulation, as well as, you know, other more raw just DNA-on-glass-type techniques. And we&#8217;ve done some work looking at the susceptibility of DNA stored in this unencapsulated form to things like atmospheric humidity, to temperature changes and, most excitingly, to things like neutron radiation. So we&#8217;ve stored our data in this physical form, we&#8217;ve archived it, and coming back to it, likely many years in the future because the properties of DNA match up very well with archival storage, we need to convert it back into the digital domain. And this is done through a technique called DNA sequencing. What this does is it puts the molecules through some sort of machine, and on the other side of the machine, we get out, you know, a noisy representation of what the actual sequence of bases in the molecules were. We have one final step. We need to take this series of noisy sequences and convert it back into ones and zeros. Once we do this, we return to our original data and we&#8217;ve completed, let&#8217;s call it, one DNA data storage cycle.</p>



<p><strong>STRAUSS:</strong> We&#8217;ll get into this in more detail later, but maybe, Sergey, we dig a little bit on encoding-decoding end of things and how DNA is different as a medium from other types of media.</p>



<p><strong>YEKHANIN:</strong> Sure. So, like, I mean, coding is an important aspect of this whole idea of DNA data storage because we have to deal with errors—it’s a new medium—but talking about error-correcting codes in the context of DNA data storage, so, I mean, usually, like … what are error-correcting codes about? Like, on the very high level, right, I mean, you have some data—think of it as a binary string—you want to store it, but there are errors. So usually, like, in most, kind of, forms of media, the errors are bit flips. Like, you store a 0; you get a 1. Or you store a 1; you get a 0. So these are called substitution errors. The field of error-correcting codes, it started, like, in the 1950s, so, like, it’s 70 years old at least. So we, kind of, we understand how to deal with this kind of error reasonably well, so with substitution errors. In DNA data storage, the way you store your data is that given, like, some large amount of digital data, you have the freedom of choosing which short DNA molecules to generate. So in a DNA molecule, it’s a sequence of the bases A, G, C, and T, and you have the freedom to decide, like, which of the short molecules you need to generate, and then those molecules get stored, and then during the storage, some of them are lost; some of them can be damaged. There can be insertions and deletions of bases on every molecule. Like, we call them strands. So you need redundancy, and there are two forms of redundancy. There&#8217;s redundancy that goes across strands, and there is redundancy on the strand. And so, yeah, so, kind of, from the error-correcting side of things, like, we get to decide what kind of redundancy we want to introduce—across strands, on the strand—and then, like, we want to make sure that our encoding and decoding algorithms are efficient. So that&#8217;s the coding theory angle on the field.</p>



<p><strong>NGUYEN: </strong>Yeah, and then, you know, from there, once you have that data encoded into DNA, the question is how do you <em>make</em> that data on a scale that&#8217;s compatible with digital data storage? And so that&#8217;s where a lot of the work came in for really automating the synthesis process and also the reading process, as well. So synthesis is what we consider the writing process of DNA data storage. And so, you know, we came up with some unique ideas there. We made a chip that enabled us to get to the densities that we needed. And then on the reading side, we used different sequencing technologies. And it was great to see that we could actually just, kind of, pull sequencing technologies off the shelf because people are so interested in reading biological DNA. So we explored the Illumina technologies and also Oxford Nanopore, which is a new technology coming in the horizon. And then preservation, too, because we have to make sure that the data that’s stored in the DNA doesn&#8217;t get damaged and that we can recover it using the error-correcting codes.</p>



<p><strong>STRAUSS:</strong> Yeah, absolutely. And it&#8217;s clear that—and it&#8217;s also been our experience that—DNA data storage and projects like this require more than just a team of computer scientists. Bichlien, you’ve had the opportunity to collaborate with many people in all different disciplines. So do you want to talk a little bit about that? What kind of expertise, you know, other disciplines that are relevant to bringing DNA data storage to reality?</p>



<p><strong>NGUYEN: </strong>Yeah, well, it&#8217;s such a futuristic technology, right? When you begin to work on the tech, you realize how many disciplines and domains you actually have to reach in and leverage. One concrete example is that in order to fabricate an electronic chip to synthesize DNA, we really had to pull in a lot of material science research because there&#8217;s different capabilities that are needed when trying to use liquid on a chip. We, you know, have to think about DNA data <em>storage</em> itself. And that&#8217;s a very different beast than, you know, the traditional storage mediums. And so we worked with teams who literally create, you know, these little tiny micro- or nanocapsules in glass and being able to store that there. It&#8217;s really interesting, this multidisciplinarity, because we&#8217;re, in a way, bridging software with wetware with hardware. And so you, kind of, need <em>all</em> the different disciplines to actually get you to where you need to go.</p>



<p><strong>STRAUSS:</strong> Yeah, absolutely. And, you know, building on, you know, collaborators, I think one area that was super interesting, as well, and was pretty early on in the project was building that first end-to-end system that we collaborated with University of Washington, the Molecular Information Systems Lab there, to build. And really, at that point, you know, there had been work suggesting that DNA data storage was viable, but nobody had really shown an end-to-end system, from beginning to end, and in fact, my manager at the time, Doug Carmean, used to call it the “bubble gum and shoestring” system. But it was a crucial first step because it shows it was possible to really fully automate the process. And there have been several interesting challenges there in the system, but we noticed that one particularly challenging one was synthesis. That first system that we built was capable of storing the word “hello,” and that was <em>all</em> we could store. So it wasn&#8217;t a very high-capacity system. But in order to be able to store a lot more volumes of data instead of a simple word, we really needed much more advanced synthesis systems. And this is what both Bichlien and Jake ended up working on, so do you want to talk a little bit about that and the importance of that particular work?</p>



<p><strong>SMITH: </strong>Yeah, absolutely. As you said, Karin, the amount of DNA that is required to store the massive amount of data we spoke about earlier is far beyond the amount of DNA that&#8217;s needed for any, air quotes, traditional applications of synthetic DNA, whether it&#8217;s your gene construction or it&#8217;s your primer synthesis or such. And so we really had to rethink how you make DNA at scale and think about how could this actually scale to meet the demand. And so Bichlien started out looking at a thing called a <em>microelectrode array</em>, where you have this big checkerboard of small individual reaction sites, and in each reaction site, we used electrochemistry in order to control base by base—A, C, T, or G by A, C, T, or G—the sequence that was growing at that particular reaction site. We got this down to the nanoscale. And so what this means practically is that on one of these chips, we could synthesize at any given time on the order of hundreds of millions of individual strands. So once we had the synthesis working with the traditional chemistry where you&#8217;re doing chemical synthesis—each base is added in using a mixture of chemicals that are added to the individual spots—they&#8217;re activated. But each coupling happens due to some energy you <em>prestored</em> in the synthesis of your reagents. And this makes the synthesis of those reagents costly and themselves a bottleneck. And so taking, you know, a look forward at what else was happening in the synthetic biology world, the, you know, next big word in DNA synthesis was and still is <em>enzymatic synthesis</em>, where rather than having to, you know, spend a lot of energy to chemically <em>pre</em>-activate reagents that will go in to make your actual DNA strands, we capitalize on nature&#8217;s synthetic robots—<em>enzymes</em>—to start with less-activated, less-expensive-to-get-to, cheaply-produced-through-natural-processes substrates, and we use the enzymes themselves, toggling <em>their</em> activity over each of the individual chips, or each of the individual spots on our checkerboard, to construct DNA strands. And so we got a little bit into this project. You know, we successfully showed that we could put down selectively one base at a given time. We hope that others will, kind of, take up the work that we&#8217;ve put out there, you know, particularly our wonderful collaborators at Ansa who helped us design the enzymatic system. And one day we will see, you know, a truly parallelized, in this fashion, enzymatic DNA system that can achieve the scales necessary.</p>



<p><strong>NGUYEN: </strong>It&#8217;s interesting to note that even though it&#8217;s DNA and we&#8217;re still storing data in these DNA strands, chemical synthesis and enzymatic synthesis provide different errors that you see in the actual files, right, in the DNA files. And so I know that we talked to Sergey about how do we deal with these new types of errors and also the new capabilities that you can have, for example, if you don&#8217;t control base by base the DNA synthesis.</p>



<p><strong>YEKHANIN:</strong> This whole field of DNA data storage, like, the technologies on the biology side are advancing rapidly, right. And there are different approaches to synthesis. There are different approaches to sequencing. And, presumably, the way the storage is actually done, like, is also progressing, right, and we had works on that. So there is, kind of, this very general, kind of, high-level error profile that you can say that these are the type of errors that you encounter in DNA data storage. Like, in DNA molecules—just the sequence of these bases, A, G, C, T, in maybe a length of, like, 200 or so and you store a very, very large number of them—the errors that you see is that some of these strands, kind of, will disappear. Some of these strings can be torn apart like, let’s say, in two pieces, maybe even more. And then on every strand, you also encounter these errors—insertions, deletions, substitutions—with different rates. Like, the likelihood of all kinds of these errors may differ very significantly across different technologies that you use on the biology side. And also there can be error bursts somehow. Maybe you can get an insertion of, I don’t know, 10 A’s, like, in a row, or you can <em>lose</em>, like, you know, 10 bases in a row. So if you don&#8217;t, kind of, quantify, like, what are the likelihoods of all these bad events happening, then I think this still, kind of, fits at least the majority of approaches to DNA data storage, maybe not exactly all of them, but it fits the majority. So when we design coding schemes, we are trying also, kind of, to look ahead in the sense that, like, we don&#8217;t know, like, in five years, like, how will these error profiles, how will it look like. So the technologies that we develop on the error-correction side, we try to keep them very flexible, so whether it&#8217;s enzymatic synthesis, whether it&#8217;s Nanopore technology, whether it’s Illumina technology that is being used, the error-correction algorithms would be able to adapt and would still be useful. But, I mean, this makes also coding aspect harder because, [LAUGHTER] kind of, you want to keep all this flexibility in mind.</p>



<p><strong>STRAUSS:</strong> So, Sergey, we are at an interesting moment now because you’re open sourcing the Trellis BMA piece of code, right, that you published a few years ago. Can you talk a little bit about that specific problem of trace reconstruction and then the paper specifically and how it solves it?</p>



<p><strong>YEKHANIN:</strong> Absolutely, yeah, so this Trellis BMA paper for that we are releasing the source code right now, this is, kind of, this is the latest in our sequence of publications on error-correction for DNA data storage. And I should say that, like, we already discussed that the project is, kind of, very interdisciplinary. So, like, we have experts from all kinds of fields. But really even within, like, within this coding theory, like, within computer science/information theory, coding theory, in our algorithms, we use ideas from very different branches. I mean, there are some core ideas from, like, core algorithm space, and I won’t go into these, but let me just focus, kind of, on two aspects. So when we just faced this problem of coding for DNA data storage and we were thinking about, OK, so how to exactly design the coding scheme and what are the algorithms that we’ll be using for error correction, so, I mean, we’re always studying the literature, and we came up on this problem called <em>trace reconstruction</em> that was pretty popular—I mean, somewhat popular, I would say—in computer science and in statistics. It didn’t have much motivation, but very strong mathematicians had been looking at it. And the problem is as follows. So, like, there is a long binary string picked at random, and then it’s transmitted over a deletion channel, so some bits—some zeros and some ones—at certain coordinates get deleted and you get to see, kind of, the shortened version of the string. But you get to see it multiple times. And the question is, like, how many times do you need to see it so that you can get a reasonably accurate estimate of the original string that was transmitted? So that was called trace reconstruction, and we took a lot of motivation—we took a lot of inspiration—from the problem, I would say, because really, in DNA data storage, if we think about a single strand, like, a single strand which is being stored, after we read it, we usually get multiple reads of this string. And, well, the errors there are not just deletions. There are insertions, substitutions, and, like, inversive errors, but still we could rely on this literature in computer science that already had some ideas. So there was an algorithm called BMA, <em>Bitwise Majority Alignment</em>. We extended it—we adopted it, kind of, for the needs of DNA data storage—and it became, kind of, one of the tools in our toolbox for error correction.</p>



<p>So we also started to use ideas from literature on electrical engineering, what are called <em>convolutional error-correcting codes</em> and a certain, kind of, class of algorithms for decoding errors in these convolutional error-correcting codes called, like, I mean, Trellis is the main data structure, like, Trellis-based algorithms for decoding convolutional codes, like, Viterbi algorithm or BCJR algorithm. Convolutional codes allow you to introduce redundancy on the string. So, like, with algorithms kind of similar to BMA, like, they were good for doing error correction when there was no redundancy on the strand itself. Like, when there is redundancy on the strand, kind of, we could do some things, but really it was very limited. With Trellis-based approaches, like, again inspired by the literature in electrical engineering, we had an approach to introduce redundancy on the strand, so that allowed us to have more powerful error-correction algorithms. And then in the end, we have this algorithm, which we call Trellis BMA, which, kind of, combines ideas from both fields. So it&#8217;s based on Trellis, but it&#8217;s also more efficient than standard Trellis-based algorithms because it uses ideas from BMA from computer science literature. So this is, kind of, this is a mix of these two approaches. And, yeah, that’s the paper that we wrote about three years ago. And now we&#8217;re open sourcing it. So it is the most powerful algorithm for DNA error correction that we developed in the group. We’re really happy that now we are making it publicly available so that anybody can experiment with the source code. Because, again, the field has expanded a lot, and now there are multiple groups around the globe that work just specifically on error correction apart from all other aspects, so, yeah, so we are really happy that it’s become publicly available to hopefully further advance the field.</p>



<p><strong>STRAUSS:</strong> Yeah, absolutely, and I&#8217;m always amazed by, you know, how, <em>it</em> <em>is</em> really about building on other people&#8217;s work. Jake and Bichlien, you recently published a paper in <em>Nature Communications</em>. Can you tell us a little bit about what it was, what you exposed the DNA to, and what it was specifically about?</p>



<p><strong>NGUYEN:</strong>&nbsp;Yeah. So that paper was on the effects of neutron radiation on DNA data storage. So, you know, when we started the DNA Data Storage Project, it was really a comparison, right, between the different storage medias that exist today. And one of the issues that have come up through the years of development of those technologies was, you know, hard errors and soft errors that were induced by radiation. So we wanted to know, does that <em>maybe</em> happen in DNA? We know that DNA, in humans at least, is affected by radiation from cosmic rays. And so that was really the motivation for this type of experiment. So what we did was we essentially took our DNA files and dried them and threw them in a neutron accelerator, which was fantastic. It was so exciting. That&#8217;s, kind of, the merge of, you know, sci fi <em>with</em> sci fi at the same time. [LAUGHS] It was fantastic. And we irradiated for over 80 million years—</p>



<p><strong>STRAUSS: </strong>The equivalent of …</p>



<p><strong>NGUYEN: </strong>The <em>equivalent</em> of 80 million years.</p>



<p><strong>STRAUSS: </strong>Yes, because it&#8217;s a lot of radiation all at the same time, …</p>



<p><strong>NGUYEN: </strong>It’s a lot of radiation …</p>



<p><strong>STRAUSS:</strong> … and it&#8217;s <em>accelerated</em> radiation exposure?</p>



<p><strong>NGUYEN: </strong>Yeah, I would say it&#8217;s accelerated <em>aging</em> with radiation. It&#8217;s an insane amount of radiation. And it was surprising that even though we irradiated our DNA files with that much radiation, there wasn&#8217;t that much damage. And that&#8217;s surprising because, you know, we know that humans, if we were to be irradiated like that, it would be disastrous. But in, you know, DNA, our files were able to be recovered with zero bit errors.</p>



<p><strong>STRAUSS:</strong> And why that difference?</p>



<p><strong>NGUYEN: </strong>Well, we think there&#8217;s a few reasons. One is that when you look at the interaction between a neutron and the actual elemental composition of DNA—which is basically carbons, oxygens, and hydrogens, maybe a phosphorus—the neutrons don&#8217;t interact with the DNA much. And if it did interact, we would have, for example, a strand break, which based on the error-correcting codes, we can recover from. So essentially, there&#8217;s not much … one, there&#8217;s not much interaction between neutrons and DNA, and second, we have error-correcting codes that would prevent any data loss.</p>



<p><strong>STRAUSS:</strong> Awesome, so yeah, this is another milestone that contributes towards the technology becoming a reality. There are also other conditions that are needed for technology to be brought to the market. And one thing I&#8217;ve worked on is to, you know, create the DNA Data Storage Alliance; this is something Microsoft co-founded with Illumina, Twist Bioscience, and Western Digital. And the goal there was to essentially provide the right conditions for the technology to thrive commercially. We did bring together multiple universities and companies that were interested in the technology. And one thing that we&#8217;ve seen with storage technologies that&#8217;s been pretty important is standardization and making sure that the technology’s interoperable. And, you know, we&#8217;ve seen stalemate situations like Blu-ray and high-definition DVD, where, you know, really we couldn&#8217;t decide on a standard, and the technology, it took a while for the technology to be picked up, and the intent of the DNA Data Storage [Alliance] is to provide an ecosystem of companies, universities, groups interested in making sure that this time, it&#8217;s an interoperable technology from the get-go, and that increases the chances of commercial adoption.<strong> </strong>As a group, we often talk about how amazing it is to work for a company that empowers us to do this kind of research. And for me, one of Microsoft Research’s unique strengths, particularly in this project, is the opportunity to work with such a diverse set of collaborators on such a multidisciplinary project like we have. How do you all think <em>where</em> you&#8217;ve done this work has impacted how you&#8217;ve gone about it and the contributions you’ve been able to make?</p>



<p><strong>NGUYEN:</strong>&nbsp;I&#8217;m going to start with if we look around this table and we see who&#8217;s sitting at it, which is two chemists, a computer architect, and a coding theorist, and we come together and we&#8217;re like, what can we make that would be super, super impactful? I think that&#8217;s the answer right there, is that being at Microsoft and being in a culture that really fosters this type of interdisciplinary collaboration <em>is</em> the key to getting a project like this off the ground.</p>



<p><strong>SMITH:</strong>&nbsp;Yeah, absolutely. And we should acknowledge the gigantic contributions made by our collaborators at the University of Washington. Many of them would fall in <em>not</em> any of these three categories. They’re electrical engineers, they&#8217;re mechanical engineers, they&#8217;re pure biologists that we worked with. And each of them brought their own perspective, and particularly when you talk about going to a true end-to-end system, those perspectives were invaluable as we were trying to fit all the puzzle pieces together.</p>



<p><strong>STRAUSS:</strong> Yeah, absolutely. We&#8217;ve had great collaborations over time—University of Washington, ETH Zürich, Los Alamos National Lab, ChipIr, Twist Bioscience, Ansa Biotechnologies. Yeah, it’s been really great and a great set of different disciplines, all the way from coding theorists to the molecular biology and chemistry, electrical and mechanical engineering. One of the great things about research is there&#8217;s never a shortage of interesting questions to pursue, and for us, this particular work has opened the door to research in adjacent domains, including sustainability fields. DNA data storage requires small amounts of materials to accommodate the large amounts of data, and early on, we wanted to understand if DNA data storage was, as it seemed, a more sustainable way to store information. And we learned a lot. Bichlien and Jake, you had experience in green chemistry when you came to Microsoft. What new findings did we make, and what sustainability benefits do we get with DNA data storage? And, finally, what new sustainability work has the project led to?</p>



<p><strong>NGUYEN: </strong>As a part of this project, if we&#8217;re going to bring new technologies to the forefront, you know, to the world, we should make sure that they have a lower carbon footprint, for example, than previous technologies. And so we ran a life cycle assessment—which is a way to systematically evaluate the environmental impacts of anything of interest—and we did this on DNA data storage and compared it to electronic storage medium<a id="_ftnref1" href="#_ftn1">[1]</a>, and we noticed that if we were able to store all of our digital information in DNA, that we would have benefits associated with carbon emissions. We would be able to reduce that because we don&#8217;t need as much infrastructure compared to the traditional storage methods. And there would be an energy reduction, as well, because this is a passive way of archival data storage. So that was, you know, the main takeaways that we had. But that also, kind of, led us to think about other technologies that would be beneficial beyond data storage and how we could use the same kind of life cycle thinking towards that.</p>



<p><strong>SMITH:</strong> This design approach that you&#8217;ve, you know, talked about us stumbling on, not inventing but seeing other people doing in the literature and trying to implement ourselves on the DNA Data Storage Project, you know, is something that can be much bigger than any single material. And where we think there&#8217;s a, you know, chance for folks like ourselves at Microsoft Research to make a real impact on this sustainability-focused design is through the application of machine learning, artificial intelligence—the <em>new</em> tools that will allow us to look at much bigger design spaces than we could previously to evaluate sustainability metrics that were not possible when everything was done manually and to ultimately, you know, at the end of the day, take a sustainability-first look at what a material should be composed of. And so we&#8217;ve tried to prototype this with a few projects. We had another wonderful collaboration with the University of Washington where we looked at recyclable circuit boards and a novel material called a vitrimer that it could possibly be made out of<a href="#_ftn2" id="_ftnref2">[2]</a>. We&#8217;ve had another great collaboration with the University of Michigan, where we&#8217;ve looked at the design of charge-carrying molecules in these things called flow batteries that have good potential for energy smoothing in, you know, renewables production, trying to get us out of that day-night, boom-bust cycle<a href="#_ftn3" id="_ftnref3">[3]</a>. And we had one more project, you know, this time with collaborators at the University of Berkeley, where we looked at, you know, design of a class of materials called a metal organic framework, which have great promise in low-energy-cost gas separation, such as pulling CO<sub>2</sub> out of the, you know, plume of a smokestack or, you know, ideally out of the air itself<a href="#_ftn4" id="_ftnref4">[4]</a>.</p>



<p><strong>STRAUSS:</strong> For me, the DNA work has made me much more open to projects outside my own research area—as Bichlien mentioned, my core research area is computer architecture, but we&#8217;ve ventured in quite a bit of other areas here—and going way beyond my own comfort zone and really made me love interdisciplinary projects like this and try, really try, to do the most important work I can. And this is what attracted me to these other areas of environmental sustainability that Bichlien and Jake covered, where there&#8217;s absolutely no lack of problems. Like them, I&#8217;m super interested in using AI to solve many of them. So how do each of you think working on the DNA Data Storage Project has influenced your research approach more generally and how you think about research questions to pursue next?</p>



<p><strong>YEKHANIN:</strong> It definitely expanded the horizons a lot, like, just, kind of, just having this interactions with people, kind of, whose core areas of research are so different from my own and also a lot of learning even within my own field that we had to do to, kind of, carry this project out. So, I mean, it was a great and rewarding experience.</p>



<p><strong>NGUYEN: </strong>Yeah, for me, it&#8217;s kind of the opposite of Karin, right. I started as an organic chemist and then now really, one, appreciate the breadth and depth of going from a concept to a real end-to-end prototype and all the requirements that you need to get there. And then also, really the importance of having, you know, a background in computer science and really being able to understand the lingo that is used <em>in</em> multidisciplinary projects because you might say something and someone else interprets it very differently, and it&#8217;s because you&#8217;re not speaking the same language. And so that understanding that you have to really be … you have to learn a little bit of vocabulary from each person and understand how they contribute and then how <em>your</em> ideas can contribute to their ideas has been really impactful in my career here.</p>



<p><strong>SMITH: </strong>Yeah, I think the key change in approach that I took away—and I think many of us took away from the DNA Data Storage Project—was rather than starting with an academic question, we started with a vision of what we wanted to happen, and then we derived the research questions from analyzing what would need to happen in the world—what are the bottlenecks that need to be solved in order for us to achieve, you know, that goal? And this is something that we&#8217;ve taken with us into the sustainability-focused research and, you know, something that I think will affect all the research I do going forward.</p>



<p><strong>STRAUSS:</strong> Awesome. As we close, let&#8217;s reflect a bit on what a world in which DNA data storage is widely used might look like. If everything goes as planned, what do you hope the lasting impact of this work will be? Sergey, why don’t you lead us off.</p>



<p><strong>YEKHANIN:</strong> Sure, I remember that, like, when … in the early days when I started working on this project actually, you, Karin, told me that you were taking an Uber ride somewhere and you were talking to the taxi driver, and the taxi driver—I don&#8217;t know if you remember that—but the taxi driver mentioned that he has a camera which is recording everything that&#8217;s happening in the car. And then you had a discussion with him about, like, how long does he keep the data, how long does he keep the videos. And he told you that he keeps it for about a couple of days because it&#8217;s too expensive. But otherwise, like, if it weren&#8217;t that expensive, he would keep it for much, much longer because, like, he wants to have these recordings if later somebody is upset about the ride and, I don’t know, he is getting sued or something. So this is, like, this is one small narrow application area where DNA data storage would clearly, kind of, if it happens, then it will solve it. Because then, kind of, this long-term archival storage will become very cheap, available to everybody; it would become a commodity basically. There are many things that will be enabled, like this helping the Uber drivers, for instance. But also one has to think of, of course, like, about, kind of, the broader implications so that we don&#8217;t get into something negative because again this power of recording everything and storing everything, it can also lead to some use cases that might be, kind of, morally wrong. So, again, hopefully by the time that we get to, like, really wide deployments of this technology, the regulation will also be catching up and the, like, we will have great use cases and we won’t have bad ones. I mean, that&#8217;s how I think of it. But definitely there are lots of, kind of, great scenarios that this can enable.</p>



<p><strong>SMITH: </strong>Yeah. I&#8217;ll grab onto the word you use there, which is making DNA a <em>commodity</em>. And one of the things that I hope comes out of this project, you know, besides all the great benefits of DNA data storage itself is spillover benefits into the field of health—where if we make DNA synthesis at large scale truly a commodity thing, which I hope some of the work that we&#8217;ve done to really accelerate the throughput of synthesis will do—then this will open new doors in what we can do in terms of gene synthesis, in terms of, like, fundamental biotech research that will lead to that next set of drugs and, you know, give us medications or treatments that we could not have thought possible if we were not able to synthesize DNA and related molecules at that scale.</p>



<p><strong>NGUYEN: </strong>So much information gets lost because of just time. And so I think being able to recover really ancient history that humans wrote in the future, I think, is something that I really hope could be achieved because we&#8217;re so information rich, but in the course of time, we become information poor, and so I would like for our future generations to be able to understand the life of, you know, an everyday 21st-century person.</p>



<p><strong>STRAUSS:</strong> Well, Bichlien, Jake, Sergey, it&#8217;s been fun having this conversation with you today and collaborating with you in all of this amazing project [MUSIC] and all the research we&#8217;ve done together. Thank you so much.</p>



<p><strong>YEKHANIN:</strong>&nbsp;Thank you, Karin.</p>



<p><strong>SMITH: </strong>Thank you.</p>



<p><strong>NGUYEN: </strong>Thanks.</p>



<p>[MUSIC FADES]</p>

				</span>
			</div>
			<button
				class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle"
				aria-expanded="false"
				data-show-less-text="Show less"
				type="button"
				aria-controls="show-more-show-less-toggle-10"
				aria-label="Show more content"
				data-alternate-aria-label="Show less content">
				Show more			</button>
		</div>
	</div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p><a id="_ftn1" href="#_ftnref1">[1]</a> The team presented the findings from their life cycle assessment of DNA data storage in the paper <a href="https://www.microsoft.com/en-us/research/publication/architecting-datacenters-for-sustainability-greener-data-storage-using-synthetic-dna/">Architecting Datacenters for Sustainability: Greener Data Storage using Synthetic DNA</a>.</p>



<p><a href="#_ftnref2" id="_ftn2">[2]</a> For more information, check out the podcast episode <a href="https://www.microsoft.com/en-us/research/podcast/collaborators-sustainable-electronics-with-jake-smith-and-aniruddh-vashisth/">Collaborators: Sustainable electronics with Jake Smith and Aniruddh Vashisth</a> and the paper <a href="https://www.microsoft.com/en-us/research/publication/recyclable-vitrimer-based-printed-circuit-boards-for-sustainable-electronics/">Recyclable vitrimer-based printed circuit boards for sustainable electronics</a>.</p>



<p><a href="#_ftnref3" id="_ftn3">[3]</a> For more information, check out the podcast episode <a href="https://www.microsoft.com/en-us/research/podcast/collaborators-renewable-energy-storage-with-bichlien-nguyen-and-david-kwabi/">Collaborators: Renewable energy storage with Bichlien Nguyen and David Kwabi</a>.</p>



<p><a href="#_ftnref4" id="_ftn4">[4]</a> For more information, check out the paper <a href="https://www.microsoft.com/en-us/research/publication/mofdiff-coarse-grained-diffusion-for-metal-organic-framework-design/?msockid=3be03cfe91b667f415c429e2900566db">MOFDiff: Coarse-grained Diffusion for Metal-Organic Framework Design</a>.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/podcast/ideas-the-journey-to-dna-data-storage/">Ideas: The journey to DNA data storage</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
	</channel>
</rss>
