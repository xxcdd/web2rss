<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Microsoft Research</title>
	<atom:link href="https://www.microsoft.com/en-us/research/feed/" rel="self" type="application/rss+xml" />
	<link>https://www.microsoft.com/en-us/research/</link>
	<description></description>
	<lastBuildDate>Wed, 27 Nov 2024 20:28:25 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.6.2</generator>
	<item>
		<title>Advances in run-time strategies for next-generation foundation models</title>
		<link>https://www.microsoft.com/en-us/research/blog/advances-in-run-time-strategies-for-next-generation-foundation-models/</link>
		
		<dc:creator><![CDATA[Alyssa Hughes (2ADAPTIVE LLC dba 2A Consulting)]]></dc:creator>
		<pubDate>Wed, 27 Nov 2024 14:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1106850</guid>

					<description><![CDATA[<p>Discover the most effective run-time strategies on the OpenAI o1-preview model, improving accuracy in medical language tasks.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/advances-in-run-time-strategies-for-next-generation-foundation-models/">Advances in run-time strategies for next-generation foundation models</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img fetchpriority="high" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MedPrompt-BlogHeroFeature-1400x788-1.png" alt="A visual illustration of Medprompt performance on the MedQA benchmark. Moving from left to right on a horizontal line, the illustration shows how different Medprompt components and additive contributions improve accuracy starting with zero-shot at 81.7 accuracy, to random few-shot at 83.9 accuracy, to random few-shot, chain-of-thought at 87.3 accuracy, to kNN, few-shot, chain-of-thought at 88.4 accuracy, to ensemble with choice shuffle at 90.2 accuracy. " class="wp-image-1106865" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MedPrompt-BlogHeroFeature-1400x788-1.png 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MedPrompt-BlogHeroFeature-1400x788-1-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MedPrompt-BlogHeroFeature-1400x788-1-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MedPrompt-BlogHeroFeature-1400x788-1-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MedPrompt-BlogHeroFeature-1400x788-1-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MedPrompt-BlogHeroFeature-1400x788-1-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MedPrompt-BlogHeroFeature-1400x788-1-240x135.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MedPrompt-BlogHeroFeature-1400x788-1-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MedPrompt-BlogHeroFeature-1400x788-1-960x540.png 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MedPrompt-BlogHeroFeature-1400x788-1-1280x720.png 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure>



<p>Groundbreaking advancements in frontier language models are progressing rapidly, paving the way for boosts in accuracy and reliability of generalist models, making them highly effective in specialized domains. As part of our ongoing exploration of foundation model capabilities, we developed <a href="https://www.microsoft.com/en-us/research/blog/the-power-of-prompting/?msockid=06148b71b49b652837109fc1b5b66432" target="_blank" rel="noreferrer noopener">Medprompt</a> last year—a novel approach to maximize model performance on specialized domain and tasks without fine-tuning. By leveraging multiphase prompting, Medprompt optimizes inference by identifying the most effective chain-of-thought (CoT) examples at run time and<em> </em>drawing on multiple calls to refine output. When deployed with GPT-4, Medprompt achieved an impressive 90.2% accuracy on the MedQA benchmark (USMLE-style), outperforming all other methods.&nbsp;</p>



<figure class="wp-block-image aligncenter size-full"><img decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/llm_timeline1-Fig1-1400x788-1.jpg" alt="A line chart that plots the MedQA test accuracy (y-axis) over time (x-axis).  

Open AI o1-preview model achieves the highest result at 96.0% accuracy followed by Med-Gemini at 91.1%; GPT-4 (Medprompt) at 90.2%; Med PaLM 2 at 86.5; GPT-4 base at 86.1; Med PaLM at 67.2; GPT-3.5 base at 60.2, BioMedLM at 50.3; DRAGON at 47.5; BioLinkBERT at 45.1; PubMedBERT at 38.1.  " class="wp-image-1107318" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/llm_timeline1-Fig1-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/llm_timeline1-Fig1-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/llm_timeline1-Fig1-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/llm_timeline1-Fig1-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/llm_timeline1-Fig1-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/llm_timeline1-Fig1-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/llm_timeline1-Fig1-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/llm_timeline1-Fig1-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/llm_timeline1-Fig1-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/llm_timeline1-Fig1-1400x788-1-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">Figure 1. Comparative analyses of performance of multiple models on MedQA.</figcaption></figure>



<p>Less than a year later, our tests show the OpenAI o1-preview demonstrated superior performance over Medprompt, reaching 96% on the same benchmark (Figure 1)—without using sophisticated prompt guidance and control. This advancement is driven by the model’s integration of run-time strategies at its core, enabling state-of-the-art results on medical licensing exams in the United States and Japan, medical subsets of the Massive Multitask Language Understanding (MMLU) benchmark, and nursing exams (NCLEX) as shown in Figure 2.&nbsp;</p>



<figure class="wp-block-image aligncenter size-full"><img decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/main_radar_70_1011-Fig2-1400x788-1.jpg" alt="A spider web chart plotting the performance of OpenAI o1-preview (0 shot ensemble) compared to GPT-4 (Medprompt) and GPT-4 (5 shot) model performance on medical challenge problems. o1-preview achieves state-of-the-art results on MedQA US (4-option), JMLE-2024, MedMCQA Dev, MMLU Anatomy, MMLU Medical Genetics, MMLU Professional Medicine, MMLU College Biology, and MMLU College Medicine, and NCLEX. GPT-4 (Medprompt) performed better than OpenAI o1-preview (0 shot ensemble) on MMLU Clinical Knowledge" class="wp-image-1107330" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/main_radar_70_1011-Fig2-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/main_radar_70_1011-Fig2-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/main_radar_70_1011-Fig2-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/main_radar_70_1011-Fig2-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/main_radar_70_1011-Fig2-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/main_radar_70_1011-Fig2-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/main_radar_70_1011-Fig2-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/main_radar_70_1011-Fig2-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/main_radar_70_1011-Fig2-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/main_radar_70_1011-Fig2-1400x788-1-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">Figure 2. Comparisons on a wide range of medical challenge benchmarks.</figcaption></figure>



<p>These results are notable, prompting us to publish our recent study, findings, and analyses, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/abs/2411.03590" target="_blank" rel="noreferrer noopener"><em>From Medprompt to o1: Exploration of Run-Time Strategies for Medical Challenge Problems and Beyond</em><span class="sr-only"> (opens in new tab)</span></a>. But the numbers are only part of the story. In this blog, we discuss prompting strategies to make the most of o1-preview models and other factors to consider as well as directions forward for run-time strategies.</p>



<h2 class="wp-block-heading" id="is-o1-preview-just-fancy-prompting">Is o1-preview “just” fancy prompting?&nbsp;</h2>



<p>The introduction of the OpenAI o1 model series marks a significant shift from prior GPT models. Unlike GPT, o1 models are trained using reinforcement learning (RL) techniques that enable them to “think” before generating outputs. While Medprompt relies on a cascade of operations with GPT-4 at run time guided by a multistage prompt, the o1 series incorporates this run-time reasoning directly into its RL-based design. The built-in functionality enables the o1 models to significantly outperform even the best results using GPT-4 and Medprompt. The performance gains come with a notable tradeoff: its per-token cost was approximately six times that of GPT-4o at the time of our evaluation. While the results for GPT-4o with Medprompt fall short of o1-preview model performance, the combination offers a more cost-effective alternative. The cost-benefit tradeoffs are highlighted in the following figure, with the x-axis presented on a logarithmic scale.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/cost_frontier1-Fig-3-1400x788-1.jpg" alt="A line chart plotting accuracy on the MedQA Test (y-axis) versus total cost on a logarithmic scale (x-axis). OpenAI o1-preview using 5x, 10x, and 15x Ensemble hover around 1000 total cost. OpenAI o1-preview using Tailored Prompt, Minimal Prompt, Few-shot, kNN Few-shot are around 100 total cost. GPT-4o with Medprompt is below 100; kNN Few-shot CoT, Few-shot CoT, and Few-Shot are at 10; Zero-shot is at 1. GPT-4-Turbo with Medprompt is at 200; kNN Few-shot CoT, Few-shot CoT, and Few-Shot hover near 50, Zero-shot is near 5. " class="wp-image-1107345" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/cost_frontier1-Fig-3-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/cost_frontier1-Fig-3-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/cost_frontier1-Fig-3-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/cost_frontier1-Fig-3-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/cost_frontier1-Fig-3-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/cost_frontier1-Fig-3-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/cost_frontier1-Fig-3-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/cost_frontier1-Fig-3-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/cost_frontier1-Fig-3-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/cost_frontier1-Fig-3-1400x788-1-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">Figure 3. Pareto frontier showing accuracy versus total API cost (log scale) on the MedQA benchmark (1273 questions total). o1-preview (Sep 2024) is compared with GPT-4o (Aug 2024) and GPT-4 Turbo (Nov 2023).</figcaption></figure>



<h2 class="wp-block-heading" id="can-we-prompt-engineer-o1-preview">Can we prompt engineer o1-preview?</h2>



<p>The o1-preview model exhibits distinct run-time behaviors compared to the GPT series. While some of our more dynamic prompting strategies performed better than expected with o1-preview models, our most tried-and-true strategy was anything but consistent throughout our evaluation. Figure 4 captures specific performance results for Tailored Prompt, Ensembling, and Few-Shot Prompting on o1-preview. Here’s a summary of our findings:&nbsp;</p>



<ol start="1" class="wp-block-list">
<li><strong>Tailored Prompt</strong>: While minimal prompting—like a brief, one-sentence description followed by a question—offered a strong baseline performance, detailed task descriptions were best for eliciting accurate responses.</li>



<li><strong>Ensembling</strong>: Generating multiple answers per question and using majority voting across different reasoning paths boosted reliability, while shuffling answers in runs produced richer reasoning chains and improved outcomes. Ensembling continues to yield consistent performance improvements.</li>



<li><strong>Few-Shot Prompting</strong>: Guiding the model with a few examples produced inconsistent results and, on average, decreased performance compared with GPT models.</li>
</ol>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1400" height="699" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/experiment_slope_chart_nomedmcqa2-Fig4-1400x788-1.jpg" alt="Three charts show the accuracy of o1-preview when combined with Tailored Prompt, Ensemble, and 5-shot KNN based on an average baseline of medical benchmarks. Tailored Prompts improves accuracy from 94.2 to 94.7; Ensemble (15x) improves accuracy from 94.2 to 95.5; 5-shot KNN decreases accuracy from 94.2 to 93.7.  " class="wp-image-1107585" style="object-fit:cover" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/experiment_slope_chart_nomedmcqa2-Fig4-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/experiment_slope_chart_nomedmcqa2-Fig4-1400x788-1-300x150.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/experiment_slope_chart_nomedmcqa2-Fig4-1400x788-1-1024x511.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/experiment_slope_chart_nomedmcqa2-Fig4-1400x788-1-768x383.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/experiment_slope_chart_nomedmcqa2-Fig4-1400x788-1-240x120.jpg 240w" sizes="(max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">Figure 4. Tests of different prompting strategies across benchmark datasets.</figcaption></figure>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1061244">
		

	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/about-microsoft-research/" aria-label="About Microsoft Research" data-bi-cN="About Microsoft Research" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/07/About-page-promo_1066x600.jpg" alt="" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">About Microsoft Research</h2>
				
								<p class="large">Advancing science and technology to benefit humanity</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://www.microsoft.com/en-us/research/about-microsoft-research/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="View our story" data-bi-cN="About Microsoft Research" target="_blank">
							View our story						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="do-results-stand-in-another-language">Do results stand in another language?&nbsp;</h2>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/jmle-2024-bar1-BlogHeroFeature-1400x788-1.jpg" alt="A chart with two bar charts measuring the accuracy (y-axis) by short and long questions (x-axis) on the Japanese Medical Licensing Examination. The short question bar is slightly higher than the long question ratio for o1-preview (0-shot ensemble). The short question bar is about two points less accurate than the long question bar for o1-preview (0-shot). The short answer bar is a point more accurate than the long question bar for GPT-4o (Medprompt). The short question bar is one point less accurate than the long question bar for GPT-4o (0 shot). " class="wp-image-1107453" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/jmle-2024-bar1-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/jmle-2024-bar1-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/jmle-2024-bar1-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/jmle-2024-bar1-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/jmle-2024-bar1-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/jmle-2024-bar1-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/jmle-2024-bar1-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/jmle-2024-bar1-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/jmle-2024-bar1-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/jmle-2024-bar1-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">Figure 5. JMLE-2024: National medical licensing exam held in Japan (Feb 2024).</figcaption></figure>



<p>We expanded our research to include a new multilingual benchmark based on the Japanese national medical licensing exam.&nbsp;The JMLE (Japanese Medical Licensing Examination) is written in Japanese and administered in February 2024, after the o1-preview model’s knowledge cutoff. <em>Even without translation to English, the o1-preview model achieved a remarkable score of 98.2% accuracy (Figure 5), well above the exam’s minimum passing score of approximately 80%.&nbsp;</em>&nbsp;</p>



<h2 class="wp-block-heading" id="do-reasoning-tokens-improve-performance">Do reasoning tokens improve performance?&nbsp;</h2>



<p>For fun, we conducted tests to determine whether increasing the number of reasoning tokens could improve performance. Our findings showed that by adjusting the prompt, we were able to consistently increase the number of reasoning tokens used by o1-preview, and the increase was directly correlated with improved performance as demonstrated in Figure 6.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/reasoning_token_slope_chart2-Fig6-1400x788-1.jpg" alt="A chart plotting the impact of reasoning tokens on accuracy. JMLE achieved 95.3% accuracy for Quick Response Prompt and 96.7% accuracy for Extended Reasoning Prompt. MMLU achieved 94.9% accuracy for Quick Response Prompt and 94.7% accuracy for Extended Reasoning Prompt. MedQA achieved 94.3% accuracy for Quick Response Prompt and 95.1% accuracy for Extended Reasoning Prompt. USMLE Sample Exam achieved 92.6% accuracy for Quick Response Prompt and 93.1% accuracy for Extended Reasoning Prompt. USMLE Self Assessment achieved 91.8% accuracy for Quick Response Prompt and 92.2% accuracy for Extended Reasoning Prompt. " class="wp-image-1107408" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/reasoning_token_slope_chart2-Fig6-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/reasoning_token_slope_chart2-Fig6-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/reasoning_token_slope_chart2-Fig6-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/reasoning_token_slope_chart2-Fig6-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/reasoning_token_slope_chart2-Fig6-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/reasoning_token_slope_chart2-Fig6-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/reasoning_token_slope_chart2-Fig6-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/reasoning_token_slope_chart2-Fig6-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/reasoning_token_slope_chart2-Fig6-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/reasoning_token_slope_chart2-Fig6-1400x788-1-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">Figure 6. The effect of two prompting strategies that elicit variable length reasoning chains across benchmark datasets.</figcaption></figure>



<h2 class="wp-block-heading" id="what-s-the-takeaway">What’s the takeaway?&nbsp;</h2>



<p>Bottom line: There’s a little something for everyone when it comes to run-time strategies. We’re excited by the performance gains from GPT models to o1-preview models. While these improvements are significant, so is the cost. For those needing proven accuracy on a budget, Medprompt leveraging calls to GPT-4 is a viable option for medicine and beyond. We summarize the relative performance of prompting strategies in Figure 7 to determine the best option, or <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/abs/2411.03590" target="_blank" rel="noreferrer noopener">check out the paper for a detailed breakdown of every dataset, experimental configuration, and prompt template<span class="sr-only"> (opens in new tab)</span></a>.&nbsp;</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/performance_heatmap2-Fig7-1400x788-1.jpg" alt="A matrix that shows the relative performance of prompting strategies over baseline medical benchmarks. The top row from left to right are the results for baseline numbers: JMLE = 95.6%; MMLU = 94.6%; MedMCQA = 81.4%; MedQA = 94.9%; USMLE Sample Exam = 94.0%; USMLE Self Assessment = 91.8%. The second row from left to right, 5-shot Random baseline difference: JMLE = +1.2%; MMLU = -1.1%; MedMCQA = 0.0%; MedQA = -1.4%; USMLE Sample Exam = -0.4%; USMLE Self Assessment = -1.0%. The third row from left to right, 5-shot KNN baseline difference: JMLE = +0.6%; MMLU = -0.1%; MedMCQA = +1.2%; MedQA = -2.2%; USMLE Sample Exam = -0.3%; USMLE Self Assessment = -0.6%. The fourth row from left to right, Bootstrap Ensemble (5x) baseline difference: JMLE = +1.5%; MMLU = +0.1%; MedMCQA = +1.3%; MedQA = +0.7%; USMLE Sample Exam = +1.3%; USMLE Self Assessment = +1.0%. The fifth row from left to right, Bootstrap Ensemble (10x) baseline difference: JMLE = +1.4%; MMLU = +0.6%; MedMCQA = +1.5%; MedQA = +0.7%; USMLE Sample Exam = +1.3%; USMLE Self Assessment = +1.1%. The sixth row from left to right, Ensemble (15x) baseline difference: JMLE = +1.5%; MMLU = +0.6%; MedMCQA = +2.0%; MedQA = +1.1%; USMLE Sample Exam = +2.0%; USMLE Self Assessment = +1.3%. The seventh row from left to right, Tailored Prompt baseline difference: JMLE = +1.6%; MMLU = +0.4%; MedMCQA = +0.9%; MedQA = +0.2%; USMLE Sample Exam = +0.0%; USMLE Self Assessment = +0.4%. The eighth row from left to right, Tailored Bootstrap Ensemble (5x) baseline difference: JMLE = +2.2%; MMLU = +0.7%; MedMCQA = +1.8%; MedQA = +0.8%; USMLE Sample Exam = +0.9%; USMLE Self Assessment = +1.1%. The ninth row from left to right, Tailored Bootstrap Ensemble (10x) baseline difference: JMLE = +2.3%; MMLU = +0.7%; MedMCQA = +2.1%; MedQA = +0.9%; USMLE Sample Exam = +0.9%; USMLE Self Assessment = +1.2%. The tenth row from left to right, Tailored Ensemble (15x) baseline difference: JMLE = +2.5%; MMLU = +0.4%; MedMCQA = +2.6%; MedQA = +1.1%; USMLE Sample Exam = +0.9%; USMLE Self Assessment = +1.4%.   " class="wp-image-1107396" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/performance_heatmap2-Fig7-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/performance_heatmap2-Fig7-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/performance_heatmap2-Fig7-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/performance_heatmap2-Fig7-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/performance_heatmap2-Fig7-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/performance_heatmap2-Fig7-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/performance_heatmap2-Fig7-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/performance_heatmap2-Fig7-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/performance_heatmap2-Fig7-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/performance_heatmap2-Fig7-1400x788-1-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">Figure 7. Heatmap showing absolute accuracy and relative performance over baseline zero-shot prompt (in parenthesis) across all benchmark datasets.</figcaption></figure>



<h2 class="wp-block-heading" id="anything-more-to-consider">Anything more to consider?</h2>



<p>We highlighted several considerations in the paper that are worth checking out. Here are three opportunities that are top of mind:</p>



<ul class="wp-block-list">
<li><em>Research on run-time strategies</em>. The research community has largely relied on boosting model capabilities with data, compute, and model size, predictably achieving gains by way of scaling laws. A promising new direction is inference-time scaling—the value of investing in additional computation and machinery for guiding inference at run time. We highlight in the paper opportunities to guide run-time allocations to boost efficiency, accuracy, and intellectual capabilities, including meta reasoning and reflection in real time and learning <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://erichorvitz.com/cc_aij_horvitz.pdf">during the “idle” time<span class="sr-only"> (opens in new tab)</span></a> between problem solving. We see a great deal of opportunity for new research and development on real-time and “offline” reasoning, learning, and reflection.</li>



<li><em>Benchmark saturation</em>. With the rapid advancement of state-of-the-art models, many existing medical benchmarks are reaching “saturation,” where models perform extremely well on standing medical competency challenges, considered extremely difficult just a few years ago. Current benchmarks, such as USMLE and JMLE, were designed to assess the performance of medical students and clinicians and are increasingly inadequate for evaluating cutting-edge AI models. To drive understandings of models and guide research, we need to design more challenging medical benchmarks. </li>



<li><em>From benchmarks to clinical applications.</em> We note that, while benchmarks offer valuable insights into performance and accuracy, they often fail to capture the complexities and nuances of real-world clinical decision making and healthcare delivery, more broadly. Conducting clinical trials to rigorously evaluate the impact of AI applications on patient care poses far greater difficulties than benchmarking models against challenge problems drawn from medical competency exams. Yet, studies of AI deployments in realistic clinical settings are essential for understanding model capabilities and for guiding the effective integration of AI into healthcare.</li>
</ul>



<h2 class="wp-block-heading" id="resources">Resources&nbsp;</h2>



<ul class="wp-block-list">
<li><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/abs/2411.03590">From Medprompt to o1: Exploration of Run-Time Strategies for Medical Challenge Problems and Beyond<span data-contrast="auto" xml:lang="EN-US" lang="EN-US" class="TextRun EmptyTextRun SCXW11105993 BCX8" style="-webkit-user-drag: none; -webkit-tap-highlight-color: transparent; margin: 0px; padding: 0px; user-select: text; font-size: 11pt; line-height: 19.6917px; font-family: Aptos, Aptos_EmbeddedFont, Aptos_MSFontService, sans-serif; font-variant-ligatures: none !important;"></span>&nbsp;<span class="sr-only"> (opens in new tab)</span></a></li>



<li><a href="https://www.microsoft.com/en-us/research/publication/can-generalist-foundation-models-outcompete-special-purpose-tuning-case-study-in-medicine/">Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine</a>&nbsp;</li>



<li><a href="https://www.microsoft.com/en-us/research/blog/the-power-of-prompting/">The Power of Prompting</a></li>



<li><a href="https://www.microsoft.com/en-us/research/blog/steering-at-the-frontier-extending-the-power-of-prompting/">Steering at the Frontier: Extending the Power of Prompting</a></li>



<li><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/abs/2303.13375" target="_blank" rel="noreferrer noopener">Capabilities of GPT-4 on Medical Challenge Problems<span class="sr-only"> (opens in new tab)</span></a></li>
</ul>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/advances-in-run-time-strategies-for-next-generation-foundation-models/">Advances in run-time strategies for next-generation foundation models</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Accelerating drug discovery with TamGen: A generative AI approach to target-aware molecule generation</title>
		<link>https://www.microsoft.com/en-us/research/blog/accelerating-drug-discovery-with-tamgen-a-generative-ai-approach-to-target-aware-molecule-generation/</link>
		
		<dc:creator><![CDATA[Alyssa Hughes (2ADAPTIVE LLC dba 2A Consulting)]]></dc:creator>
		<pubDate>Mon, 25 Nov 2024 21:02:26 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1105683</guid>

					<description><![CDATA[<p>TamGen uses generative AI to design new drug candidate compounds to treat TB, going beyond traditional methods by generating novel chemical structures. Learn how a collaboration with the Global Health Drug Discovery Institute is making this possible.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/accelerating-drug-discovery-with-tamgen-a-generative-ai-approach-to-target-aware-molecule-generation/">Accelerating drug discovery with TamGen: A generative AI approach to target-aware molecule generation</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen-BlogHeroFeature-1400x788-1.jpg" alt="TamGen logo on a gradient background." class="wp-image-1105698" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure>



<p>The <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="http://www.ghddi.org/" target="_blank" rel="noreferrer noopener">Global Health Drug Discovery Institute<span class="sr-only"> (opens in new tab)</span></a> (GHDDI) and Microsoft Research have reached a milestone in tuberculosis (TB) drug research with <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.nature.com/articles/s41467-024-53632-4" target="_blank" rel="noreferrer noopener">TamGen<span class="sr-only"> (opens in new tab)</span></a>, an <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/TamGen" target="_blank" rel="noreferrer noopener">open-source<span class="sr-only"> (opens in new tab)</span></a>, transformer-based chemical language model for developing target-specific drug compounds. Working in collaboration, the joint team successfully identified several promising inhibitors for a TB protease, with the most effective compound showing significant bioactivity. Research shows that TamGen can also optimize existing molecules by designing target-aware molecule fragments, potentially enabling the discovery of novel compounds that build on a known molecular core structure.&nbsp;&nbsp;</p>



<h2 class="wp-block-heading" id="generative-ai-helps-overcome-limitations-in-drug-discovery">Generative AI helps overcome limitations in drug discovery</h2>



<p>Generative AI is opening new avenues for scientific exploration by allowing computers to autonomously learn and produce original content. TamGen offers a new approach to drug discovery by applying the principles of generative AI to molecular design. Unlike traditional methods, which depend on systematically screening known compounds—a process that is long, complex, and costly due to its reliance on empirical knowledge and the time-consuming task of exploring a vast chemical library—generative AI provides opportunities for designing entirely new chemical structures.&nbsp;&nbsp;</p>



<p>TamGen goes beyond analyzing existing data by generating chemically diverse compounds that conventional approaches might miss. Figure 1 shows that generative AI expands chemical exploration, allowing for a deeper and more comprehensive search for therapeutic solutions compared to traditional methods.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="2560" height="1441" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGenFig1_1400x788-scaled.jpg" alt="Two funnels that illustrate compared to the traditional screening-based approach, the generative AI-based approach enables the exploration of a broader range of novel compounds. The table below the two funnels has two rows labeled “Valuable cmpd%” and “Novel cmpd” to compare “Screening-based” approach and “Generative-based” approach. The “Screening-based” column shows “Low”, “No”, “High” verse to “Generative-based” shows “High” “Yes”, “Low”" class="wp-image-1106838" style="object-fit:cover" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGenFig1_1400x788-scaled.jpg 2560w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGenFig1_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGenFig1_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGenFig1_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGenFig1_1400x788-1536x865.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGenFig1_1400x788-2048x1153.jpg 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGenFig1_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGenFig1_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGenFig1_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGenFig1_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGenFig1_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGenFig1_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGenFig1_1400x788-1920x1080.jpg 1920w" sizes="(max-width: 2560px) 100vw, 2560px" /><figcaption class="wp-element-caption">Figure 1. Compared with the traditional screening-based approach to drug discovery, a generative AI-based approach enables the discovery of novel compounds. </figcaption></figure>



<h2 class="wp-block-heading" id="tamgen-workflow">TamGen workflow&nbsp;</h2>



<p>TamGen’s workflow uses generative AI to design target-specific chemical compounds. Building on the success of large language models (LLMs), we adapted a similar approach for molecular generation, using a training method like that of GPT models, which involves next-token prediction. Molecules were first converted into a simplified molecular input line entry system (SMILES)—a notation representing molecular structures as symbol sequences, similar to text. We then developed a protein encoder to process information about proteins, including their 3D structure.&nbsp;&nbsp;</p>



<p>A contextual encoder<strong> </strong>combines insights from medical professionals with data on the protein target and existing compounds that have proven to be effective or promising. Using expert knowledge and computational analysis, this encoder guides the compound generator to produce new molecules that are more likely to bind to a given protein. This workflow is illustrated in Figure 2.&nbsp;</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1400" height="1468" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig2-v2.jpg" alt="The protein encoder process proteins information, including 3D information, to help with protein representation. The contextual encoder captures protein-ligand information to generate new molecules bind to proteins. Then the new molecules will be converted into SMILES and pre-trained a GPT-like model to generate molecular compounds." class="wp-image-1106658" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig2-v2.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig2-v2-286x300.jpg 286w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig2-v2-977x1024.jpg 977w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig2-v2-768x805.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig2-v2-172x180.jpg 172w" sizes="(max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">Figure 2. TamGen’s workflow&nbsp;</figcaption></figure>



<h2 class="wp-block-heading" id="evaluating-tamgen-computationally">Evaluating TamGen computationally&nbsp;</h2>



<div class="annotations " data-bi-aN="margin-callout">
	<ul class="annotations__list card depth-16 bg-body p-4 annotations__list--right">
		<li class="annotations__list-item">
						<span class="annotations__type d-block text-uppercase font-weight-semibold text-neutral-300 small">Download</span>
			<a href="https://github.com/microsoft/TamGen" target="_self" class="annotations__link font-weight-semibold text-decoration-none" data-bi-type="annotated-link" aria-label="TamGen" data-bi-aN="margin-callout" data-bi-cN="TamGen">
				TamGen&nbsp;<span class="glyph-append glyph-append-share glyph-append-xsmall"></span>
			</a>
					</li>
	</ul>
</div>



<p>To evaluate TamGen’s performance, we compared it to five other common methods used to create 3D shapes of molecules intended to bind to certain proteins. We evaluated these methods using the CrossDocked benchmark, a dataset used in AI research to assess the quality of molecule generation conditioned on a target protein.</p>



<p>Evaluation metrics:&nbsp;</p>



<ul class="wp-block-list">
<li><strong>Docking score</strong>: Measures how well a molecule binds to a target protein. </li>
</ul>



<ul class="wp-block-list">
<li><strong>Quantitative estimate of drug-likeness (QED)</strong>: Assesses how good a candidate a molecule is for a drug. </li>
</ul>



<ul class="wp-block-list">
<li><strong>Synthesis accessibility score (SAS)</strong>:<strong> </strong>Measures how easy or difficult it is to synthesize a particular chemical compound in a lab. </li>
</ul>



<ul class="wp-block-list">
<li><strong>Ro5 (Lipinski&#8217;s rule of five)</strong>: Determines how likely a compound can be developed into an oral drug.  </li>
</ul>



<ul class="wp-block-list">
<li><strong>LogP</strong>: Tests a compound&#8217;s ability to move between water and fats. </li>
</ul>



<ul class="wp-block-list">
<li><strong>Diversity</strong>: Measures the range of different molecular structures and properties in a collection of compounds.  </li>
</ul>



<p>The findings, illustrated in Figure 3, show TamGen’s overall performance. While other methods may produce compounds that bind more strongly, they often include multiple interconnected ring structures. Research indicates that more of these structures can lower synthesis accessibility (SAS) and increase cellular toxicity, making these compounds harder to develop. We believe that molecular pretraining of the model contributed to the overall effectiveness of the compounds TamGen generated.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1400" height="1074" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig3-v2.jpg" alt="The figure uses the shade of the color to illustrate the performance of “Docking”, “QED”, “Lipinski”, “SAS”, “logP”, “Diversity” between TamGen 5 other methods including “Pocket2Mol”, “ResGen”, “TargeDiff”, “3D-AR”, “LiGAN”. The results show that TamGen achieved the best overall performance." class="wp-image-1106661" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig3-v2.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig3-v2-300x230.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig3-v2-1024x786.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig3-v2-768x589.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig3-v2-80x60.jpg 80w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig3-v2-235x180.jpg 235w" sizes="(max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">Figure 3. Results from TamGen’s computational performance verification</figcaption></figure>



<h2 class="wp-block-heading" id="experimental-lab-verification">Experimental lab verification&nbsp;</h2>



<p>To ensure real-world applicability, we also validated our findings in a hands-on lab environment. Here, we focused on the ClpP protease in <em>Mycobacterium tuberculosis</em> as the target because it plays a significant role in the bacterium’s survival under stress conditions. We proposed the Design-Refine-Test pipeline to effectively identify molecular compounds for TB drug discovery.</p>



<p><strong>Design stage</strong>: We began by using TamGen to analyze the <em>binding pocket</em> of the protease, where molecules can attach and influence its function. TamGen generated about 2,600 potential compounds that could fit into this pocket. We assessed these compounds based on how well they could attach to the protease and their predicted biological effects, narrowing it down to four promising candidates. </p>



<p><strong>Refine stage</strong>: Next, we entered the four compounds into TamGen, along with three molecular fragments that had been validated in previous lab experiments. This generated a total of 8,600 new compounds, which we screened again using the same criteria, eventually narrowing the selection to 296 compounds.</p>



<p><strong>Test stage</strong>: Because synthesizing all 296 compounds wasn’t feasible, we identified similar compounds available in commercial libraries and tested their initial activity against TB. Five compounds showed promising results. We then synthesized one of the originals and two variants of another. Additionally, we categorized the generated compounds into clusters, selected the top 10% from each cluster based on docking scores, and after manual review, synthesized eight more compounds. </p>



<p>The team from Microsoft Research generated the compounds by TamGen, and the GHDDI team conducted binding analysis, structure–activity relationship studies, and lab experiments to verify the compounds’ inhibitory effect on the ClpP protease, measuring their capacity to interfere with or reduce its activity. Lower IC50 values signify greater potency. Out of the 16 compounds tested, 14 showed strong inhibitory activity measuring under 40 µM, indicating high potential. The most effective compound had a measured IC50 value of 1.88 µM.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1400" height="677" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig4-v2.jpg" alt="The figure shows step by step the process and results, as the context described, of how TamGen analyzes the binding pocket of the protease on Design stage, Refine stage and Test stage." class="wp-image-1106655" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig4-v2.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig4-v2-300x145.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig4-v2-1024x495.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig4-v2-768x371.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/TamGen_blog_Fig4-v2-240x116.jpg 240w" sizes="(max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">Figure 4. The hands-on lab verification process  </figcaption></figure>



<h2 class="wp-block-heading" id="from-molecule-to-fragment-generation">From molecule to fragment generation&nbsp;</h2>



<p>In addition to generating new molecules, TamGen can optimize existing ones by designing smaller molecular fragments. In this fragment generation process, TamGen builds on a given protein target and a molecular core structure to design new compounds around that core. By incorporating information about the target protein, it generates fragments that are highly specific to the target. This approach moves beyond traditional methods that rely on pre-existing databases, which often limit both novelty and effectiveness of molecular fragments.</p>



<p>For fragment generation, we adjusted the input to TamGen’s compound generator. We modified the SMILES string to ensure it ended at the desired growth site. This was done by specifying the fragment we wanted to retain and its connection point for further growth. The tailored SMILES string was then fed into the compound generator to extend the molecule.&nbsp;</p>



<p>We evaluated this method by targeting the ClpP protease for TB, achieving a more than tenfold improvement in the binding affinity of the generated compound compared to the original. Some compounds also demonstrated slow binding, indicating potential for prolonged action and improved selectivity for the target protein.</p>



<h2 class="wp-block-heading" id="ai-s-potential-in-drug-discovery">AI’s potential in drug discovery&nbsp;</h2>



<p>TamGen showcases the transformative potential of generative AI in drug design, combining advanced molecular modeling with researcher-AI collaboration. Tasks that once took years can now be accomplished in a fraction of the time. This research underscores AI&#8217;s expanding role in drug discovery and its promise for developing effective treatments against persistent infectious diseases like TB.&nbsp;</p>



<p>Looking ahead, we plan to integrate advanced techniques into TamGen, including diffusion models for generating 3D structures, reinforcement learning to apply physical constraints, and molecular dynamics simulations to capture proteins&#8217; shifting shapes. These enhancements aim to improve how well generated compounds bind to target proteins, increase their ability to be synthesized, and strengthen other critical drug properties.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/accelerating-drug-discovery-with-tamgen-a-generative-ai-approach-to-target-aware-molecule-generation/">Accelerating drug discovery with TamGen: A generative AI approach to target-aware molecule generation</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>LazyGraphRAG: Setting a new standard for quality and cost</title>
		<link>https://www.microsoft.com/en-us/research/blog/lazygraphrag-setting-a-new-standard-for-quality-and-cost/</link>
		
		<dc:creator><![CDATA[Brenda Potts]]></dc:creator>
		<pubDate>Mon, 25 Nov 2024 17:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1106013</guid>

					<description><![CDATA[<p>Introducing a new approach to graph-enabled RAG. LazyGraphRAG needs no prior summarization of source data, avoiding prohibitive up-front indexing costs. It’s inherently scalable in cost and quality across multiple methods and search mechanisms.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/lazygraphrag-setting-a-new-standard-for-quality-and-cost/">LazyGraphRAG: Setting a new standard for quality and cost</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-large"><img loading="lazy" decoding="async" width="1024" height="576" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-BlogHeroFeature-1400x788-1-1024x576.png" alt="A white geometric network icon with interconnected nodes and lines is centered on a gradient background transitioning from blue on the left to pink on the right." class="wp-image-1106046" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-BlogHeroFeature-1400x788-1-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-BlogHeroFeature-1400x788-1-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-BlogHeroFeature-1400x788-1-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-BlogHeroFeature-1400x788-1-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-BlogHeroFeature-1400x788-1-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-BlogHeroFeature-1400x788-1-240x135.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-BlogHeroFeature-1400x788-1-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-BlogHeroFeature-1400x788-1-960x540.png 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-BlogHeroFeature-1400x788-1-1280x720.png 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-BlogHeroFeature-1400x788-1.png 1400w" sizes="(max-width: 1024px) 100vw, 1024px" /></figure>



<h2 class="wp-block-heading" id="affordable-graphrag-for-every-use-case">Affordable GraphRAG for every use case</h2>



<p>The <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://aka.ms/graphrag" target="_blank" rel="noreferrer noopener">GraphRAG project<span class="sr-only"> (opens in new tab)</span></a> aims to expand the class of questions that AI systems can answer over private datasets by leveraging the implicit relationships within unstructured text.&nbsp;</p>



<p>A key advantage of GraphRAG over conventional vector RAG (or “semantic search”) is its ability to answer <em>global</em> <em>queries</em> that address the entire dataset, such as “what are the main themes in the data?”, or “what are the most important implications for X?”. Conversely, vector RAG excels for <em>local</em> <em>queries</em> where the answer resembles the query and can be found within specific text regions, as is typically the case for “who”, “what”, “when”, and “where” questions.&nbsp;</p>



<p>In recent blog posts, we have shared two new query mechanisms that exploit the rich, summary-based data index created by GraphRAG to improve <a href="https://www.microsoft.com/en-us/research/blog/introducing-drift-search-combining-global-and-local-search-methods-to-improve-quality-and-efficiency/" target="_blank" rel="noreferrer noopener">local search performance</a> and <a href="https://www.microsoft.com/en-us/research/blog/graphrag-improving-global-search-via-dynamic-community-selection/" target="_blank" rel="noreferrer noopener">global search costs</a>, respectively.&nbsp;</p>



<p>In this blog post, we introduce a radically different approach to graph-enabled RAG that requires no prior summarization of the source data, avoiding the up-front indexing costs that may be prohibitive for some users and use cases. We call this approach “LazyGraphRAG”.&nbsp;</p>



<p>A key advantage of LazyGraphRAG is its inherent scalability in terms of both cost and quality. Across a range of competing methods (standard vector RAG, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/profintegra/raptor-rag" target="_blank" rel="noreferrer noopener">RAPTOR<span class="sr-only"> (opens in new tab)</span></a>, and GraphRAG <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://microsoft.github.io/graphrag/query/local_search/" target="_blank" rel="noreferrer noopener">local<span class="sr-only"> (opens in new tab)</span></a>, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://microsoft.github.io/graphrag/query/global_search/" target="_blank" rel="noreferrer noopener">global<span class="sr-only"> (opens in new tab)</span></a>, and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://microsoft.github.io/graphrag/query/drift_search/" target="_blank" rel="noreferrer noopener">DRIFT<span class="sr-only"> (opens in new tab)</span></a> search mechanisms), LazyGraphRAG shows strong performance across the cost-quality spectrum as follows:&nbsp;</p>



<ul class="wp-block-list">
<li>LazyGraphRAG data indexing costs are identical to vector RAG and 0.1% of the costs of full GraphRAG.&nbsp;</li>



<li>For comparable query costs to vector RAG, LazyGraphRAG outperforms all competing methods on local queries, including long-context vector RAG and GraphRAG <a href="https://www.microsoft.com/en-us/research/blog/introducing-drift-search-combining-global-and-local-search-methods-to-improve-quality-and-efficiency/" target="_blank" rel="noreferrer noopener">DRIFT</a> search (our recently introduced RAG approach shown to outperform vector RAG) as well as GraphRAG local search.&nbsp;</li>



<li>The same LazyGraphRAG configuration also shows comparable answer quality to GraphRAG Global Search for global queries, but <em>more than 700 times lower query cost</em>.</li>



<li>For 4% of the query<em> </em>cost of GraphRAG global search, LazyGraphRAG <em>significantly outperforms all competing methods</em> on both local and global query types, including GraphRAG global search at the C2 level (the third level of the community hierarchy recommended for most applications).</li>
</ul>



<p>LazyGraphRAG is coming soon to our open-source <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/graphrag" target="_blank" rel="noreferrer noopener">GraphRAG library<span class="sr-only"> (opens in new tab)</span></a>, providing a unified query interface for both local and global queries over a lightweight data index that is comparable in cost to standard vector RAG.&nbsp;</p>



<h2 class="wp-block-heading" id="blending-vector-rag-and-graph-rag-with-deferred-llm-use">Blending vector RAG and Graph RAG with deferred LLM use</h2>



<p>LazyGraphRAG aims to blend the advantages of vector RAG and Graph RAG while overcoming their respective limitations:&nbsp;</p>



<ul class="wp-block-list">
<li>Vector RAG is a form of <em>best-first</em> <em>search</em> that uses the similarity with the query to select the best-matching source text chunks. However, it has no sense of the breadth of the dataset to consider for global queries.&nbsp;</li>



<li>GraphRAG global search is a form of <em>breadth-first search</em> that uses the community structure of source text entities to ensure that queries are answered considering the full breadth of the dataset. However, it has no sense of the best communities to consider for local queries.</li>
</ul>



<p>LazyGraphRAG combines best-first and breadth-first search dynamics in an iterative deepening manner (Table 1). Compared to the global search mechanism of full GraphRAG, this approach is “lazy” in ways that defer LLM use and dramatically increase the efficiency of answer generation. Overall performance can be scaled via a single main parameter – the <em>relevance test budget </em>– that controls the cost-quality trade-off in a consistent manner.</p>



<figure class="wp-block-table"><table><thead><tr><th></th><th>GraphRAG</th><th>LazyGraphRAG</th></tr></thead><tbody><tr><td>Build index</td><td>Uses an LLM to extract and describe entities and their relationships, b) uses an LLM to summarize all observations of each entity and relationship, c) uses graph statistics to optimize the entity graph and extract hierarchical community structure</td><td>Uses NLP noun phrase extraction to extract concepts and their co-occurrences, b) uses graph statistics to optimize the concept graph and extract hierarchical community structure</td></tr><tr><td>Summarize index</td><td>Uses an LLM to summarize entities and relationships in each community</td><td>None – the “lazy” approach defers all LLM use until query time</td></tr><tr><td>Refine query</td><td>None – the original query is used throughout</td><td>Uses an LLM to a) identify relevant subqueries and recombine them into a single expanded query, b) refine subqueries with matching concepts from the concept graph</td></tr><tr><td>Match query</td><td>None – all queries are answered using all community summaries (<em>breadth first</em>)&nbsp;</td><td>For each of <em>q </em>subqueries [3-5]: <br>&#8211; Uses text chunk embeddings and chunk-community relationships to first rank text chunks by similarity to the query, then rank communities by the rank of their top-<em>k</em> text chunks (<em>best first</em>) <br>&#8211; Uses an LLM-based sentence-level relevance assessor to rate the relevance of the top-<em>k</em> untested text chunks from communities in rank order (<em>breadth first</em>) <br>&#8211; Recurses into relevant sub-communities after <em>z </em>successive communities yield zero relevant text chunks  (<em>iterative deepening</em>)<br>&#8211; Terminates when no relevant communities remain or <em>relevance test budget</em> / <em>q</em> is reached</td></tr><tr><td>Map answers</td><td>Uses an LLM to answer the original query over random batches of community summaries in parallel</td><td>For each of <em>q</em> subqueries [3-5]:&nbsp;<br>&#8211; Builds a subgraph of concepts from the relevant text chunks&nbsp;<br>&#8211; Uses the community assignments of concepts to group related chunks together&nbsp;<br>&#8211; Uses an LLM to extract subquery-relevant claims from groups of related chunks as a way of focusing on relevant content only&nbsp;<br>&#8211; Ranks and filters extracted claims to fit a pre-defined context window size&nbsp;</td></tr><tr><td>Reduce answers</td><td>Uses an LLM to answer the original query using the mapped answers</td><td>Uses an LLM to answer the expanded query using the extracted map claims</td></tr></tbody></table></figure>



<h2 class="wp-block-heading" id="lazygraphrag-answer-quality-is-state-of-the-art">LazyGraphRAG answer quality is state-of-the-art</h2>



<p>We compared LazyGraphRAG at varying levels of relevance test budget to a range of competing methods, as follows:&nbsp;</p>



<ul class="wp-block-list">
<li>Dataset: 5,590 AP news articles (used with license)</li>



<li>Queries: 100 synthetic queries (50 local and 50 global), generated using a new method to be described in a future blog post</li>



<li>Metrics: Comprehensiveness, Diversity, Empowerment (as described <a href="https://www.microsoft.com/en-us/research/publication/from-local-to-global-a-graph-rag-approach-to-query-focused-summarization/">here</a>, with an LLM used to compare pairs of answers head-to-head on each metric)</li>



<li>Conditions: Includes LazyGraphRAG with three relevance test budget settings, in addition to eight competing conditions from GraphRAG and literature (Table 2).</li>
</ul>



<figure class="wp-block-table"><table><thead><tr><th>Condition</th><th>Description</th></tr></thead><tbody><tr><td>Z100_Lite</td><td>LazyGraphRAG with a relevance test budget of 100 and using a low-cost LLM model at all steps&nbsp;</td></tr><tr><td>Z500</td><td>LazyGraphRAG with a relevance test budget of 500, using a low-cost LLM for relevance tests and a more advanced (higher cost) LLM for query refinement and map/reduce answer generation&nbsp;</td></tr><tr><td>Z1500</td><td>LazyGraphRAG with a relevance test budget of 1,500, using a low-cost LLM for relevance tests and a more advanced (higher cost) LLM for query refinement and map/reduce answer generation&nbsp;</td></tr><tr><td>C1</td><td>GraphRAG Global Search at community level 1&nbsp;</td></tr><tr><td>C2</td><td>GraphRAG Global Search at community level 2</td></tr><tr><td>C3_Dynamic</td><td>GraphRAG Global Search at community level 3 using dynamic community selection (a cost saving solution introduced in our <a href="https://www.microsoft.com/en-us/research/blog/graphrag-improving-global-search-via-dynamic-community-selection/" target="_blank" rel="noreferrer noopener">previous blog post</a>)</td></tr><tr><td>LS</td><td>GraphRAG Local Search</td></tr><tr><td>DRIFT</td><td>GraphRAG DRIFT Search</td></tr><tr><td>SS_8K</td><td>Semantic Search with vector RAG retrieving 8k tokens of input&nbsp;</td></tr><tr><td>SS_64K</td><td>Semantic Search with vector RAG retrieving 64k tokens of input (included to examine the effect of long-context window on vector RAG)&nbsp;</td></tr><tr><td>RAPTOR</td><td>A hierarchical clustering-based approach to RAG that shares some similar features to LazyGraphRAG&nbsp;</td></tr></tbody></table></figure>



<p>Figure 1 shows LazyGraphRAG win rates against each of the eight competing conditions. At the lowest budget level of 100 relevance tests using a low-cost LLM model&nbsp;(same cost as SS_8K), LazyGraphRAG significantly outperforms all conditions on local and global queries, except for GraphRAG global search conditions for global queries. At an increased budget of 500 relevance tests using a more advanced LLM model (4% of the query cost of C2), LazyGraphRAG significantly outperforms all conditions on both local and global queries. LazyGraphRAG’s win rates continue to increase as the relevance test budget increases to 1,500, demonstrating its scalability in terms of cost vs. quality.&nbsp;</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1400" height="2100" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-blog_fig1.png" alt="Figure 1. Clustered bar charts showing win rates of LazyGraphRAG with relevance test budgets of (100, 500, 1,500) over competing conditions for each combination of (local, global) queries and (comprehensiveness, diversity, empowerment) metrics." class="wp-image-1106040" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-blog_fig1.png 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-blog_fig1-200x300.png 200w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-blog_fig1-683x1024.png 683w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-blog_fig1-768x1152.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-blog_fig1-1024x1536.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-blog_fig1-1365x2048.png 1365w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/LazyGraphRAG-blog_fig1-120x180.png 120w" sizes="(max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">Figure 1. Clustered bar charts showing win rates of LazyGraphRAG with relevance test budgets of (100, 500, 1,500) over competing conditions for each combination of (local, global) queries and (comprehensiveness, diversity, empowerment) metrics</figcaption></figure>



<p>An example comparison is shown in Table 3.</p>



<p><em>Query: Describe what factors individuals in the United States should consider when selecting a health insurance plan during the open enrollment period for 2024 health insurance plans.</em></p>



<figure class="wp-block-table"><table class="has-fixed-layout"><thead><tr><th>Vector RAG (64K context)</th><th>LazyGraphRAG</th></tr></thead><tbody><tr><td># Factors to Consider When Selecting a Health Insurance Plan During Open Enrollment for 2024&nbsp;<br>&nbsp;<br>Selecting a health insurance plan during the open enrollment period can be a complex process, but understanding the key factors to consider can help individuals make informed decisions. Here are some important aspects to keep in mind:&nbsp;<br>&nbsp;<br>## Coverage and Networks&nbsp;<br>&nbsp;<br>One of the primary considerations is the coverage network of the plan. Insurers form coverage networks of hospitals and doctors, and it is crucial to ensure that your preferred healthcare providers are included in the network of the plan you are considering. Many marketplace plans, such as Health Maintenance Organizations (HMOs) and Exclusive Provider Organizations (EPOs), do not cover claims for most non-emergency care outside their networks [Data: Sources (47419, 47421, 47420, 47410, 47408)].&nbsp;<br>&nbsp;<br>## Costs: Premiums, Deductibles, and Out-of-Pocket Maximums&nbsp;<br>&nbsp;<br>Understanding the costs associated with a health insurance plan is essential. This includes the monthly premiums, deductibles, copays, and coinsurance. Plans are typically categorized into different levels such as platinum, gold, silver, and bronze, with bronze plans generally having lower premiums but higher out-of-pocket costs [Data: Sources (47411, 47412, 47416, 47414, 47418)]. Additionally, it is important to consider the out-of-pocket maximum, which is the most you will have to pay in a year for covered services before the insurance starts covering all costs [Data: Sources (47416, 47415)].&nbsp;<br>&nbsp;<br>## Tax Credits and Cost-Sharing Reductions&nbsp;<br>&nbsp;<br>Individuals with lower incomes may qualify for income-based tax credits to help reduce the cost of premiums. It is important to accurately estimate your income for the coming year when applying for these credits to avoid having to repay any excess credits received [Data: Sources (47412, 47413)]. Additionally, those with incomes below 250% of the federal poverty level may qualify for cost-sharing reductions if they select a silver plan, which can lower deductibles and out-of-pocket maximums [Data: Sources (47417, 47418)].&nbsp;<br>&nbsp;<br>## Special Considerations for Medicaid and Medicare&nbsp;<br>&nbsp;<br>For those transitioning from Medicaid, it is important to understand that marketplace plans may come with higher premiums and copays compared to Medicaid. Additionally, many people who have been removed from Medicaid may be eligible to enroll in marketplace plans through an extended enrollment period [Data: Sources (6284, 6285, 6286, 6287)]. For seniors, the Medicare open enrollment period allows for switching between traditional Medicare and Medicare Advantage plans, with considerations for prescription drug coverage and additional benefits like dental and vision [Data: Sources (72060, 72061, 72062, 72063, 72064)].&nbsp;<br>&nbsp;<br>## Assistance and Resources&nbsp;<br>&nbsp;<br>Navigating the complexities of health insurance can be challenging, and seeking assistance from insurance agents or government-funded navigators can be beneficial. These professionals can help individuals understand their options and make informed decisions without recommending specific plans [Data: Sources (47424, 47425, 47426)].&nbsp;<br>&nbsp;<br>## Conclusion&nbsp;<br>&nbsp;<br>Selecting the right health insurance plan requires careful consideration of various factors including coverage networks, costs, eligibility for tax credits and cost-sharing reductions, and available assistance. By thoroughly evaluating these aspects, individuals can choose a plan that best meets their healthcare needs and financial situation for the upcoming year.</td><td>### Types of Health Insurance Plans&nbsp;<br>&nbsp;<br>When selecting a health insurance plan during the open enrollment period for 2024, individuals in the United States have several options to consider:&nbsp;<br>&nbsp;<br>1. **Marketplace Plans**: These are available under the Affordable Care Act (ACA) and are categorized into platinum, gold, silver, and bronze levels. Bronze plans generally have lower premiums but higher out-of-pocket costs, while platinum plans have higher premiums but lower out-of-pocket costs [Data: Sources (47411, 47420, 47421)].&nbsp;<br>&nbsp;<br>2. **Low-Cost Marketplace Plans**: These often come with high deductibles, requiring patients to pay thousands of dollars before most coverage kicks in. They also have annual out-of-pocket maximums that can exceed $9,000 for individuals and $18,000 for families [Data: Sources (47415, 47416, 47414)].&nbsp;<br>&nbsp;<br>3. **Exclusive Provider Organizations (EPOs) and Health Maintenance Organizations (HMOs)**: EPOs and HMOs generally restrict patients to a network of doctors and require a primary care doctor to direct care. They tend to be cheaper but lack out-of-network flexibility [Data: Sources (47420, 47421, 43218, 43217)].&nbsp;<br>&nbsp;<br>4. **Preferred Provider Organizations (PPOs)**: These plans allow for out-of-network services but at a higher cost. They offer more flexibility compared to HMOs and EPOs [Data: Sources (43217)].&nbsp;<br>&nbsp;<br>5. **High-Deductible Health Plans (HDHPs)**: Defined as plans with a deductible of at least $1,600 for individual coverage or $3,200 for family coverage, with out-of-pocket maximums of no more than $8,050 or $16,100, respectively. HDHPs usually have lower premiums, and sometimes companies contribute to a health savings account (HSA) to help cover the deductible [Data: Sources (43227, 43226)].&nbsp;<br>&nbsp;<br>6. **Medicare Advantage**: These are privately run versions of the federal government&#8217;s Medicare program, mostly for people aged 65 and over. They often include prescription drug coverage and may offer additional benefits like dental or vision coverage not provided by traditional Medicare [Data: Sources (72063, 72061, 72060, 72062)].&nbsp;<br>&nbsp;<br>7. **Short-Term Health Insurance Plans**: These plans are limited to three months and can only be renewed for a maximum of four months under new rules. They are intended for temporary coverage but often lack comprehensive benefits [Data: Sources (97999, 97995, 97996, 97997)].&nbsp;<br>&nbsp;<br>### Cost Factors: Premiums, Deductibles, Co-pays, and Out-of-Pocket Maximums&nbsp;<br>&nbsp;<br>The overall cost of health insurance plans in 2024 is influenced by several factors:&nbsp;<br>&nbsp;<br>&#8211; **Premiums**: This is the set monthly cost you pay for your health insurance plan. Premiums have been rising, with a notable increase of 7% for both family and single plans in 2023, partly due to inflation [Data: Sources (83383, 83382, 83384, 83385, 83381, +more)].&nbsp;<br>&nbsp;<br>&#8211; **Deductibles**: The amount you pay out-of-pocket for health care services before your insurance starts to pay. For HDHPs, the deductible is at least $1,600 for individual coverage or $3,200 for family coverage [Data: Sources (43226, 43225)].&nbsp;<br>&nbsp;<br>&#8211; **Co-pays and Co-insurance**: These are the costs you pay each time you receive a medical service. Co-pays are fixed amounts, while co-insurance is a percentage of the service cost.&nbsp;<br>&nbsp;<br>&#8211; **Out-of-Pocket Maximums**: This is the maximum amount you will pay for covered services in a year. For example, HDHPs have out-of-pocket maximums of no more than $8,050 for individual coverage or $16,100 for family coverage [Data: Sources (43227, 43226)].&nbsp;<br>&nbsp;<br>### Provider Networks: In-Network vs. Out-of-Network&nbsp;<br>&nbsp;<br>The network of healthcare providers is a crucial factor in selecting a health insurance plan:&nbsp;<br>&nbsp;<br>&#8211; **In-Network Providers**: These are doctors and hospitals that have agreements with your insurance plan to provide services at lower rates. Ensuring your preferred doctors and specialists are in-network can save you significant costs [Data: Sources (43216, 47419)].&nbsp;<br>&nbsp;<br>&#8211; **Out-of-Network Providers**: Services from these providers are usually more expensive and may not be covered at all, except in emergencies. PPO plans offer some out-of-network coverage but at a higher cost, while HMOs and EPOs generally do not cover non-emergency out-of-network care [Data: Sources (43217, 47421)].&nbsp;<br>&nbsp;<br>### Specific Medical Needs and Services&nbsp;<br>&nbsp;<br>When selecting a health insurance plan, individuals should consider their specific medical needs:&nbsp;<br>&nbsp;<br>&#8211; **Prescription Drugs**: Ensure that your medications are covered by the plan&#8217;s formulary, as drug coverage can change annually [Data: Sources (43220, 43218, 43219)].&nbsp;<br>&nbsp;<br>&#8211; **Mental Health Services**: Coverage for mental health treatments is essential, especially with new rules pushing insurers to increase their coverage of these services [Data: Sources (97031, 97028, 97027, 97030, 97033, +more)].&nbsp;<br>&nbsp;<br>&#8211; **Chronic Conditions**: Plans should cover ongoing treatments and medications for chronic conditions. Medicare Supplement Insurance (Medigap) can help cover gaps in Medicare for chronic disease management [Data: Sources (93367, 93368)].&nbsp;<br>&nbsp;<br>&#8211; **Preventive Care**: Coverage for preventive services like cancer screenings and HIV prevention is mandated under the ACA, though its future is uncertain due to ongoing legal battles [Data: Sources (71106, 71109, 71098, 71099, 71100, +more)].&nbsp;<br>&nbsp;<br>### Key Dates and Steps for Open Enrollment&nbsp;<br>&nbsp;<br>The open enrollment period for 2024 health insurance plans involves several key dates and steps:&nbsp;<br>&nbsp;<br>&#8211; **Marketplace Plans**: Open enrollment starts on November 1, 2023, and runs through mid-December in most states, ending on January 16, 2024 [Data: Sources (47419, 47411, 47416, 47421, 47409, +more)].&nbsp;<br>&nbsp;<br>&#8211; **Medicare**: Open enrollment for Medicare runs from October 15, 2023, to December 7, 2023. During this period, individuals can choose between traditional Medicare, Medicare Advantage plans, and prescription drug plans [Data: Sources (72061, 72063, 72060, 72062)].&nbsp;<br>&nbsp;<br>&#8211; **Special Enrollment Periods**: Individuals who lose coverage due to life events like job loss or moving may qualify for special enrollment periods. For example, those removed from Medicaid may enroll in marketplace plans through July 2024 [Data: Sources (6288, 6289)].&nbsp;<br>&nbsp;<br>By considering these factors, individuals can make informed decisions about their health insurance coverage for 2024, ensuring they select plans that best meet their medical needs and financial situations.</td></tr></tbody></table></figure>



<h2 class="wp-block-heading" id="looking-forward">Looking forward</h2>



<p>LazyGraphRAG shows that it is possible for a single, flexible query mechanism to substantially outperform a diverse range of specialized query mechanisms across the local-global query spectrum, and to do so without the up-front costs of LLM data summarization. Its very fast and almost-free indexing make LazyGraphRAG ideal for one-off queries, exploratory analysis, and streaming data use cases, while its ability to smoothly increase answer quality with increasing relevance test budget makes it a valuable tool for benchmarking RAG approaches in general (e.g., “RAG approach X beats LazyGraphRAG with budget Y for task Z”).&nbsp;</p>



<p>Does this mean that all graph-enabled RAG should be lazy? We believe the answer is no, for three reasons:&nbsp;</p>



<ol class="wp-block-list">
<li>A GraphRAG data index of entity, relationship, and community summaries has use value beyond question answering (e.g., reading and sharing as reports).&nbsp;</li>



<li>A GraphRAG data index of entity, relationship, and community summaries, combined with a LazyGraphRAG-like search mechanism, is likely to achieve better results than LazyGraphRAG alone.&nbsp;</li>



<li>A new kind of GraphRAG data index designed to support a LazyGraphRAG-like search mechanism (e.g., through pre-emptive claim and topic extraction) is likely to achieve the best possible results.&nbsp;</li>
</ol>



<p>We will be exploring these directions in the coming period, with all advances (including LazyGraphRAG itself) released via the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/graphrag" target="_blank" rel="noreferrer noopener">GraphRAG GitHub repository<span class="sr-only"> (opens in new tab)</span></a>. Stay tuned!&nbsp;</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/lazygraphrag-setting-a-new-standard-for-quality-and-cost/">LazyGraphRAG: Setting a new standard for quality and cost</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Ideas: The journey to DNA data storage</title>
		<link>https://www.microsoft.com/en-us/research/podcast/ideas-the-journey-to-dna-data-storage/</link>
		
		<dc:creator><![CDATA[Alyssa Hughes (2ADAPTIVE LLC dba 2A Consulting)]]></dc:creator>
		<pubDate>Tue, 19 Nov 2024 14:00:00 +0000</pubDate>
				<category><![CDATA[Microsoft Research Podcast]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1103874</guid>

					<description><![CDATA[<p>Research manager Karin Strauss and members of the DNA Data Storage Project reflect on the path to developing a synthetic DNA–based system for archival data storage, including the recent open-source release of its most powerful algorithm for DNA error correction.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/podcast/ideas-the-journey-to-dna-data-storage/">Ideas: The journey to DNA data storage</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1401" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Karin-Jake-Bichlien-Sergey_Ideas_Hero_Feature_No_Text_1400x788.jpg" alt="Outlined illustrations of Karin Strauss, Jake Smith, Bichlien Nguyen, and Sergey Yekhanin for the Microsoft Research Podcast, Ideas series." class="wp-image-1103877" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Karin-Jake-Bichlien-Sergey_Ideas_Hero_Feature_No_Text_1400x788.jpg 1401w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Karin-Jake-Bichlien-Sergey_Ideas_Hero_Feature_No_Text_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Karin-Jake-Bichlien-Sergey_Ideas_Hero_Feature_No_Text_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Karin-Jake-Bichlien-Sergey_Ideas_Hero_Feature_No_Text_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Karin-Jake-Bichlien-Sergey_Ideas_Hero_Feature_No_Text_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Karin-Jake-Bichlien-Sergey_Ideas_Hero_Feature_No_Text_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Karin-Jake-Bichlien-Sergey_Ideas_Hero_Feature_No_Text_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Karin-Jake-Bichlien-Sergey_Ideas_Hero_Feature_No_Text_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Karin-Jake-Bichlien-Sergey_Ideas_Hero_Feature_No_Text_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Karin-Jake-Bichlien-Sergey_Ideas_Hero_Feature_No_Text_1400x788-1280x720.jpg 1280w" sizes="(max-width: 1401px) 100vw, 1401px" /></figure>


<div class="wp-block-msr-podcast-container my-4">
	<iframe loading="lazy" src="https://player.blubrry.com/?podcast_id=138524698&modern=1" class="podcast-player" frameborder="0" height="164px" width="100%" scrolling="no" title="Podcast Player"></iframe>
</div>



<p>Behind every emerging technology is a great idea propelling it forward. In the Microsoft Research Podcast series <em>Ideas</em>, members of the research community at Microsoft discuss the beliefs that animate their research, the experiences and thinkers that inform it, and the positive human impact it targets.</p>



<p>Accommodating the increasing amounts of digital data the world is producing requires out-of-the-box thinking. In this episode, guest host <a href="https://www.microsoft.com/en-us/research/people/kstrauss/?msockid=35739e94ab6c69d41b738b93aa076831">Karin Strauss</a>, an innovation strategist and senior principal research manager at Microsoft, brings together members of her team to explore a more sustainable, more cost-effective alternative for archival data storage: synthetic DNA. Strauss, Principal Researcher <a href="https://www.microsoft.com/en-us/research/people/bnguy/?msockid=35739e94ab6c69d41b738b93aa076831">Bichlien Nguyen</a>, Senior Researcher <a href="https://www.microsoft.com/en-us/research/people/jakesmith/?msockid=35739e94ab6c69d41b738b93aa076831">Jake Smith</a>, and Partner Research Manager <a href="https://www.microsoft.com/en-us/research/people/yekhanin/?msockid=35739e94ab6c69d41b738b93aa076831">Sergey Yekhanin</a> discuss how Microsoft Research’s contributions have helped bring “science fiction,” as Strauss describes it, closer to reality, including its role in establishing the DNA Data Storage Alliance to foster collaboration in developing the technology and to establish specifications for interoperability. They also talk about the scope of collaboration with other fields, such as the life sciences and electrical and mechanical engineering, and the coding theory behind the project, including the group’s most powerful algorithm for DNA error correction, Trellis BMA, which is now open source.&nbsp;</p>



<div class="wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-fill-github"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://github.com/microsoft/TrellisBMA">Get the Trellis BMA code</a></div>
</div>



<div class="wp-block-group msr-pattern-link-list is-layout-flow wp-block-group-is-layout-flow">
<hr class="wp-block-separator has-alpha-channel-opacity"/>



<h2 class="wp-block-heading h5" id="learn-more-1">Learn more:</h2>



<ul class="wp-block-list list-unstyled">
<li><a href="https://www.microsoft.com/en-us/research/publication/trellis-bma-coded-trace-reconstruction-on-ids-channels-for-dna-storage/">Trellis BMA: coded trace reconstruction on IDS channels for DNA storage<br></a>Publication, July 2021</li>



<li><a href="https://www.microsoft.com/en-us/research/publication/evaluating-the-risk-of-data-loss-due-to-particle-radiation-damage-in-a-dna-data-storage-system/">Evaluating the risk of data loss due to particle radiation damage in a DNA data storage system | Nature Communications<br></a>Publication, September 2024</li>



<li><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://dnastoragealliance.org/" target="_blank" rel="noreferrer noopener">DNA Data Storage Alliance<span class="sr-only"> (opens in new tab)</span></a><br>Alliance homepage</li>



<li><a href="https://www.microsoft.com/en-us/research/publication/architecting-datacenters-for-sustainability-greener-data-storage-using-synthetic-dna/">Architecting Datacenters for Sustainability: Greener Data Storage using Synthetic DNA</a><br>Publication, September 2021</li>



<li><a href="https://www.microsoft.com/en-us/research/video/microsoft-and-uw-demonstrate-first-fully-automated-dna-data-storage/">Microsoft and UW demonstrate first fully automated DNA data storage</a><br>Video, March 2019</li>



<li><a href="https://www.microsoft.com/en-us/research/podcast/storing-digital-data-in-synthetic-dna-with-dr-karin-strauss/">Storing digital data in synthetic DNA with Dr. Karin Strauss</a><br>Microsoft Research Podcast, October 2018</li>



<li><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.youtube.com/watch?v=mILu11yqtG8" target="_blank" rel="noreferrer noopener">Microsoft and University of Washington DNA Storage Research Project<span class="sr-only"> (opens in new tab)</span></a><br>Video, July 2016</li>
</ul>
</div>



<section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast">
	<div class="subscribe-to-podcast__inner border-top border-bottom border-width-2">
		<h2 class="h5 subscribe-to-podcast__heading">
			Subscribe to the <a href="https://www.microsoft.com/en-us/research/podcast">Microsoft Research Podcast</a>:		</h2>
		<ul class="subscribe-to-podcast__list list-unstyled">
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://itunes.apple.com/us/podcast/microsoft-research-a-podcast/id1318021537?mt=2" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="black" viewBox="0 0 32 32">  <path d="M7.12 0c-3.937-0.011-7.131 3.183-7.12 7.12v17.76c-0.011 3.937 3.183 7.131 7.12 7.12h17.76c3.937 0.011 7.131-3.183 7.12-7.12v-17.76c0.011-3.937-3.183-7.131-7.12-7.12zM15.817 3.421c3.115 0 5.932 1.204 8.079 3.453 1.631 1.693 2.547 3.489 3.016 5.855 0.161 0.787 0.161 2.932 0.009 3.817-0.5 2.817-2.041 5.339-4.317 7.063-0.812 0.615-2.797 1.683-3.115 1.683-0.12 0-0.129-0.12-0.077-0.615 0.099-0.792 0.192-0.953 0.64-1.141 0.713-0.296 1.932-1.167 2.677-1.911 1.301-1.303 2.229-2.932 2.677-4.719 0.281-1.1 0.244-3.543-0.063-4.672-0.969-3.595-3.907-6.385-7.5-7.136-1.041-0.213-2.943-0.213-4 0-3.636 0.751-6.647 3.683-7.563 7.371-0.245 1.004-0.245 3.448 0 4.448 0.609 2.443 2.188 4.681 4.255 6.015 0.407 0.271 0.896 0.547 1.1 0.631 0.447 0.192 0.547 0.355 0.629 1.14 0.052 0.485 0.041 0.62-0.072 0.62-0.073 0-0.62-0.235-1.199-0.511l-0.052-0.041c-3.297-1.62-5.407-4.364-6.177-8.016-0.187-0.943-0.224-3.187-0.036-4.052 0.479-2.323 1.396-4.135 2.921-5.739 2.199-2.319 5.027-3.543 8.172-3.543zM16 7.172c0.541 0.005 1.068 0.052 1.473 0.14 3.715 0.828 6.344 4.543 5.833 8.229-0.203 1.489-0.713 2.709-1.619 3.844-0.448 0.573-1.537 1.532-1.729 1.532-0.032 0-0.063-0.365-0.063-0.803v-0.808l0.552-0.661c2.093-2.505 1.943-6.005-0.339-8.296-0.885-0.896-1.912-1.423-3.235-1.661-0.853-0.161-1.031-0.161-1.927-0.011-1.364 0.219-2.417 0.744-3.355 1.672-2.291 2.271-2.443 5.791-0.348 8.296l0.552 0.661v0.813c0 0.448-0.037 0.807-0.084 0.807-0.036 0-0.349-0.213-0.683-0.479l-0.047-0.016c-1.109-0.885-2.088-2.453-2.495-3.995-0.244-0.932-0.244-2.697 0.011-3.625 0.672-2.505 2.521-4.448 5.079-5.359 0.547-0.193 1.509-0.297 2.416-0.281zM15.823 11.156c0.417 0 0.828 0.084 1.131 0.24 0.645 0.339 1.183 0.989 1.385 1.677 0.62 2.104-1.609 3.948-3.631 3.005h-0.015c-0.953-0.443-1.464-1.276-1.475-2.36 0-0.979 0.541-1.828 1.484-2.328 0.297-0.156 0.709-0.235 1.125-0.235zM15.812 17.464c1.319-0.005 2.271 0.463 2.625 1.291 0.265 0.62 0.167 2.573-0.292 5.735-0.307 2.208-0.479 2.765-0.905 3.141-0.589 0.52-1.417 0.667-2.209 0.385h-0.004c-0.953-0.344-1.157-0.808-1.553-3.527-0.452-3.161-0.552-5.115-0.285-5.735 0.348-0.823 1.296-1.285 2.624-1.291z"/></svg>
						<span class="subscribe-to-podcast__link-text">Apple Podcasts</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribebyemail.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.4 6a2.392 2.392 0 00-2.372 2.119L16 15.6l11.972-7.481A2.392 2.392 0 0025.6 6H6.4zM4 10.502V22.8a2.4 2.4 0 002.4 2.4h19.2a2.4 2.4 0 002.4-2.4V10.502l-11.365 7.102a1.2 1.2 0 01-1.27 0L4 10.502z"/></svg>
						<span class="subscribe-to-podcast__link-text">Email</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribeonandroid.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M12.414 4.02c-.062.012-.126.023-.18.06a.489.489 0 00-.12.675L13.149 6.3c-1.6.847-2.792 2.255-3.18 3.944h13.257c-.388-1.69-1.58-3.097-3.179-3.944l1.035-1.545a.489.489 0 00-.12-.675.492.492 0 00-.675.135l-1.14 1.68a7.423 7.423 0 00-2.55-.45c-.899 0-1.758.161-2.549.45l-1.14-1.68a.482.482 0 00-.494-.195zm1.545 3.824a.72.72 0 110 1.44.72.72 0 010-1.44zm5.278 0a.719.719 0 110 1.44.719.719 0 110-1.44zM8.44 11.204A1.44 1.44 0 007 12.644v6.718c0 .795.645 1.44 1.44 1.44.168 0 .33-.036.48-.09v-9.418a1.406 1.406 0 00-.48-.09zm1.44 0V21.76c0 .793.646 1.44 1.44 1.44h10.557c.793 0 1.44-.647 1.44-1.44V11.204H9.878zm14.876 0c-.169 0-.33.035-.48.09v9.418c.15.052.311.09.48.09a1.44 1.44 0 001.44-1.44v-6.719a1.44 1.44 0 00-1.44-1.44zM11.8 24.16v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84zm5.759 0v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84z"/></svg>
						<span class="subscribe-to-podcast__link-text">Android</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://open.spotify.com/show/4ndjUXyL0hH1FXHgwIiTWU" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M16 4C9.383 4 4 9.383 4 16s5.383 12 12 12 12-5.383 12-12S22.617 4 16 4zm5.08 17.394a.781.781 0 01-1.086.217c-1.29-.86-3.477-1.434-5.303-1.434-1.937.002-3.389.477-3.403.482a.782.782 0 11-.494-1.484c.068-.023 1.71-.56 3.897-.562 1.826 0 4.365.492 6.171 1.696.36.24.457.725.217 1.085zm1.56-3.202a.895.895 0 01-1.234.286c-2.338-1.457-4.742-1.766-6.812-1.747-2.338.02-4.207.466-4.239.476a.895.895 0 11-.488-1.723c.145-.041 2.01-.5 4.564-.521 2.329-.02 5.23.318 7.923 1.995.419.26.547.814.286 1.234zm1.556-3.745a1.043 1.043 0 01-1.428.371c-2.725-1.6-6.039-1.94-8.339-1.942h-.033c-2.781 0-4.923.489-4.944.494a1.044 1.044 0 01-.474-2.031c.096-.023 2.385-.55 5.418-.55h.036c2.558.004 6.264.393 9.393 2.23.497.292.663.931.371 1.428z"/></svg>
						<span class="subscribe-to-podcast__link-text">Spotify</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.667 4a2.676 2.676 0 00-2.612 2.13v.003c-.036.172-.055.35-.055.534v18.666c0 .183.019.362.055.534v.003a2.676 2.676 0 002.076 2.075h.002c.172.036.35.055.534.055h18.666A2.676 2.676 0 0028 25.333V6.667a2.676 2.676 0 00-2.13-2.612h-.003A2.623 2.623 0 0025.333 4H6.667zM8 8h1.333C17.42 8 24 14.58 24 22.667V24h-2.667v-1.333c0-6.618-5.382-12-12-12H8V8zm0 5.333h1.333c5.146 0 9.334 4.188 9.334 9.334V24H16v-1.333A6.674 6.674 0 009.333 16H8v-2.667zM10 20a2 2 0 11-.001 4.001A2 2 0 0110 20z"/></svg>
						<span class="subscribe-to-podcast__link-text">RSS Feed</span>
					</a>
				</li>
					</ul>
	</div>
</section>


<div class="wp-block-msr-show-more">
	<div class="bg-neutral-100 p-5">
		<div class="show-more-show-less" data-mount="show-more-show-less">
			<div>
				<span>
					

<h2 class="wp-block-heading" id="transcript-1">Transcript</h2>



<p>[TEASER]</p>



<p>[MUSIC PLAYS UNDER DIALOGUE]</p>



<p><strong>JAKE SMITH: </strong>This really starts from the fundamental data production–data storage gap, where we produce way more data nowadays than we could ever have imagined years ago. And it&#8217;s more than we can practically store in magnetic media. And so we really need a denser medium on the other side to contain that. DNA is extremely dense. It holds far, far more information per unit volume, per unit mass than any storage media that we have available today. This, along with the fact that DNA is itself a relatively rugged molecule—it lives in our body; it lives outside our body for thousands and thousands of years if we, you know, leave it alone to do its thing—makes it a very attractive media. </p>



<p><strong>BICHLIEN NGUYEN:</strong> It&#8217;s such a futuristic technology, right? When you begin to work on the tech, you realize how many disciplines and domains you actually have to reach in and leverage. It&#8217;s really interesting, this multidisciplinarity, because we&#8217;re, in a way, bridging software with wetware with hardware. And so you, kind of, need <em>all</em> the different disciplines to actually get you to where you need to go.&nbsp;</p>



<p><strong>SERGEY YEKHANIN:</strong> We all work for Microsoft; we are all Microsoft researchers. Microsoft isn’t a startup. But that team, the team that drove the DNA Data Storage Project, it did feel like a startup, and it was something unusual and exciting for me.</p>



<p><strong>SERIES INTRO: </strong>You’re listening to <em>Ideas</em>, a Microsoft Research Podcast that dives deep into the world of technology research and the profound questions behind the code. In this series, we’ll explore the technologies that are shaping our future and the big ideas that propel them forward.</p>



				</span>
				<span id="show-more-show-less-toggle-1" class="show-more-show-less-toggleable-content">
					



<p>[MUSIC FADES]</p>



<p><strong>GUEST HOST KARIN STRAUSS:</strong> I&#8217;m your guest host Karin Strauss, a senior principal research manager at Microsoft. For nearly a decade, my colleagues and I—along with a fantastic and talented group of collaborators from academia and industry—have been working together to help close the data creation–data storage gap. We&#8217;re producing far more digital information than we can possibly store. One solution we&#8217;ve explored uses synthetic DNA as a medium, and over the years, we&#8217;ve contributed to steady and promising progress in the area. We&#8217;ve helped push the boundaries of how much DNA writer can simultaneously store, shown that full automation is possible, and helped create an ecosystem for the commercial success of DNA data storage. And just this week, we&#8217;ve made one of our most advanced tools for encoding and decoding data in DNA open source. Joining me today to discuss the state of DNA data storage and some of our contributions are several members of the DNA Data Storage Project at Microsoft Research: Principal Researcher Bichlien Nguyen, Senior Researcher Jake Smith, and Partner Research Manager Sergey Yekhanin. Bichlien, Jake, and Sergey, welcome to the podcast.<s></s></p>



<p><strong>BICHLIEN NGUYEN:</strong> Thanks for having us, Karin.</p>



<p><strong>SERGEY YEKHANIN:</strong> Thank you so much.</p>



<p><strong>JAKE SMITH: </strong>Yes, thank you.</p>



<p><strong>STRAUSS:</strong> So before getting into the details of DNA data storage and our work, I&#8217;d like to talk about the big idea behind the work and how we all got here. I&#8217;ve often described the DNA Data Storage Project as turning science fiction into reality. When we started the project in 2015, though, the idea of using DNA for archival storage was already out there and had been for over five decades. Still, when I talked about the work in the area, people were pretty skeptical in the beginning, and I heard things like, “Wow, why are you thinking about that? It&#8217;s so far off.” So, first, please share a bit of your research backgrounds and then how you came to work on this project. Where did you first encounter this idea, what do you remember about your initial impressions—<em>or</em> the impressions of others—and what made you want to get involved? Sergey, why don’t you start.</p>



<p><strong>YEKHANIN:</strong> Thanks so much. So I’m a coding theorist by training, so, like, my core areas of research have been error-correcting codes and also computational complexity theory. So I joined the project probably, like, within half a year of the time that it was born, and thanks, Karin, for inviting me to join. So, like, that was roughly the time when I moved from a different lab, from the Silicon Valley lab in California to the Redmond lab, and actually, it just so happened that at that moment, I was thinking about what to do next. Like, in California, I was mostly working on coding for distributed storage, and when I joined here, that effort kept going. But I had some free cycles, and that was the moment when Karin came just to my office and told me about the project. So, indeed, initially, it did feel a lot like science fiction. Because, I mean, we are used to coding for digital storage media, like for magnetic storage media, and here, like, this is biology, and, like, why exactly these kind of molecules? There are so many different molecules. Like, why that? But honestly, like, I didn&#8217;t try to pretend to be a biologist and make conclusions about whether this is the right medium or the wrong medium. So I tried to look into these kinds of questions from a technical standpoint, and there was a lot of, kind of, deep, interesting coding questions, and that was the main attraction for me. At the same time, I wasn’t convinced that we will get as far as we actually got, and I wasn&#8217;t immediately convinced about the future of the field, but, kind of, just the depth and the richness of the, what I’ll call, technical problems, that&#8217;s what made it appealing for me, and I, kind of, enthusiastically joined. And also, I guess, the culture of the team. So, like, it did feel like a startup. Like, we all work for Microsoft; we’re all Microsoft researchers. Microsoft isn’t a startup. But that team, the team that drove the DNA Data Storage Project, it did feel like a startup, and it was something unusual and exciting for me.</p>



<p><strong>NGUYEN: </strong>Oh, I love that, Sergey. So my background is in organic chemistry, and Karin had reached out to me, and I interviewed not knowing what Karin wanted. Actually … so I took the job kind of blind because I was like, “Hmm, Microsoft Research? … DNA biotech? &#8230;” I was very, very curious, and then when she told me that this project was about DNA data storage, I was like, this is a crazy, crazy idea. I definitely was not sold on it, but I was like, well, look, I get to meet and work with so many interesting people from different backgrounds that, one, even if it doesn&#8217;t work out, I’m going to learn something, and, two, I think it <em>could</em> work, like it could work. And so I think that&#8217;s really what motivated me to join.</p>



<p><strong>SMITH: </strong>The first thing that you think when you hear about we&#8217;re going to take what is our hard drive and we&#8217;re going to turn that into DNA is that this is nuts. But, you know, it didn&#8217;t take very long after that. I come from a chemistry, biotech-type background where I&#8217;ve been working on designing drugs, and there, DNA is this thing off in the nethers, you know. You look at it every now and then to see what information it can tell you about, you know, what maybe your drug might be hitting on the target side, and it&#8217;s, you know, that connection—that the DNA <em>contains</em> the information in the living systems, the DNA <em>contains</em> the information in our assays, and why could the DNA not contain the information that we, you know, think more about every day, that information that lives in our computers—as an extremely cool idea.</p>



<p><strong>STRAUSS:</strong> Through our work, we&#8217;ve had years to wrap our heads around DNA data storage. But, Jake, could you tell us a little bit about how DNA data storage works and why we&#8217;re interested in looking into the technology?</p>



<p><strong>SMITH: </strong>So you mentioned it earlier, Karin, that this really starts from the fundamental data production–data storage gap, where we produce way more data nowadays than we could ever have imagined years ago. And it&#8217;s more than we can practically store in magnetic media. This is a problem because, you know, we have data; we have recognized the <em>value</em> of data with the rise of large language models and these other big generative models. The data that we do produce, our video has gone from, you know, substantially small, down at 480 resolution, all the way up to things at 8K resolution that now take orders of magnitude more storage. And so we really need a denser medium on the other side to contain that. DNA is extremely dense. It holds far, far more information per unit volume, per unit mass than any storage media that we have available today. This, along with the fact that DNA is itself a relatively rugged molecule—it lives in our body; it lives outside our body for thousands and thousands of years if we, you know, leave it alone to do its thing—makes it a very attractive media, particularly compared to the traditional magnetic media, which has lower density and a much shorter lifetime on the, you know, scale of decades at most.</p>



<p>So how does DNA data storage actually work? Well, at a very high level, we start out in the digital domain, where we have our information represented as ones and zeros, and we need to convert that into a series of A&#8217;s, C&#8217;s, T&#8217;s, and G&#8217;s that we could then actually produce, and this is really the domain of Sergey. He&#8217;ll tell us much more about how this works later on. For now, let&#8217;s just assume we&#8217;ve done this. And now our information, you know, lives in the DNA base domain. It&#8217;s still in the digital world. It&#8217;s just represented as A’s, C’s, T’s, and G’s, and we now need to make this <em>physical</em> so that we can store it. This is accomplished through large-scale DNA synthesis. Once the DNA has been synthesized with the sequences that we specified, we need to store it. There&#8217;s a lot of ways we can think about storing it. Bichlien’s done great work looking at DNA encapsulation, as well as, you know, other more raw just DNA-on-glass-type techniques. And we&#8217;ve done some work looking at the susceptibility of DNA stored in this unencapsulated form to things like atmospheric humidity, to temperature changes and, most excitingly, to things like neutron radiation. So we&#8217;ve stored our data in this physical form, we&#8217;ve archived it, and coming back to it, likely many years in the future because the properties of DNA match up very well with archival storage, we need to convert it back into the digital domain. And this is done through a technique called DNA sequencing. What this does is it puts the molecules through some sort of machine, and on the other side of the machine, we get out, you know, a noisy representation of what the actual sequence of bases in the molecules were. We have one final step. We need to take this series of noisy sequences and convert it back into ones and zeros. Once we do this, we return to our original data and we&#8217;ve completed, let&#8217;s call it, one DNA data storage cycle.</p>



<p><strong>STRAUSS:</strong> We&#8217;ll get into this in more detail later, but maybe, Sergey, we dig a little bit on encoding-decoding end of things and how DNA is different as a medium from other types of media.</p>



<p><strong>YEKHANIN:</strong> Sure. So, like, I mean, coding is an important aspect of this whole idea of DNA data storage because we have to deal with errors—it’s a new medium—but talking about error-correcting codes in the context of DNA data storage, so, I mean, usually, like … what are error-correcting codes about? Like, on the very high level, right, I mean, you have some data—think of it as a binary string—you want to store it, but there are errors. So usually, like, in most, kind of, forms of media, the errors are bit flips. Like, you store a 0; you get a 1. Or you store a 1; you get a 0. So these are called substitution errors. The field of error-correcting codes, it started, like, in the 1950s, so, like, it’s 70 years old at least. So we, kind of, we understand how to deal with this kind of error reasonably well, so with substitution errors. In DNA data storage, the way you store your data is that given, like, some large amount of digital data, you have the freedom of choosing which short DNA molecules to generate. So in a DNA molecule, it’s a sequence of the bases A, G, C, and T, and you have the freedom to decide, like, which of the short molecules you need to generate, and then those molecules get stored, and then during the storage, some of them are lost; some of them can be damaged. There can be insertions and deletions of bases on every molecule. Like, we call them strands. So you need redundancy, and there are two forms of redundancy. There&#8217;s redundancy that goes across strands, and there is redundancy on the strand. And so, yeah, so, kind of, from the error-correcting side of things, like, we get to decide what kind of redundancy we want to introduce—across strands, on the strand—and then, like, we want to make sure that our encoding and decoding algorithms are efficient. So that&#8217;s the coding theory angle on the field.</p>



<p><strong>NGUYEN: </strong>Yeah, and then, you know, from there, once you have that data encoded into DNA, the question is how do you <em>make</em> that data on a scale that&#8217;s compatible with digital data storage? And so that&#8217;s where a lot of the work came in for really automating the synthesis process and also the reading process, as well. So synthesis is what we consider the writing process of DNA data storage. And so, you know, we came up with some unique ideas there. We made a chip that enabled us to get to the densities that we needed. And then on the reading side, we used different sequencing technologies. And it was great to see that we could actually just, kind of, pull sequencing technologies off the shelf because people are so interested in reading biological DNA. So we explored the Illumina technologies and also Oxford Nanopore, which is a new technology coming in the horizon. And then preservation, too, because we have to make sure that the data that’s stored in the DNA doesn&#8217;t get damaged and that we can recover it using the error-correcting codes.</p>



<p><strong>STRAUSS:</strong> Yeah, absolutely. And it&#8217;s clear that—and it&#8217;s also been our experience that—DNA data storage and projects like this require more than just a team of computer scientists. Bichlien, you’ve had the opportunity to collaborate with many people in all different disciplines. So do you want to talk a little bit about that? What kind of expertise, you know, other disciplines that are relevant to bringing DNA data storage to reality?</p>



<p><strong>NGUYEN: </strong>Yeah, well, it&#8217;s such a futuristic technology, right? When you begin to work on the tech, you realize how many disciplines and domains you actually have to reach in and leverage. One concrete example is that in order to fabricate an electronic chip to synthesize DNA, we really had to pull in a lot of material science research because there&#8217;s different capabilities that are needed when trying to use liquid on a chip. We, you know, have to think about DNA data <em>storage</em> itself. And that&#8217;s a very different beast than, you know, the traditional storage mediums. And so we worked with teams who literally create, you know, these little tiny micro- or nanocapsules in glass and being able to store that there. It&#8217;s really interesting, this multidisciplinarity, because we&#8217;re, in a way, bridging software with wetware with hardware. And so you, kind of, need <em>all</em> the different disciplines to actually get you to where you need to go.</p>



<p><strong>STRAUSS:</strong> Yeah, absolutely. And, you know, building on, you know, collaborators, I think one area that was super interesting, as well, and was pretty early on in the project was building that first end-to-end system that we collaborated with University of Washington, the Molecular Information Systems Lab there, to build. And really, at that point, you know, there had been work suggesting that DNA data storage was viable, but nobody had really shown an end-to-end system, from beginning to end, and in fact, my manager at the time, Doug Carmean, used to call it the “bubble gum and shoestring” system. But it was a crucial first step because it shows it was possible to really fully automate the process. And there have been several interesting challenges there in the system, but we noticed that one particularly challenging one was synthesis. That first system that we built was capable of storing the word “hello,” and that was <em>all</em> we could store. So it wasn&#8217;t a very high-capacity system. But in order to be able to store a lot more volumes of data instead of a simple word, we really needed much more advanced synthesis systems. And this is what both Bichlien and Jake ended up working on, so do you want to talk a little bit about that and the importance of that particular work?</p>



<p><strong>SMITH: </strong>Yeah, absolutely. As you said, Karin, the amount of DNA that is required to store the massive amount of data we spoke about earlier is far beyond the amount of DNA that&#8217;s needed for any, air quotes, traditional applications of synthetic DNA, whether it&#8217;s your gene construction or it&#8217;s your primer synthesis or such. And so we really had to rethink how you make DNA at scale and think about how could this actually scale to meet the demand. And so Bichlien started out looking at a thing called a <em>microelectrode array</em>, where you have this big checkerboard of small individual reaction sites, and in each reaction site, we used electrochemistry in order to control base by base—A, C, T, or G by A, C, T, or G—the sequence that was growing at that particular reaction site. We got this down to the nanoscale. And so what this means practically is that on one of these chips, we could synthesize at any given time on the order of hundreds of millions of individual strands. So once we had the synthesis working with the traditional chemistry where you&#8217;re doing chemical synthesis—each base is added in using a mixture of chemicals that are added to the individual spots—they&#8217;re activated. But each coupling happens due to some energy you <em>prestored</em> in the synthesis of your reagents. And this makes the synthesis of those reagents costly and themselves a bottleneck. And so taking, you know, a look forward at what else was happening in the synthetic biology world, the, you know, next big word in DNA synthesis was and still is <em>enzymatic synthesis</em>, where rather than having to, you know, spend a lot of energy to chemically <em>pre</em>-activate reagents that will go in to make your actual DNA strands, we capitalize on nature&#8217;s synthetic robots—<em>enzymes</em>—to start with less-activated, less-expensive-to-get-to, cheaply-produced-through-natural-processes substrates, and we use the enzymes themselves, toggling <em>their</em> activity over each of the individual chips, or each of the individual spots on our checkerboard, to construct DNA strands. And so we got a little bit into this project. You know, we successfully showed that we could put down selectively one base at a given time. We hope that others will, kind of, take up the work that we&#8217;ve put out there, you know, particularly our wonderful collaborators at Ansa who helped us design the enzymatic system. And one day we will see, you know, a truly parallelized, in this fashion, enzymatic DNA system that can achieve the scales necessary.</p>



<p><strong>NGUYEN: </strong>It&#8217;s interesting to note that even though it&#8217;s DNA and we&#8217;re still storing data in these DNA strands, chemical synthesis and enzymatic synthesis provide different errors that you see in the actual files, right, in the DNA files. And so I know that we talked to Sergey about how do we deal with these new types of errors and also the new capabilities that you can have, for example, if you don&#8217;t control base by base the DNA synthesis.</p>



<p><strong>YEKHANIN:</strong> This whole field of DNA data storage, like, the technologies on the biology side are advancing rapidly, right. And there are different approaches to synthesis. There are different approaches to sequencing. And, presumably, the way the storage is actually done, like, is also progressing, right, and we had works on that. So there is, kind of, this very general, kind of, high-level error profile that you can say that these are the type of errors that you encounter in DNA data storage. Like, in DNA molecules—just the sequence of these bases, A, G, C, T, in maybe a length of, like, 200 or so and you store a very, very large number of them—the errors that you see is that some of these strands, kind of, will disappear. Some of these strings can be torn apart like, let’s say, in two pieces, maybe even more. And then on every strand, you also encounter these errors—insertions, deletions, substitutions—with different rates. Like, the likelihood of all kinds of these errors may differ very significantly across different technologies that you use on the biology side. And also there can be error bursts somehow. Maybe you can get an insertion of, I don’t know, 10 A’s, like, in a row, or you can <em>lose</em>, like, you know, 10 bases in a row. So if you don&#8217;t, kind of, quantify, like, what are the likelihoods of all these bad events happening, then I think this still, kind of, fits at least the majority of approaches to DNA data storage, maybe not exactly all of them, but it fits the majority. So when we design coding schemes, we are trying also, kind of, to look ahead in the sense that, like, we don&#8217;t know, like, in five years, like, how will these error profiles, how will it look like. So the technologies that we develop on the error-correction side, we try to keep them very flexible, so whether it&#8217;s enzymatic synthesis, whether it&#8217;s Nanopore technology, whether it’s Illumina technology that is being used, the error-correction algorithms would be able to adapt and would still be useful. But, I mean, this makes also coding aspect harder because, [LAUGHTER] kind of, you want to keep all this flexibility in mind.</p>



<p><strong>STRAUSS:</strong> So, Sergey, we are at an interesting moment now because you’re open sourcing the Trellis BMA piece of code, right, that you published a few years ago. Can you talk a little bit about that specific problem of trace reconstruction and then the paper specifically and how it solves it?</p>



<p><strong>YEKHANIN:</strong> Absolutely, yeah, so this Trellis BMA paper for that we are releasing the source code right now, this is, kind of, this is the latest in our sequence of publications on error-correction for DNA data storage. And I should say that, like, we already discussed that the project is, kind of, very interdisciplinary. So, like, we have experts from all kinds of fields. But really even within, like, within this coding theory, like, within computer science/information theory, coding theory, in our algorithms, we use ideas from very different branches. I mean, there are some core ideas from, like, core algorithm space, and I won’t go into these, but let me just focus, kind of, on two aspects. So when we just faced this problem of coding for DNA data storage and we were thinking about, OK, so how to exactly design the coding scheme and what are the algorithms that we’ll be using for error correction, so, I mean, we’re always studying the literature, and we came up on this problem called <em>trace reconstruction</em> that was pretty popular—I mean, somewhat popular, I would say—in computer science and in statistics. It didn’t have much motivation, but very strong mathematicians had been looking at it. And the problem is as follows. So, like, there is a long binary string picked at random, and then it’s transmitted over a deletion channel, so some bits—some zeros and some ones—at certain coordinates get deleted and you get to see, kind of, the shortened version of the string. But you get to see it multiple times. And the question is, like, how many times do you need to see it so that you can get a reasonably accurate estimate of the original string that was transmitted? So that was called trace reconstruction, and we took a lot of motivation—we took a lot of inspiration—from the problem, I would say, because really, in DNA data storage, if we think about a single strand, like, a single strand which is being stored, after we read it, we usually get multiple reads of this string. And, well, the errors there are not just deletions. There are insertions, substitutions, and, like, inversive errors, but still we could rely on this literature in computer science that already had some ideas. So there was an algorithm called BMA, <em>Bitwise Majority Alignment</em>. We extended it—we adopted it, kind of, for the needs of DNA data storage—and it became, kind of, one of the tools in our toolbox for error correction.</p>



<p>So we also started to use ideas from literature on electrical engineering, what are called <em>convolutional error-correcting codes</em> and a certain, kind of, class of algorithms for decoding errors in these convolutional error-correcting codes called, like, I mean, Trellis is the main data structure, like, Trellis-based algorithms for decoding convolutional codes, like, Viterbi algorithm or BCJR algorithm. Convolutional codes allow you to introduce redundancy on the string. So, like, with algorithms kind of similar to BMA, like, they were good for doing error correction when there was no redundancy on the strand itself. Like, when there is redundancy on the strand, kind of, we could do some things, but really it was very limited. With Trellis-based approaches, like, again inspired by the literature in electrical engineering, we had an approach to introduce redundancy on the strand, so that allowed us to have more powerful error-correction algorithms. And then in the end, we have this algorithm, which we call Trellis BMA, which, kind of, combines ideas from both fields. So it&#8217;s based on Trellis, but it&#8217;s also more efficient than standard Trellis-based algorithms because it uses ideas from BMA from computer science literature. So this is, kind of, this is a mix of these two approaches. And, yeah, that’s the paper that we wrote about three years ago. And now we&#8217;re open sourcing it. So it is the most powerful algorithm for DNA error correction that we developed in the group. We’re really happy that now we are making it publicly available so that anybody can experiment with the source code. Because, again, the field has expanded a lot, and now there are multiple groups around the globe that work just specifically on error correction apart from all other aspects, so, yeah, so we are really happy that it’s become publicly available to hopefully further advance the field.</p>



<p><strong>STRAUSS:</strong> Yeah, absolutely, and I&#8217;m always amazed by, you know, how, <em>it</em> <em>is</em> really about building on other people&#8217;s work. Jake and Bichlien, you recently published a paper in <em>Nature Communications</em>. Can you tell us a little bit about what it was, what you exposed the DNA to, and what it was specifically about?</p>



<p><strong>NGUYEN:</strong>&nbsp;Yeah. So that paper was on the effects of neutron radiation on DNA data storage. So, you know, when we started the DNA Data Storage Project, it was really a comparison, right, between the different storage medias that exist today. And one of the issues that have come up through the years of development of those technologies was, you know, hard errors and soft errors that were induced by radiation. So we wanted to know, does that <em>maybe</em> happen in DNA? We know that DNA, in humans at least, is affected by radiation from cosmic rays. And so that was really the motivation for this type of experiment. So what we did was we essentially took our DNA files and dried them and threw them in a neutron accelerator, which was fantastic. It was so exciting. That&#8217;s, kind of, the merge of, you know, sci fi <em>with</em> sci fi at the same time. [LAUGHS] It was fantastic. And we irradiated for over 80 million years—</p>



<p><strong>STRAUSS: </strong>The equivalent of …</p>



<p><strong>NGUYEN: </strong>The <em>equivalent</em> of 80 million years.</p>



<p><strong>STRAUSS: </strong>Yes, because it&#8217;s a lot of radiation all at the same time, …</p>



<p><strong>NGUYEN: </strong>It’s a lot of radiation …</p>



<p><strong>STRAUSS:</strong> … and it&#8217;s <em>accelerated</em> radiation exposure?</p>



<p><strong>NGUYEN: </strong>Yeah, I would say it&#8217;s accelerated <em>aging</em> with radiation. It&#8217;s an insane amount of radiation. And it was surprising that even though we irradiated our DNA files with that much radiation, there wasn&#8217;t that much damage. And that&#8217;s surprising because, you know, we know that humans, if we were to be irradiated like that, it would be disastrous. But in, you know, DNA, our files were able to be recovered with zero bit errors.</p>



<p><strong>STRAUSS:</strong> And why that difference?</p>



<p><strong>NGUYEN: </strong>Well, we think there&#8217;s a few reasons. One is that when you look at the interaction between a neutron and the actual elemental composition of DNA—which is basically carbons, oxygens, and hydrogens, maybe a phosphorus—the neutrons don&#8217;t interact with the DNA much. And if it did interact, we would have, for example, a strand break, which based on the error-correcting codes, we can recover from. So essentially, there&#8217;s not much … one, there&#8217;s not much interaction between neutrons and DNA, and second, we have error-correcting codes that would prevent any data loss.</p>



<p><strong>STRAUSS:</strong> Awesome, so yeah, this is another milestone that contributes towards the technology becoming a reality. There are also other conditions that are needed for technology to be brought to the market. And one thing I&#8217;ve worked on is to, you know, create the DNA Data Storage Alliance; this is something Microsoft co-founded with Illumina, Twist Bioscience, and Western Digital. And the goal there was to essentially provide the right conditions for the technology to thrive commercially. We did bring together multiple universities and companies that were interested in the technology. And one thing that we&#8217;ve seen with storage technologies that&#8217;s been pretty important is standardization and making sure that the technology’s interoperable. And, you know, we&#8217;ve seen stalemate situations like Blu-ray and high-definition DVD, where, you know, really we couldn&#8217;t decide on a standard, and the technology, it took a while for the technology to be picked up, and the intent of the DNA Data Storage [Alliance] is to provide an ecosystem of companies, universities, groups interested in making sure that this time, it&#8217;s an interoperable technology from the get-go, and that increases the chances of commercial adoption.<strong> </strong>As a group, we often talk about how amazing it is to work for a company that empowers us to do this kind of research. And for me, one of Microsoft Research’s unique strengths, particularly in this project, is the opportunity to work with such a diverse set of collaborators on such a multidisciplinary project like we have. How do you all think <em>where</em> you&#8217;ve done this work has impacted how you&#8217;ve gone about it and the contributions you’ve been able to make?</p>



<p><strong>NGUYEN:</strong>&nbsp;I&#8217;m going to start with if we look around this table and we see who&#8217;s sitting at it, which is two chemists, a computer architect, and a coding theorist, and we come together and we&#8217;re like, what can we make that would be super, super impactful? I think that&#8217;s the answer right there, is that being at Microsoft and being in a culture that really fosters this type of interdisciplinary collaboration <em>is</em> the key to getting a project like this off the ground.</p>



<p><strong>SMITH:</strong>&nbsp;Yeah, absolutely. And we should acknowledge the gigantic contributions made by our collaborators at the University of Washington. Many of them would fall in <em>not</em> any of these three categories. They’re electrical engineers, they&#8217;re mechanical engineers, they&#8217;re pure biologists that we worked with. And each of them brought their own perspective, and particularly when you talk about going to a true end-to-end system, those perspectives were invaluable as we were trying to fit all the puzzle pieces together.</p>



<p><strong>STRAUSS:</strong> Yeah, absolutely. We&#8217;ve had great collaborations over time—University of Washington, ETH Zürich, Los Alamos National Lab, ChipIr, Twist Bioscience, Ansa Biotechnologies. Yeah, it’s been really great and a great set of different disciplines, all the way from coding theorists to the molecular biology and chemistry, electrical and mechanical engineering. One of the great things about research is there&#8217;s never a shortage of interesting questions to pursue, and for us, this particular work has opened the door to research in adjacent domains, including sustainability fields. DNA data storage requires small amounts of materials to accommodate the large amounts of data, and early on, we wanted to understand if DNA data storage was, as it seemed, a more sustainable way to store information. And we learned a lot. Bichlien and Jake, you had experience in green chemistry when you came to Microsoft. What new findings did we make, and what sustainability benefits do we get with DNA data storage? And, finally, what new sustainability work has the project led to?</p>



<p><strong>NGUYEN: </strong>As a part of this project, if we&#8217;re going to bring new technologies to the forefront, you know, to the world, we should make sure that they have a lower carbon footprint, for example, than previous technologies. And so we ran a life cycle assessment—which is a way to systematically evaluate the environmental impacts of anything of interest—and we did this on DNA data storage and compared it to electronic storage medium<a id="_ftnref1" href="#_ftn1">[1]</a>, and we noticed that if we were able to store all of our digital information in DNA, that we would have benefits associated with carbon emissions. We would be able to reduce that because we don&#8217;t need as much infrastructure compared to the traditional storage methods. And there would be an energy reduction, as well, because this is a passive way of archival data storage. So that was, you know, the main takeaways that we had. But that also, kind of, led us to think about other technologies that would be beneficial beyond data storage and how we could use the same kind of life cycle thinking towards that.</p>



<p><strong>SMITH:</strong> This design approach that you&#8217;ve, you know, talked about us stumbling on, not inventing but seeing other people doing in the literature and trying to implement ourselves on the DNA Data Storage Project, you know, is something that can be much bigger than any single material. And where we think there&#8217;s a, you know, chance for folks like ourselves at Microsoft Research to make a real impact on this sustainability-focused design is through the application of machine learning, artificial intelligence—the <em>new</em> tools that will allow us to look at much bigger design spaces than we could previously to evaluate sustainability metrics that were not possible when everything was done manually and to ultimately, you know, at the end of the day, take a sustainability-first look at what a material should be composed of. And so we&#8217;ve tried to prototype this with a few projects. We had another wonderful collaboration with the University of Washington where we looked at recyclable circuit boards and a novel material called a vitrimer that it could possibly be made out of<a href="#_ftn2" id="_ftnref2">[2]</a>. We&#8217;ve had another great collaboration with the University of Michigan, where we&#8217;ve looked at the design of charge-carrying molecules in these things called flow batteries that have good potential for energy smoothing in, you know, renewables production, trying to get us out of that day-night, boom-bust cycle<a href="#_ftn3" id="_ftnref3">[3]</a>. And we had one more project, you know, this time with collaborators at the University of Berkeley, where we looked at, you know, design of a class of materials called a metal organic framework, which have great promise in low-energy-cost gas separation, such as pulling CO<sub>2</sub> out of the, you know, plume of a smokestack or, you know, ideally out of the air itself<a href="#_ftn4" id="_ftnref4">[4]</a>.</p>



<p><strong>STRAUSS:</strong> For me, the DNA work has made me much more open to projects outside my own research area—as Bichlien mentioned, my core research area is computer architecture, but we&#8217;ve ventured in quite a bit of other areas here—and going way beyond my own comfort zone and really made me love interdisciplinary projects like this and try, really try, to do the most important work I can. And this is what attracted me to these other areas of environmental sustainability that Bichlien and Jake covered, where there&#8217;s absolutely no lack of problems. Like them, I&#8217;m super interested in using AI to solve many of them. So how do each of you think working on the DNA Data Storage Project has influenced your research approach more generally and how you think about research questions to pursue next?</p>



<p><strong>YEKHANIN:</strong> It definitely expanded the horizons a lot, like, just, kind of, just having this interactions with people, kind of, whose core areas of research are so different from my own and also a lot of learning even within my own field that we had to do to, kind of, carry this project out. So, I mean, it was a great and rewarding experience.</p>



<p><strong>NGUYEN: </strong>Yeah, for me, it&#8217;s kind of the opposite of Karin, right. I started as an organic chemist and then now really, one, appreciate the breadth and depth of going from a concept to a real end-to-end prototype and all the requirements that you need to get there. And then also, really the importance of having, you know, a background in computer science and really being able to understand the lingo that is used <em>in</em> multidisciplinary projects because you might say something and someone else interprets it very differently, and it&#8217;s because you&#8217;re not speaking the same language. And so that understanding that you have to really be … you have to learn a little bit of vocabulary from each person and understand how they contribute and then how <em>your</em> ideas can contribute to their ideas has been really impactful in my career here.</p>



<p><strong>SMITH: </strong>Yeah, I think the key change in approach that I took away—and I think many of us took away from the DNA Data Storage Project—was rather than starting with an academic question, we started with a vision of what we wanted to happen, and then we derived the research questions from analyzing what would need to happen in the world—what are the bottlenecks that need to be solved in order for us to achieve, you know, that goal? And this is something that we&#8217;ve taken with us into the sustainability-focused research and, you know, something that I think will affect all the research I do going forward.</p>



<p><strong>STRAUSS:</strong> Awesome. As we close, let&#8217;s reflect a bit on what a world in which DNA data storage is widely used might look like. If everything goes as planned, what do you hope the lasting impact of this work will be? Sergey, why don’t you lead us off.</p>



<p><strong>YEKHANIN:</strong> Sure, I remember that, like, when … in the early days when I started working on this project actually, you, Karin, told me that you were taking an Uber ride somewhere and you were talking to the taxi driver, and the taxi driver—I don&#8217;t know if you remember that—but the taxi driver mentioned that he has a camera which is recording everything that&#8217;s happening in the car. And then you had a discussion with him about, like, how long does he keep the data, how long does he keep the videos. And he told you that he keeps it for about a couple of days because it&#8217;s too expensive. But otherwise, like, if it weren&#8217;t that expensive, he would keep it for much, much longer because, like, he wants to have these recordings if later somebody is upset about the ride and, I don’t know, he is getting sued or something. So this is, like, this is one small narrow application area where DNA data storage would clearly, kind of, if it happens, then it will solve it. Because then, kind of, this long-term archival storage will become very cheap, available to everybody; it would become a commodity basically. There are many things that will be enabled, like this helping the Uber drivers, for instance. But also one has to think of, of course, like, about, kind of, the broader implications so that we don&#8217;t get into something negative because again this power of recording everything and storing everything, it can also lead to some use cases that might be, kind of, morally wrong. So, again, hopefully by the time that we get to, like, really wide deployments of this technology, the regulation will also be catching up and the, like, we will have great use cases and we won’t have bad ones. I mean, that&#8217;s how I think of it. But definitely there are lots of, kind of, great scenarios that this can enable.</p>



<p><strong>SMITH: </strong>Yeah. I&#8217;ll grab onto the word you use there, which is making DNA a <em>commodity</em>. And one of the things that I hope comes out of this project, you know, besides all the great benefits of DNA data storage itself is spillover benefits into the field of health—where if we make DNA synthesis at large scale truly a commodity thing, which I hope some of the work that we&#8217;ve done to really accelerate the throughput of synthesis will do—then this will open new doors in what we can do in terms of gene synthesis, in terms of, like, fundamental biotech research that will lead to that next set of drugs and, you know, give us medications or treatments that we could not have thought possible if we were not able to synthesize DNA and related molecules at that scale.</p>



<p><strong>NGUYEN: </strong>So much information gets lost because of just time. And so I think being able to recover really ancient history that humans wrote in the future, I think, is something that I really hope could be achieved because we&#8217;re so information rich, but in the course of time, we become information poor, and so I would like for our future generations to be able to understand the life of, you know, an everyday 21st-century person.</p>



<p><strong>STRAUSS:</strong> Well, Bichlien, Jake, Sergey, it&#8217;s been fun having this conversation with you today and collaborating with you in all of this amazing project [MUSIC] and all the research we&#8217;ve done together. Thank you so much.</p>



<p><strong>YEKHANIN:</strong>&nbsp;Thank you, Karin.</p>



<p><strong>SMITH: </strong>Thank you.</p>



<p><strong>NGUYEN: </strong>Thanks.</p>



<p>[MUSIC FADES]</p>

				</span>
			</div>
			<button
				class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle"
				aria-expanded="false"
				data-show-less-text="Show less"
				type="button"
				aria-controls="show-more-show-less-toggle-1"
				aria-label="Show more content"
				data-alternate-aria-label="Show less content">
				Show more			</button>
		</div>
	</div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p><a id="_ftn1" href="#_ftnref1">[1]</a> The team presented the findings from their life cycle assessment of DNA data storage in the paper <a href="https://www.microsoft.com/en-us/research/publication/architecting-datacenters-for-sustainability-greener-data-storage-using-synthetic-dna/">Architecting Datacenters for Sustainability: Greener Data Storage using Synthetic DNA</a>.</p>



<p><a href="#_ftnref2" id="_ftn2">[2]</a> For more information, check out the podcast episode <a href="https://www.microsoft.com/en-us/research/podcast/collaborators-sustainable-electronics-with-jake-smith-and-aniruddh-vashisth/">Collaborators: Sustainable electronics with Jake Smith and Aniruddh Vashisth</a> and the paper <a href="https://www.microsoft.com/en-us/research/publication/recyclable-vitrimer-based-printed-circuit-boards-for-sustainable-electronics/">Recyclable vitrimer-based printed circuit boards for sustainable electronics</a>.</p>



<p><a href="#_ftnref3" id="_ftn3">[3]</a> For more information, check out the podcast episode <a href="https://www.microsoft.com/en-us/research/podcast/collaborators-renewable-energy-storage-with-bichlien-nguyen-and-david-kwabi/">Collaborators: Renewable energy storage with Bichlien Nguyen and David Kwabi</a>.</p>



<p><a href="#_ftnref4" id="_ftn4">[4]</a> For more information, check out the paper <a href="https://www.microsoft.com/en-us/research/publication/mofdiff-coarse-grained-diffusion-for-metal-organic-framework-design/?msockid=3be03cfe91b667f415c429e2900566db">MOFDiff: Coarse-grained Diffusion for Metal-Organic Framework Design</a>.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/podcast/ideas-the-journey-to-dna-data-storage/">Ideas: The journey to DNA data storage</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Introducing Yasuyuki Matsushita: Tackling societal challenges with AI at Microsoft Research Asia &#8211; Tokyo </title>
		<link>https://www.microsoft.com/en-us/research/blog/introducing-yasuyuki-matsushita-tackling-societal-challenges-with-ai-at-microsoft-research-asia-tokyo/</link>
		
		<dc:creator><![CDATA[Alyssa Hughes (2ADAPTIVE LLC dba 2A Consulting)]]></dc:creator>
		<pubDate>Mon, 18 Nov 2024 16:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1104399</guid>

					<description><![CDATA[<p>Yasuyuki Matsushita rejoins Microsoft to lead the new Microsoft Research Asia - Tokyo lab. Learn more about his journey and his perspective on the Tokyo lab's role in evolution of AI.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/introducing-yasuyuki-matsushita-tackling-societal-challenges-with-ai-at-microsoft-research-asia-tokyo/">Introducing Yasuyuki Matsushita: Tackling societal challenges with AI at Microsoft Research Asia &#8211; Tokyo </a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<p>Earlier this year, Microsoft Research <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://news.microsoft.com/apac/2024/04/10/microsoft-to-invest-us2-9-billion-in-ai-and-cloud-infrastructure-in-japan-while-boosting-the-nations-skills-research-and-cybersecurity/?msockid=397efc5596ce69550b73ee32976668d9">announced<span class="sr-only"> (opens in new tab)</span></a> its newest lab in Tokyo, Japan. Today, we are celebrating its grand opening, reinforcing Microsoft Research&#8217;s commitment to AI research across the Asia-Pacific region. This new lab will focus on embodied AI, well-being and neuroscience, societal AI, and industry innovation—all areas that align with Japan’s socioeconomic priorities. This initiative will enhance collaboration with local academic and industrial partners, contributing to global innovation and talent development. </p>



<p>We recently spoke with Yasuyuki Matsushita, head of the newly established Tokyo lab. Matsushita, who worked at Microsoft Research Asia from 2003 to 2015, served as a professor at Osaka University for the past decade, before returning in October. He reflects on his journey, the evolution of technology, and the opportunities ahead for Microsoft Research Asia &#8211; Tokyo.</p>



<figure class="wp-block-image aligncenter size-full is-resized"><img loading="lazy" decoding="async" width="678" height="1017" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/image.jpeg" alt="Yasuyuki Matsushita, Senior Principal Research Manager, Microsoft Research Asia - Tokyo" class="wp-image-1104402" style="width:459px;height:auto" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/image.jpeg 678w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/image-200x300.jpeg 200w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/image-120x180.jpeg 120w" sizes="(max-width: 678px) 100vw, 678px" /><figcaption class="wp-element-caption">Yasuyuki Matsushita, Microsoft Research Asia &#8211; Tokyo</figcaption></figure>



<h2 class="wp-block-heading h3" id="why-return-to-microsoft-research-asia">Why return to Microsoft Research Asia?</h2>



<p><strong>Question:</strong> We are excited to have you leading the new lab in Tokyo. You worked at Microsoft Research Asia in Beijing from 2003 to 2015 before transitioning to academia. What motivated you to return after nearly a decade? </p>



<p><strong>Yasuyuki Matsushita:</strong> Microsoft Research Asia has always been an exceptional place for conducting cutting-edge research, especially in the AI era. Earlier this year, I learned about Microsoft Research Asia’s expansion, including the establishment of a new lab in Tokyo. This presented an exciting opportunity to make a meaningful impact both locally and globally, sparking my motivation to return. Additionally, Microsoft is at the forefront of AI advancements, making this an ideal moment to re-engage. I’m confident that my work can contribute meaningfully to this dynamic field. The pace of AI development today is unmatched, making this an exhilarating time to be involved. </p>



<h2 class="wp-block-heading h3" id="what-has-changed-over-the-decade">What has changed over the decade?&nbsp;</h2>



<p><strong>Question:</strong> Now that you’ve been back for a few weeks, from your perspective, what has changed at Microsoft Research Asia, and what has remained the same since you were last here? </p>



<p><strong>Yasuyuki Matsushita:</strong> The most immediate change I’ve noticed is the array of employee tools and resources, which have evolved significantly over the past decade. I’m still familiarizing myself with these new systems, designed to optimize efficiency and collaboration. Over the past ten years, Microsoft has played a key role in driving digital transformation for other companies, and it has also transformed internally. </p>



<p>Beyond these changes, much of what made Microsoft Research Asia unique remains the same. The culture and people continue to foster an environment of innovation and collaboration. The organization still attracts exceptional talent, and the spirit of research is as vibrant as ever. One of its greatest strengths is its open, collaborative approach. It has maintained long-standing partnerships with universities and research institutions, which encourage cross-regional, cross-cultural, and interdisciplinary exchanges. This synergy stimulates innovation and supports industry development. The commitment to excellence remains at the heart of Microsoft Research Asia&#8217;s identity. </p>



<h2 class="wp-block-heading h3" id="plans-for-the-microsoft-research-asia-tokyo-lab">Plans for the Microsoft Research Asia &#8211; Tokyo lab&nbsp;</h2>



<p><strong>Question</strong>: With Microsoft Research Asia expanding regionally to places like Tokyo, Vancouver, Singapore, and Hong Kong, what are your plans as the head of the Tokyo lab, and how do you see it contributing to the region’s innovation ecosystem?</p>



<p><strong>Yasuyuki Matsushita:</strong> My primary goal is to align the Tokyo lab’s growth with Microsoft Research Asia’s mission to advance science and technology for the benefit of humanity. The research efforts we’re focusing on in this lab aim to address pressing societal issues while advancing AI technologies to benefit society as a whole. </p>



<p>For instance, Japan’s aging population presents unique challenges that require efficient societal solutions—an issue faced by many nations today. Through our research, we aim to generate insights that can be applied globally to proactively address and mitigate such challenges. </p>



<p>Japan also has a strong legacy of scientific research in fields like electronics, materials science, and robotics. Its advanced industrial base, featuring renowned companies across the automotive, electronics, and machinery sectors, provides rich application scenarios for our research outcomes. Additionally, Japan’s robust education system supplies an intellectual foundation crucial for our in-depth research. </p>



<p>We’re dedicated to maintaining open research practices. By publishing our findings and open-sourcing our tools, we ensure our work benefits the broader industry and enriches the global knowledge pool. Our goal is to share insights that drive societal progress and innovation worldwide.</p>



<h2 class="wp-block-heading h3" id="cultivating-the-next-generation">Cultivating the next generation&nbsp;</h2>



<p><strong>Question:</strong> Talent is at the heart of Microsoft Research’s mission and culture. What kind of talent is Microsoft Research Asia &#8211; Tokyo looking for? In what ways can the Tokyo lab enhance its efforts to cultivate the next generation of tech innovators for the region? </p>



<p><strong>Yasuyuki Matsushita:</strong> One of the key advantages of being part of Microsoft is the close connection we have to real-world applications. This bridge between research and practice allows our work to have a direct societal impact, ensuring that innovative technology results in meaningful and beneficial outcomes. </p>



<p>When recruiting new talent, we seek bright, self-driven individuals with an innate curiosity and a passion for solving societal challenges. The most vital trait we look for is a deep desire to understand the “why” behind complex problems. While technical expertise is essential, a commitment to addressing social issues fuels creativity and drives meaningful progress. This blend of curiosity and purpose sparks innovation and propels us forward at Microsoft Research Asia. </p>



<p>At the Tokyo lab, a core part of our vision is cultivating the next wave of tech innovators. We plan to build on the legacy of successful talent programs that Microsoft Research Asia has championed throughout the region, like joint research initiatives, visiting scholar programs, and internship opportunities. These provide early career professionals and students with invaluable hands-on experiences, equipping them with essential research skills and deepening their understanding of complex technological challenges. </p>



<p>We’re committed to creating a nurturing environment where talent can thrive, collaborate, and contribute to the global tech landscape. By combining innovation with real-world impact, we aim to inspire the next generation to push boundaries and advance society.</p>



<h2 class="wp-block-heading h3" id="rapid-evolution-in-computer-vision">Rapid evolution in computer vision&nbsp;</h2>



<p><strong>Question:</strong> In today’s world, everything is moving toward digitization and intelligence. Ten years ago, your research focused on photometry and video analysis. Can you share some key outcomes from that period and explain how you see emerging technologies like AI influencing the field of computer vision? </p>



<p><strong>Yasuyuki Matsushita:</strong> Back then, my research centered on computer vision, specifically on photometry for 3D reconstruction and video analysis aimed at enhancing video quality. One of the standout projects during that period was the development of a gigapixel camera capable of capturing high-resolution 3D information. This camera played a crucial role in the <a href="https://www.microsoft.com/en-us/research/video/eheritage-program-collaboration-with-dunhuang-academy/">Dunhuang Mogao Grottoes project</a>, which sought to digitally preserve the cultural heritage of Dunhuang’s murals and Buddha shrines with unprecedented accuracy. <span data-contrast="auto" xml:lang="EN-US" lang="EN-US" class="TextRun EmptyTextRun SCXW203067509 BCX8" style="-webkit-user-drag: none; -webkit-tap-highlight-color: transparent; margin: 0px; padding: 0px; user-select: text; font-size: 12pt; line-height: 20.85px; font-family: Aptos, Aptos_EmbeddedFont, Aptos_MSFontService, sans-serif; font-variant-ligatures: none !important;"></span> </p>



<p>Another notable project was the development of video stabilization technology, which was integrated into Windows 7 as part of Media Foundation. This technology improved video quality by compensating for unwanted camera movements, delivering smoother and more professional-looking output. The creation of real-time algorithms capable of processing and stabilizing video was groundbreaking at that time. </p>



<p>Since then, the introduction of deep learning, large datasets, and sophisticated neural network architectures has propelled computer vision to new heights. Tasks that were once considered difficult, such as object detection, recognition, and segmentation, are now standard with modern AI techniques. Current research continues to push the boundaries by exploring innovative network architectures, new learning strategies, and enhanced datasets. A particularly exciting trend is the use of AI in real-world interactive scenarios, leading to the emergence of embodied AI, which is a major focus of my current work.</p>



<h2 class="wp-block-heading h3" id="understanding-embodied-ai-beyond-robotics">Understanding embodied AI beyond robotics&nbsp;</h2>



<p><strong>Question:</strong> Your current research interests include embodied AI, which is also one of the key areas at Microsoft Research Asia &#8211; Tokyo. What exactly is embodied AI, and how does it differ from robotics? </p>



<p><strong>Yasuyuki Matsushita:</strong> Embodied AI goes beyond traditional robotics. While robots are typically machines equipped with actuators designed to execute specific tasks, embodied AI focuses on developing intelligent systems that can perform complex tasks while understanding and interacting within physical and virtual environments. Robotics and AI have developed independently, but embodied AI is the convergence of these two fields, integrating AI with physical agents that can perceive, act, and learn in dynamic real-world environments. </p>



<p>This field is inherently interdisciplinary, involving aspects such as robotic control, reinforcement learning, spatial awareness, human-robot interaction, reasoning, and more. For instance, embodied AI includes the ability to infer cause and effect, such as understanding that an unsupported laptop will fall due to gravity. These types of interactions and interpretations stem from engaging with and understanding the physical world, making embodied AI an exciting and multifaceted area of research. </p>



<p>Given the complexity of embodied AI, no single organization can cover all aspects of its development alone. We look forward to collaborating with local industry and academic institutions in Japan, leveraging their expertise alongside our strengths in AI to advance the field. </p>



<h2 class="wp-block-heading h3" id="advice-for-aspiring-researchers-in-computer-vision-and-ai">Advice for aspiring researchers in computer vision and AI&nbsp;</h2>



<p><strong>Question:</strong> You&#8217;ve had an extensive career spanning academia and industry. From your experience as both an educator and a researcher, what advice would you give to young people interested in pursuing research in computer vision and AI? </p>



<p><strong>Yasuyuki Matsushita:</strong> For students interested in computer vision and AI, a strong foundation in mathematics and computer science is essential, even as specific research topics and technologies evolve. A deep understanding of fundamental mathematical concepts, such as gradients, Jacobians, and vector spaces, is indispensable. Mastery of these principles will be beneficial regardless of changes in programming languages or development platforms. </p>



<p>Maintaining a mindset of continuous learning is equally important, as the field is constantly evolving. For example, deep learning was not as prominent a decade ago but is now central to the field. At Microsoft, we emphasize the importance of a growth mindset—being adaptable, open to new technologies, and willing to pivot with industry advancements. Early career professionals should cultivate the ability to quickly acquire new skills while building on their foundational knowledge. This adaptability is key to long-term success in research and development.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/introducing-yasuyuki-matsushita-tackling-societal-challenges-with-ai-at-microsoft-research-asia-tokyo/">Introducing Yasuyuki Matsushita: Tackling societal challenges with AI at Microsoft Research Asia &#8211; Tokyo </a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>BiomedParse: A foundation model for smarter, all-in-one biomedical image analysis</title>
		<link>https://www.microsoft.com/en-us/research/blog/biomedparse-a-foundation-model-for-smarter-all-in-one-biomedical-image-analysis/</link>
		
		<dc:creator><![CDATA[Brenda Potts]]></dc:creator>
		<pubDate>Mon, 18 Nov 2024 10:14:40 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1101105</guid>

					<description><![CDATA[<p>BiomedParse reimagines medical image analysis, integrating advanced AI to capture complex insights across imaging types—a step forward for diagnostics and precision medicine.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/biomedparse-a-foundation-model-for-smarter-all-in-one-biomedical-image-analysis/">BiomedParse: A foundation model for smarter, all-in-one biomedical image analysis</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/BiomedParse-BlogHeroFeature-1400x788-1.jpg" alt="A stylized illustration of a green line-drawn hand holding a transparent prism with colorful bands of light being refracted through it against a black background." class="wp-image-1102929" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/BiomedParse-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/BiomedParse-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/BiomedParse-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/BiomedParse-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/BiomedParse-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/BiomedParse-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/BiomedParse-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/BiomedParse-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/BiomedParse-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/BiomedParse-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure>



<p>In cancer diagnosis or advanced treatments like immunotherapy, every detail in a medical image counts. Radiologists and pathologists rely on these images to track tumors, understand their boundaries, and analyze how they interact with surrounding cells. This work demands pinpoint accuracy across several tasks—identifying whether a tumor is present, locating it precisely, and mapping its contours on complex CT scans or pathology slides.&nbsp;</p>



<p>Yet, these crucial steps—object recognition, detection, and segmentation—are often tackled separately, which can limit the depth of analysis. Current tools like <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.nature.com/articles/s41467-024-44824-z" target="_blank" rel="noreferrer noopener">MedSAM<span class="sr-only"> (opens in new tab)</span></a> and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://segment-anything.com/" target="_blank" rel="noreferrer noopener">SAM<span class="sr-only"> (opens in new tab)</span></a> focus on segmentation only, thus missing out on the opportunity to blend these insights holistically and relegating object as an afterthought.&nbsp;</p>



<p>In this blog, we introduce <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.nature.com/articles/s41592-024-02499-w" target="_blank" rel="noreferrer noopener">BiomedParse<span class="sr-only"> (opens in new tab)</span></a>, a new approach for holistic image analysis by treating object as the first-class citizen. By unifying object recognition, detection, and segmentation into a single framework, BiomedParse allows users to specify what they’re looking for through a simple, natural-language prompt. The result is a more cohesive, intelligent way of analyzing medical images that supports faster, more integrated clinical insights. </p>



<p>While biomedical segmentation datasets abound, there are relatively few prior works on object detection and recognition in biomedicine, let alone datasets covering all three tasks. To pretrain BiomedParse, we created the first such dataset by harnessing OpenAI’s GPT-4 for data synthesis from <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://huggingface.co/datasets/microsoft/BiomedParseData" target="_blank" rel="noreferrer noopener">standard segmentation datasets<span class="sr-only"> (opens in new tab)</span></a>.</p>



<p>BiomedParse is a single&nbsp;foundation model that can accurately segment biomedical objects across nine modalities, as seen in Figure 1, outperforming prior best methods while requiring orders of magnitude fewer user operations, as it doesn’t require an object-specific bounding box. By learning semantic representation for individual object types, BiomedParse’s superiority is particularly pronounced in the most challenging cases with irregularly shaped objects. Through joint pretraining of object recognition, detection, and segmentation, BiomedParse opens new possibilities for holistic image analysis and image-based discovery in biomedicine.&nbsp;&nbsp;</p>



<figure class="wp-block-image aligncenter size-full is-resized"><img loading="lazy" decoding="async" width="2185" height="2560" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure1_BiomedParse-scaled.jpg" alt="a, The GPT-4 constructed ontology showing a hierarchy of object types that are used to unify semantic concepts across datasets. Bar plots showing the number of images containing that object type. b, Bar plot showing the number of image–mask–description triples for each modality in BiomedParseData. CT is abbreviation for Computed Tomography. MRI is abbreviation for Magnetic Resonance Imaging. OCT is abbreviation for Optical Coherence Tomography. c, Flowchart of BiomedParse. BiomedParse takes an image and a text prompt as input and then outputs the segmentation masks for the objects specified in the prompt. Image-specific manual interaction such as bounding box or clicks is not required in our framework. To facilitate semantic learning for the image encoder, BiomedParse also incorporates a learning objective to classify the meta-object type. For online inference, GPT-4 is used to resolve text prompt into object types using the object ontology, which also uses the meta-object type output from BiomedParse to narrow down candidate semantic labels. d, Uniform Manifold Approximation and Projection (UMAP) plots contrasting the text embeddings for different cell types derived from BiomedParse text encoder (left) and PubMedBERT (right). e, UMAP plots contrasting the image embeddings for different cell types derived from BiomedParse image encoder (left) and Focal (right). " class="wp-image-1102950" style="width:825px;height:auto" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure1_BiomedParse-scaled.jpg 2185w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure1_BiomedParse-256x300.jpg 256w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure1_BiomedParse-874x1024.jpg 874w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure1_BiomedParse-768x900.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure1_BiomedParse-1311x1536.jpg 1311w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure1_BiomedParse-1748x2048.jpg 1748w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure1_BiomedParse-154x180.jpg 154w" sizes="(max-width: 2185px) 100vw, 2185px" /><figcaption class="wp-element-caption">Figure 1. Overview of BiomedParse and BiomedParseData<em>.</em></figcaption></figure>



<h2 class="wp-block-heading" id="image-parsing-a-unifying-framework-for-holistic-image-analysis">Image parsing: a unifying framework for holistic image analysis&nbsp;</h2>



<p>Back in 2005, researchers first introduced the concept of “image parsing”—a unified approach to image analysis that jointly conducts object recognition, detection, and segmentation. Built on Bayesian networks, this early model offered a glimpse into a future of joint learning and reasoning in image analysis, though it was limited in scope and application. Fast forward to today, cutting-edge advances in generative AI have breathed new life into this vision. With our model, BiomedParse, we have created a foundation for biomedical image parsing that leverages interdependencies across the three subtasks, thus addressing key limitations in traditional methods. BiomedParse enables users to simply input a natural-language description of an object, which the model uses to predict both the object label and its segmentation mask, thus eliminating the need for a bounding box (Figure 1c). In other words, this joint learning approach lets users segment objects based on text alone.</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1044939">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">on-demand event</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/story/sep-2024-brief/?OCID=msr_researchforum_MCR_Blog_Promo" aria-label="Microsoft Research Forum Episode 4" data-bi-cN="Microsoft Research Forum Episode 4" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_Panel_1400x788.jpg" alt="Research Forum | Episode 4 Panel | John Langford, Hoifung Poon, Katja Hofmann, Jianwei Yang" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">Microsoft Research Forum Episode 4</h2>
				
								<p class="large">Learn about the latest multimodal AI models, advanced benchmarks for AI evaluation and model self-improvement, and an entirely new kind of computer for AI inference and hard optimization. </p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://www.microsoft.com/en-us/research/story/sep-2024-brief/?OCID=msr_researchforum_MCR_Blog_Promo" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Watch on-demand" data-bi-cN="Microsoft Research Forum Episode 4" target="_blank">
							Watch on-demand						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="harnessing-gpt-4-for-large-scale-data-synthesis-from-existing-datasets">Harnessing GPT-4 for large-scale data synthesis from existing datasets&nbsp;</h2>



<p>We created the first dataset for biomedical imaging parsing by harnessing GPT-4 for large-scale data synthesis from 45 existing biomedical segmentation datasets (Figure 1a and 1b). The key insight is to leverage readily available natural-language descriptions already in these datasets and use GPT-4 to organize this often messy, unstructured text with established biomedical object taxonomies.&nbsp;&nbsp;</p>



<p>Specifically, we use GPT-4 to help create a unifying biomedical object taxonomy for image analysis and harmonize natural language descriptions from existing datasets with this taxonomy. We further leverage GPT-4 to synthesize additional variations of object descriptions to facilitate more robust text prompting.&nbsp;&nbsp;</p>



<p>This enables us to construct BiomedParseData, a biomedical image analysis dataset comprising over 6 million sets of images, segmentation masks, and text descriptions drawn from more than 1 million images. This dataset includes 64 major biomedical object types, 82 fine-grained subtypes, and spans nine imaging modalities.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="2242" height="2560" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure2_BiomedParse-scaled.jpg" alt="a, Box plot comparing the Dice score between our method and competing methods on 102,855 test instances (image–mask–label triples) across nine modalities. MedSAM and SAM require bounding box as input. We consider two settings: oracle bounding box (minimum bounding box covering the gold mask); bounding boxes generated from the text prompt by Grounding DINO, a state-of-the-art text-based grounding model. Each modality category contains multiple object types. Each object type was aggregated as the instance median to be shown in the plot. n in the plot denotes the number of test instances in the corresponding modality. b, Nine examples comparing the segmentation results by BiomedParse and the ground truth, using just the text prompt at the top. c, Box plot comparing the Dice score between our method and competing methods on a cell segmentation test set with n=42 images. BiomedParse requires only a single user operation (the text prompt ‘Glandular structure in colon pathology’). By contrast, to get competitive results, MedSAM and SAM require 430 operations (one bounding box per an individual cell). d, Five examples contrasting the segmentation results by BiomedParse and MedSAM, along with text prompts used by BiomedParse and bounding boxes used by MedSAM. e, Comparison between BiomedParse and MedSAM on a benign tumor image (top) and a malignant tumor image (bottom). The improvement of BiomedParse over MedSAM is even more pronounced on abnormal cells with irregular shapes. f, Box plot comparing the two-sided K–S test P values between valid text prompt and invalid text prompt. BiomedParse learns to reject invalid text prompts describing object types not present in the image (small P value). We evaluated a total of 4,887 invalid prompts and 22,355 valid prompts. g, Plot showing the precision and recall of our method on detecting invalid text prompts across different K–S test P value cutoff. h,i, Scatter-plots comparing the area under the receiver operating characteristic curve (AUROC) (h) and F1 (i) between BiomedParse and Grounding DINO on detecting invalid descriptions. " class="wp-image-1102956" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure2_BiomedParse-scaled.jpg 2242w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure2_BiomedParse-263x300.jpg 263w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure2_BiomedParse-897x1024.jpg 897w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure2_BiomedParse-768x877.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure2_BiomedParse-1345x1536.jpg 1345w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure2_BiomedParse-1793x2048.jpg 1793w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure2_BiomedParse-158x180.jpg 158w" sizes="(max-width: 2242px) 100vw, 2242px" /><figcaption class="wp-element-caption">Figure 2: Comparison on large-scale biomedical image segmentation datasets.</figcaption></figure>



<h2 class="wp-block-heading" id="state-of-the-art-performance-across-64-major-object-types-in-9-modalities">State-of-the-art performance across 64 major object types in 9 modalities</h2>



<p>We evaluated BiomedParse on a large held-out test set with 102,855 image-mask-label sets across 64 major object types in nine modalities. BiomedParse outperformed prior best methods such as MedSAM and SAM, even when oracle per-object bounding boxes were provided. In the more realistic setting when MedSAM and SAM used a state-of-the-art object detector (Grounding DINO) to propose bounding boxes, BiomedParse outperformed them by a wide margin, between 75 and 85 absolute points in dice score (Figure 2a). BiomedParse also outperforms a variety of other prominent methods such as SegVol, Swin UNETR, nnU-Net, DeepLab V3+, and UniverSeg.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="2170" height="2560" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure3_BiomedParse-scaled.jpg" alt="a, Attention maps of text prompts for irregular-shaped objects, suggesting that BiomedParse learns rather faithful representation of their typical shapes. US, ultrasound. b–d, Scatter-plots comparing the improvement in Dice score for BiomedParse over MedSAM with shape regularity in terms of convex ratio (b), box ratio (c) and inversed rotational inertia (d). A smaller number in the x axis means higher irregularity on average. Each dot represents an object type. e, Six examples contrasting BiomedParse and MedSAM on detecting irregular-shaped objects. Plots are ordered from the least irregular one (left) to the most irregular one (right). f,g Comparison between BiomedParseData and the benchmark dataset used by MedSAM in terms of convex ratio (f) and box ratio (g). BiomedParseData is a more faithful representation of real-world challenges in terms of irregular-shaped objects. h, Box plots comparing BiomedParse and competing approaches on BiomedParseData and the benchmark dataset used by MedSAM. BiomedParse has a larger improvement on BiomedParseData, which contains more diverse images and more irregular-shaped objects. The number of object types are as follows: n=50 for MedSAM benchmark and n=112 for BiomedParseData. " class="wp-image-1102965" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure3_BiomedParse-scaled.jpg 2170w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure3_BiomedParse-254x300.jpg 254w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure3_BiomedParse-868x1024.jpg 868w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure3_BiomedParse-768x906.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure3_BiomedParse-1302x1536.jpg 1302w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure3_BiomedParse-1736x2048.jpg 1736w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure3_BiomedParse-153x180.jpg 153w" sizes="(max-width: 2170px) 100vw, 2170px" /><figcaption class="wp-element-caption">Figure 3. Evaluation on detecting irregular-shaped objects.</figcaption></figure>



<h2 class="wp-block-heading" id="recognizing-and-segmenting-irregular-and-complex-objects">Recognizing and segmenting irregular and complex objects</h2>



<p>Biomedical objects often have complex and irregular shapes, which present significant challenges for segmentation, even with oracle bounding box. By joint learning with object recognition and detection, BiomedParse learns to model object-specific shapes, and its superiority is particularly pronounced for the most challenging cases (Figure 3). Encompassing a large collection of diverse object types in nine modalities, BiomedParseData also provides a much more realistic representation of object complexity in biomedicine.&nbsp;&nbsp;</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="2276" height="2560" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure4_BiomedParse-scaled.jpg" alt="a, Six examples showing the results of object recognition by our method. Object recognition identifies and segments all objects in an image without requiring any user-provided input prompt. b–d, Scatter-plots comparing the F1 (b), Precision (c) and Recall (d) scores between BiomedParse and Grounding DINO on identifying objects presented in the image. e, Comparison between BiomedParse and Grounding DINO on object identification in terms of median F1 score across different numbers of objects in the image. f, Box plot comparing BiomedParse and MedSAM/SAM (using bounding boxes generated by Grounding DINO) on end-to-end object recognition (including segmentation) in relation to various modalities. g, Comparison between BiomedParse and MedSAM/SAM (using bounding boxes generated by Grounding DINO) on end-to-end object recognition (including segmentation) in relation to numbers of distinct objects in the image." class="wp-image-1102968" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure4_BiomedParse-scaled.jpg 2276w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure4_BiomedParse-267x300.jpg 267w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure4_BiomedParse-910x1024.jpg 910w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure4_BiomedParse-768x864.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure4_BiomedParse-1366x1536.jpg 1366w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure4_BiomedParse-1821x2048.jpg 1821w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure4_BiomedParse-160x180.jpg 160w" sizes="(max-width: 2276px) 100vw, 2276px" /><figcaption class="wp-element-caption">Figure 4. Evaluation on object recognition.</figcaption></figure>



<h2 class="wp-block-heading" id="promising-step-toward-scaling-holistic-biomedical-image-analysis">Promising step toward scaling holistic biomedical image analysis</h2>



<p>By operating through a simple text prompt, BiomedParse requires substantially less user effort than prior best methods that typically require object-specific bounding boxes, especially when an image contains a large number of objects (Figure 2c). By modeling object recognition threshold, BiomedParse can detect invalid prompt and reject segmentation requests when an object is absent from the image. BiomedParse can be used to recognize and segment all known objects in an image in one fell swoop (Figure 4). By scaling holistic image analysis, BiomedParse can potentially be applied to key precision health applications such as early detection, prognosis, treatment decision support, and progression monitoring.&nbsp;&nbsp;</p>



<p>Going forward, there are numerous growth opportunities. BiomedParse can be extended to handle more modalities and object types. It can be integrated into advanced multimodal frameworks such as <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://aka.ms/llava-med" target="_blank" rel="noreferrer noopener">LLaVA-Med<span class="sr-only"> (opens in new tab)</span></a> to facilitate conversational image analysis by “talking to the data.” To facilitate research in biomedical image analysis, we have made BiomedParse <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://aka.ms/biomedparse-release" target="_blank" rel="noreferrer noopener">open-source<span class="sr-only"> (opens in new tab)</span></a> with Apache 2.0 license. We’ve also made it available on <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://ai.azure.com/explore/models/MedImageParse/version/3/registry/azureml/latest" target="_blank" rel="noreferrer noopener">Azure AI<span class="sr-only"> (opens in new tab)</span></a> for direct deployment and real-time inference. For more information, check out our <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://microsoft.github.io/BiomedParse/" target="_blank" rel="noreferrer noopener">demo.<span class="sr-only"> (opens in new tab)</span></a>&nbsp;</p>



<p>BiomedParse is a joint work with Providence and the University of Washington&#8217;s Paul G. Allen School of Computer Science & Engineering, and brings collaboration from multiple teams within Microsoft*. It reflects Microsoft’s larger commitment to advancing multimodal generative AI for precision health, with other exciting progress such as <a href="https://www.microsoft.com/en-us/research/blog/gigapath-whole-slide-foundation-model-for-digital-pathology/" target="_blank" rel="noreferrer noopener">GigaPath<span class="sr-only"> (opens in new tab)</span></a>, <a href="https://www.microsoft.com/en-us/research/publication/large-scale-domain-specific-pretraining-for-biomedical-vision-language-processing/" target="_blank" rel="noreferrer noopener">BiomedCLIP<span class="sr-only"> (opens in new tab)</span></a>,  <a href="https://www.microsoft.com/en-us/research/project/project-maira/" target="_blank" rel="noreferrer noopener">LLaVA-Rad<span class="sr-only"> (opens in new tab)</span></a>, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/abs/2310.10765" target="_blank" rel="noreferrer noopener">BiomedJourney<span class="sr-only"> (opens in new tab)</span></a>, <a href="https://www.microsoft.com/en-us/research/publication/rad-dino-exploring-scalable-medical-image-encoders-beyond-text-supervision/" target="_blank" rel="noreferrer noopener">MAIRA<span class="sr-only"> (opens in new tab)</span></a>, <a href="https://www.microsoft.com/en-us/research/publication/rad-dino-exploring-scalable-medical-image-encoders-beyond-text-supervision/" target="_blank" rel="noreferrer noopener">Rad-DINO<span class="sr-only"> (opens in new tab)</span></a>, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/pdf/2309.07778v5" target="_blank" rel="noreferrer noopener">Virchow<span class="sr-only"> (opens in new tab)</span></a>. &nbsp;</p>



<p><em>(Acknowledgment footnote) *: Within Microsoft, it is a wonderful collaboration among Health Futures, MSR Deep Learning, and Nuance.&nbsp;</em></p>



<p>Paper co-authors: Theodore Zhao, Yu Gu, <a href="https://www.microsoft.com/en-us/research/people/jianwyan/">Jianwei Yang<span class="sr-only"> (opens in new tab)</span></a>, <a href="https://www.microsoft.com/en-us/research/people/naotous/">Naoto Usuyama<span class="sr-only"> (opens in new tab)</span></a>, Ho Hin Lee, Sid Kiblawi, <a href="https://www.microsoft.com/en-us/research/people/tristan/">Tristan Naumann<span class="sr-only"> (opens in new tab)</span></a>, <a href="https://www.microsoft.com/en-us/research/people/jfgao/">Jianfeng Gao<span class="sr-only"> (opens in new tab)</span></a>, Angela Crabtree, Jacob Abel, Christine Moung-Wen, Brian Piening, Carlo Bifulco, Mu Wei, <a href="https://www.microsoft.com/en-us/research/people/hoifung/">Hoifung Poon<span class="sr-only"> (opens in new tab)</span></a>, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://homes.cs.washington.edu/~swang/">Sheng Wang<span class="sr-only"> (opens in new tab)</span></a></p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/biomedparse-a-foundation-model-for-smarter-all-in-one-biomedical-image-analysis/">BiomedParse: A foundation model for smarter, all-in-one biomedical image analysis</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>GraphRAG: Improving global search via dynamic community selection</title>
		<link>https://www.microsoft.com/en-us/research/blog/graphrag-improving-global-search-via-dynamic-community-selection/</link>
		
		<dc:creator><![CDATA[Brenda Potts]]></dc:creator>
		<pubDate>Fri, 15 Nov 2024 16:52:47 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1101987</guid>

					<description><![CDATA[<p>Retrieval-augmented generation (RAG) helps AI systems provide more information to a large language model (LLM) when generating responses to user queries. A new method for conducting “global” queries can optimize the performance of global search in GraphRAG.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/graphrag-improving-global-search-via-dynamic-community-selection/">GraphRAG: Improving global search via dynamic community selection</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/GraphRAG-Global-Search-BlogHeroFeature-1400x788-1.jpg" alt="The image features three white icons on a gradient background transitioning from blue on the left to green on the right. The first icon, located on the left, depicts a hierarchical structure resembling a workflow with connected squares. The middle icon represents GraphRAG (interconnected nodes and lines). The third icon, on the right, shows a globe with a magnifying glass overlaying it, symbolizing global search." class="wp-image-1102437" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/GraphRAG-Global-Search-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/GraphRAG-Global-Search-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/GraphRAG-Global-Search-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/GraphRAG-Global-Search-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/GraphRAG-Global-Search-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/GraphRAG-Global-Search-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/GraphRAG-Global-Search-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/GraphRAG-Global-Search-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/GraphRAG-Global-Search-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/GraphRAG-Global-Search-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure>



<p>Retrieval-augmented generation (RAG) allows AI systems to provide additional information and context to a large language model (LLM) when generating a response to a user query. However, traditional RAG-based methods can struggle to retrieve information that requires high-level knowledge of the entire dataset, especially with abstract and global questions such as the keywordless query: “Catch me up on the last two weeks of updates.” These types of queries are known as “global” queries, as they require holistic understanding of the dataset to answer the question. <a href="https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/" target="_blank" rel="noreferrer noopener">GraphRAG</a> aims to tackle these questions in two main steps: indexing and query. The indexing engine first breaks down a collection of text documents into segments which are then clustered into hierarchical communities with entities and relationships connecting each segment up through higher levels of abstraction. We then use an LLM to generate a summary of each community, known as a community report. The indexing engine thus creates a hierarchical knowledge graph of the dataset, with each level in the hierarchy representing a different level of abstraction and summarization of the original material. In the query step, GraphRAG uses this structured knowledge to provide additional context to the LLM to help answer the question. In this blog post, we show a new method for conducting “global” queries that efficiently utilizes the knowledge graph representation and optimizes the performance of global search in GraphRAG.&nbsp;</p>



<h2 class="wp-block-heading" id="static-vs-dynamic-global-search">Static vs. dynamic global search</h2>



<p>The <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://microsoft.github.io/graphrag/query/global_search/" target="_blank" rel="noreferrer noopener">global search<span class="sr-only"> (opens in new tab)</span></a> algorithm in GraphRAG aims to answer abstract questions that require knowledge of the entire dataset. It generates answers by searching over communities at a predetermined level in the knowledge graph. Then the LLM combines and summarizes all the community reports at this level of abstraction. Finally, the summary is used as additional context for the LLM to generate the response to the user question. This map-reduce process allows the LLM to select relevant text from all the community reports to generate its final answer. This static approach is expensive and inefficient because it includes many lower-level reports that are not informative to the user query. Since it is unlikely that all community reports, especially at a high level, are relevant in answering the query, an approach that first considers the relevancy of the report prior to the resource-intensive map-reduce operation is highly desirable.&nbsp;&nbsp;</p>



<p>Here, we introduce dynamic community selection to the global search algorithm, which leverages the knowledge graph structure of the indexed dataset. Starting from the root of the knowledge graph, we use an LLM to rate how relevant a community report is in answering the user question. If the report is deemed irrelevant, we simply remove it and its nodes (or sub-communities) from the search process. On the other hand, if the report is deemed relevant, we then traverse down its child nodes and repeat the operation. Finally, only relevant reports are passed to the map-reduce operation to generate the response to the user. Figure 1 illustrates the dynamic community selection process in action.&nbsp;</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1400" height="517" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/GraphRAG-global-dynamics_Figure-1.png" alt="An image that shows the workflow of dynamic community selection in global search. Each node illustrates a community report, and the arrow indicates the rate operation." class="wp-image-1101996" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/GraphRAG-global-dynamics_Figure-1.png 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/GraphRAG-global-dynamics_Figure-1-300x111.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/GraphRAG-global-dynamics_Figure-1-1024x378.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/GraphRAG-global-dynamics_Figure-1-768x284.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/GraphRAG-global-dynamics_Figure-1-240x89.png 240w" sizes="(max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">Figure 1: Dynamic community selection workflow</figcaption></figure>



<p>The dynamic global search approach has two main benefits. First, it prunes irrelevant reports early on, reducing the total number of community reports to be considered in the map-reduce operation. Second, it enables users to search the entire knowledge graph, instead of predefining a static community level, and can lead to more detailed answers. This allows it to collect information at various levels of abstraction. Moreover, the rating operation is a classification problem, which is considerably easier to perform than summarization and text generation, therefore, a less complex model can be used. In our experiments leveraging OpenAI’s models, a GPT-4o-mini rater achieved a very similar retrieval rate as a GPT-4o rater, while operating at a fraction of both cost and time. Overall, we use the smaller and more cost-effective model, GPT-4o-mini, in the rate operation to prune any irrelevant community reports, then we use GPT-4o to perform the map-reduce operation to generate the final response.&nbsp;</p>



<h2 class="wp-block-heading" id="dynamic-community-selection-on-the-ap-news-dataset">Dynamic community selection on the AP News dataset</h2>



<p>To demonstrate the cost saving that dynamic global search brings while maintaining a similar response quality, we evaluated the two methods side by side on a dataset from AP News. We tested static and dynamic search on 50 global questions and assessed the final response quality using an LLM evaluator. Moreover, we compared the total token cost of the two methods. To compare the two methods directly, we constrained the maximum search depth on dynamic global search so that both methods used the same underlying information.</p>



<p>We use an LLM evaluator to select the best response (i.e. win rate)&nbsp;on 3 key metrics:&nbsp;</p>



<ul class="wp-block-list">
<li>Comprehensiveness: How much detail does the answer provide to cover all the aspects and details of the question?</li>



<li>Diversity: How varied and rich is the answer in providing different perspectives and insights on the question?</li>



<li>Empowerment: How well does the answer help the reader understand and make informed judgements about the topic?</li>
</ul>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1061244">
		

	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/about-microsoft-research/" aria-label="About Microsoft Research" data-bi-cN="About Microsoft Research" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/07/About-page-promo_1066x600.jpg" alt="" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">About Microsoft Research</h2>
				
								<p class="large">Advancing science and technology to benefit humanity</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://www.microsoft.com/en-us/research/about-microsoft-research/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="View our story" data-bi-cN="About Microsoft Research" target="_blank">
							View our story						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="significant-cost-reduction-while-maintaining-output-quality">Significant cost reduction while maintaining output quality</h2>



<p>The quality of responses generated from dynamic community selection are comparable to its static counterpart while reducing the total token cost. Our LLM evaluation shows that the output quality of the two methods is similar in the three key metrics across the 50 global questions on the AP News dataset, with no statistical significance between them. More importantly, we observed a significant reduction of total token costs when using the new method, with <strong>an average cost reduction of 77% </strong>over the existing static global search at community level 1. This is due to the large number of community reports being eliminated via the rating process, thus requiring fewer prompt and output tokens needed in the map-reduce operation. For instance, the existing static global search method processes about 1,500 level 1 community reports in the map-reduce operation, while only 470 community reports on average are selected in dynamic search to generate the final answer.&nbsp;&nbsp;</p>



<p>Moreover, if we allow dynamic search to continue the rating process further to deeper level community reports, we observe an improvement in its final responses. Here, we conducted the same experiment but allowed dynamic search to continue until community level 3. Out of the 50 global questions, 29 included more community reports than our static search baseline, suggesting that some community reports at deeper levels are relevant to the user question. Indeed, we observed a moderate and statistically significant improvement in both comprehensiveness and empowerment.&nbsp; Using an LLM evaluator to score pairs of responses, we observe that dynamic global search scores a win rate of 58% and 60%, respectively, against static search at level 1. Nevertheless, while the rating operation is performed by a smaller model and hence induces negligible cost, it can still lead to a higher overall cost due to the increased number of community reports that the map-reduce operation processes. In this experiment, the total cost with dynamic search at level 3 is 34% higher on average. Table 1 summarizes the results of static search at level 1 against dynamic search at level 1 and 3.&nbsp;</p>



<table width="100%" align="center" border="1" font-size="80%" cellpadding="3px">
  <tr>
    <td rowspan="2">Dynamic search</td>
    <td rowspan="2" style="text-align: center;">Num. queries</td>
    <td colspan="3" style="text-align: center;">Win rate against static search (level 1)</td>
    <td rowspan="2" style="text-align: center;">Relative cost</td>
</tr>
  <tr>
    <td style="text-align: center;">Comprehensiveness</td>
    <td style="text-align: center;">Diversity</td>
    <td style="text-align: center;">Empowerment</td>
  </tr>
  <tr>
    <td>Level 1</td>
    <td style="text-align: center;">50</td>
    <td style="text-align: center; background-color: #eeeeee;">49.5%</td>
    <td style="text-align: center; background-color: #eeeeee;">46.0%</td>
    <td style="text-align: center; background-color: #eeeeee;">48.0%</td>
    <td style="text-align: center;">-77%</td>
  </tr>
  <tr>
    <td>Level 3</td>
    <td style="text-align: center;">29*</td>
    <td style="text-align: center; background-color: #8de971;">58.8%</td>
    <td style="text-align: center; background-color: #eeeeee;">48.0%</td>
    <td style="text-align: center; background-color: #8de971;">60.0%</td>
    <td style="text-align: center;">+34%</td>
  </tr>
</table>



<div style="height:10px" aria-hidden="true" class="wp-block-spacer"></div>



<p class="has-text-align-center">Table 1. LLM evaluation on dynamic search vs static search at different community levels on global questions. We show the win rates of dynamic search at level 1 and level 3 against static search at level 1, that is the number of times the LLM evaluator prefers the generated responses from dynamic search over static search. Green background indicates there is statistical significance in the win rates, and grey cells if it is not significant. The right-most column shows the average relative token cost against static search at level 1. * Note that we only evaluate answers from dynamic search at community level 3, which contains more community reports than static search at level 1.&nbsp;</p>



<p>Overall, dynamic community selection achieved a cost reduction of 77% while achieving output quality similar to the baseline static approach. These appear to be promising numbers, but what of the result quality? Let’s examine some direct side-by-side comparisons with an example:&nbsp;</p>



<h2 class="wp-block-heading" id="comparable-response-quality-and-detail">Comparable response quality and detail</h2>



<table width="100%" align="center" border="1" font-size="80%" cellpadding="12px">
  <tr>
    <td colspan="2">Query: Across the dataset, describe the common trends in vaccination rates for major diseases.</td>
</tr>
  <tr style="vertical-align: top;">
    <td>Static global search at level 1</td>
    <td>Global search with dynamic community selection at level 3</td>
  </tr>
  <tr style="vertical-align: top;">
    <td>

<p>### Common Trends in Vaccination Rates for Major Diseases </p>

<p>#### Decline in Vaccination Rates </p>

<p>A significant trend observed across various regions is the decline in vaccination rates for major diseases. For instance, Minnesota has experienced a decrease in immunization rates, leading to recent measles outbreaks [Data: Reports (335)]. Similarly, the COVID-19 pandemic has exacerbated this issue, causing a drop in childhood immunization rates in Michigan and other areas [Data: Reports (1231, 915)]. This decline is often attributed to misinformation about vaccines, particularly the false claim linking the MMR vaccine to autism [Data: Reports (912, 1335)]. </p>

<p>#### Impact of COVID-19 </p>

The COVID-19 pandemic has had a profound impact on vaccination rates. Efforts to increase COVID-19 vaccination rates have been a major focus of public health policies, but the pandemic has also led to a decline in routine vaccinations among children [Data: Reports (920, 194)]. The pandemic has influenced public attitudes towards health measures and vaccinations, contributing to broader public health challenges [Data: Reports (920)].</p>

<p>#### Regional Variations </p>

<p>Vaccination rates vary significantly by region. West Virginia, for example, has one of the strictest school vaccination policies in the U.S., resulting in high child immunization rates [Data: Reports (1199)]. In contrast, Idaho has the highest overall childhood vaccination exemption rate in the U.S., posing significant public health risks [Data: Reports (998)]. Illinois faces challenges with low vaccination rates in schools, leading to measles outbreaks [Data: Reports (108, 82)]. </p>

<p>#### Public Health Initiatives </p>

<p>Various public health initiatives have been implemented to address vaccination rates. The CDC plays a crucial role in monitoring and reporting on vaccination rates for diseases such as measles, mumps, and polio, and provides vital information on preventable diseases [Data: Reports (914)]. Programs like the &#8216;Do It For Babydog&#8217; sweepstakes in West Virginia encourage vaccination among residents [Data: Reports (1199)]. Additionally, the Bill & Melinda Gates Foundation is heavily involved in funding and supporting vaccination programs for major diseases such as polio, malaria, HIV, and cholera [Data: Reports (1091)]. </p>

<p>### Conclusion </p>

<p>The dataset reveals a complex landscape of vaccination rates influenced by regional policies, public health initiatives, and the impact of the COVID-19 pandemic. While some regions have managed to maintain high vaccination rates through strict policies and public health campaigns, others face significant challenges due to misinformation and exemption rates. Global efforts and collaborations remain crucial in addressing these challenges and improving vaccination coverage worldwide.</p>
</td>
    <td>

<p>### Common Trends in Vaccination Rates for Major Diseases </p>

<p>#### Decline in Vaccination Rates </p>

<p>A significant trend observed across the dataset is the decline in vaccination rates for various diseases, including measles, mumps, rubella (MMR), and polio. <b>This decline is particularly evident among U.S. kindergartners, where vaccination coverage has dropped from 95.2% during the 2019-2020 school year to 93.1% in the 2022-2023 school year</b>. This reduction falls below the 95% threshold needed to achieve herd immunity, putting approximately 250,000 kindergartners at risk each year [Data: Reports (5668, 15469, 15703, 15481, 15471, +more)]. </p>

<p>#### Impact of the COVID-19 Pandemic </p>

<p>The COVID-19 pandemic has had a profound impact on vaccination rates. Efforts to promote COVID-19 vaccinations have been a major focus, with initiatives in regions like Michigan and Illinois offering free meals to students and promoting updated COVID-19 shots [Data: Reports (19125, 15466)]. However, the pandemic has also disrupted routine vaccination programs, leading to lower coverage for diseases such as measles and contributing to recent outbreaks [Data: Reports (15464, 15674, 15482, 15481, +more)]. </p>

<p>#### Regional Variations and Legislative Measures </p>

<p>There are notable regional variations in vaccination rates and exemption rates across the United States. For example, West Virginia has implemented strict immunization requirements for children entering school, resulting in high child immunization rates [Data: Reports (5674, 18874, 18899)]. In contrast, states like Minnesota and Illinois have seen increases in non-medical exemptions, contributing to lower vaccination rates and recent outbreaks [Data: Reports (15483, 15481, 108, 2705, +more)]. </p>

<p>#### Efforts to Improve Vaccination Rates </p>

<p>Various initiatives and legislative measures have been introduced to address declining vaccination rates. For instance, the Government of Sindh introduced a polio vaccination bill that includes provisions for imprisonment for parents who do not vaccinate their children [Data: Reports (15398)]. In the U.S., the CDC has recommended new COVID-19 shots for everyone aged 6 months and older and has launched initiatives to ensure equitable access to vaccines, especially in developing countries [Data: Reports (15847, 15571, 15691, 15694, +more)]. </p>

<p>### Conclusion </p>

<p>The dataset reveals a complex landscape of vaccination rates influenced by the COVID-19 pandemic, vaccine hesitancy, misinformation, and regional variations. While efforts to improve vaccination rates are ongoing, the decline in immunization coverage poses significant public health risks, highlighting the need for continued vigilance and proactive measures to ensure high vaccination rates and prevent outbreaks of vaccine-preventable diseases.</p>
    </td>
  </tr>
</table>



<div style="height:10px" aria-hidden="true" class="wp-block-spacer"></div>



<p class="has-text-align-center">Table 2. Generated response from static search (level 1) and dynamic search (level 3) to the same global question on the AP News dataset.&nbsp;</p>



<p>Table 2 shows an example output from static search at level 1 and dynamic search at level 3 to the same question. While the two outputs contain similar high-level topics, the response from dynamic search provided specific data such as the reduction of vaccination rates in certain demographics. We also notice that the response from dynamic search made significantly more references to the source material, indicated by “[Data Reports]” in the text. By selectively providing information that is relevant to the question, this alleviates the map-reduce operation from having to filter and process all the community reports all at once, and therefore it can generate a response that is more comprehensive and specific to the user question.&nbsp;</p>



<p>Overall, dynamic community selection proposes an alternative method to perform global search in GraphRAG by leveraging the indexed knowledge graph and the usage of cheaper LLM models in the rate relevancy operation. These changes led to lower total token cost and potential improvements to response detail and quality.&nbsp;</p>



<h2 class="wp-block-heading" id="availability">Availability</h2>



<div class="annotations " data-bi-aN="margin-callout">
	<ul class="annotations__list card depth-16 bg-body p-4 annotations__list--right">
		<li class="annotations__list-item">
						<span class="annotations__type d-block text-uppercase font-weight-semibold text-neutral-300 small">Blog</span>
			<a href="https://www.microsoft.com/en-us/research/blog/introducing-drift-search-combining-global-and-local-search-methods-to-improve-quality-and-efficiency/" target="_self" class="annotations__link font-weight-semibold text-decoration-none" data-bi-type="annotated-link" aria-label="Introducing DRIFT Search: Combining global and local search methods to improve quality and efficiency" data-bi-aN="margin-callout" data-bi-cN="Introducing DRIFT Search: Combining global and local search methods to improve quality and efficiency">
				Introducing DRIFT Search: Combining global and local search methods to improve quality and efficiency&nbsp;<span class="glyph-append glyph-append-chevron-right glyph-append-xsmall"></span>
			</a>
					</li>
	</ul>
</div>



<p><strong>You can experiment with dynamic global search on the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/graphrag">GraphRAG GitHub repository<span class="sr-only"> (opens in new tab)</span></a>.<span data-contrast="auto" xml:lang="EN-US" lang="EN-US" class="TextRun EmptyTextRun SCXW249363172 BCX8" style="-webkit-user-drag: none; -webkit-tap-highlight-color: transparent; margin: 0px; padding: 0px; user-select: text; font-size: 12pt; line-height: 20.7px; font-family: Aptos, Aptos_EmbeddedFont, Aptos_MSFontService, sans-serif; font-variant-ligatures: none !important;"></span> </strong></p>



<p>Dynamic global search is the second of several major optimizations to GraphRAG that are being explored. If you are interested in optimizations for local questions, please check out our recent blog post on <a href="https://www.microsoft.com/en-us/research/blog/introducing-drift-search-combining-global-and-local-search-methods-to-improve-quality-and-efficiency/" target="_blank" rel="noreferrer noopener">DRIFT search</a>. Stay tuned for our upcoming work, where we explore a radically different approach to graph-enabled RAG that is significantly more cost-efficient while improving answer quality for both local and global questions.&nbsp;</p>



<div style="height:30px" aria-hidden="true" class="wp-block-spacer"></div>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/graphrag-improving-global-search-via-dynamic-community-selection/">GraphRAG: Improving global search via dynamic community selection</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Orca-AgentInstruct: Agentic flows can be effective synthetic-data generators</title>
		<link>https://www.microsoft.com/en-us/research/blog/orca-agentinstruct-agentic-flows-can-be-effective-synthetic-data-generators/</link>
		
		<dc:creator><![CDATA[Brenda Potts]]></dc:creator>
		<pubDate>Thu, 14 Nov 2024 17:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1063530</guid>

					<description><![CDATA[<p>Orca-AgentInstruct, from Microsoft Research, can generate diverse, high-quality synthetic data at scale to post-train and fine-tune base LLMs for expanded capabilities, continual learning, and increased performance. </p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/orca-agentinstruct-agentic-flows-can-be-effective-synthetic-data-generators/">Orca-AgentInstruct: Agentic flows can be effective synthetic-data generators</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/07/Orca-3-2024-BlogHeroFeature-1400x788-1.jpg" alt="Orca-3 blog - abstract wave graphic" class="wp-image-1063551" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/07/Orca-3-2024-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/07/Orca-3-2024-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/07/Orca-3-2024-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/07/Orca-3-2024-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/07/Orca-3-2024-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/07/Orca-3-2024-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/07/Orca-3-2024-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/07/Orca-3-2024-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/07/Orca-3-2024-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/07/Orca-3-2024-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure>



<p>Our work on <a href="https://www.microsoft.com/en-us/research/publication/orca-progressive-learning-from-complex-explanation-traces-of-gpt-4/" target="_blank" rel="noreferrer noopener">Orca</a> and <a href="https://www.microsoft.com/en-us/research/blog/orca-2-teaching-small-language-models-how-to-reason/" target="_blank" rel="noreferrer noopener">Orca 2</a> demonstrated the power of using synthetic data for the post-training of small language models and getting them to levels of performance previously found only in much larger language models. Orca-AgentInstruct is another step in this direction, where we explore using agentic flows to generate diverse and high-quality data at scale. Orca-AgentInstruct is an agentic solution for synthetic-data generation. By leveraging an agentic framework, AgentInstruct can generate tailored datasets, comprising both prompts and responses, from raw data sources, paving the way to building a synthetic data factory for model fine-tuning.&nbsp;&nbsp;</p>



<p>The efficacy of this approach is exemplified by the substantial improvement observed by fine-tuning a base Mistral 7-billion-parameter model and using AgentInstruct to generate a 25-million-pair dataset. The fine-tuned model (which we refer to as Orca-3-Mistral) showcases a notable performance gain across multiple benchmarks. For example, it shows 40% improvement on AGIEval, 19% improvement on MMLU, 54% improvement on GSM8K, 38% improvement on BBH, 45% improvement on AlpacaEval, and a 31.34% reduction of inaccurate or unreliable results across multiple summarization benchmarks.</p>



<p>We are making a 1-million-pair subset (<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://huggingface.co/datasets/microsoft/orca-agentinstruct-1M-v1" target="_blank" rel="noreferrer noopener">orca-agentinstruct-1M</a>) of this dataset publicly available, along with a <a href="https://www.microsoft.com/en-us/research/publication/agentinstruct-toward-generative-teaching-with-agentic-flows/" target="_blank" rel="noreferrer noopener">report</a> describing the data generation procedure, to encourage research on synthetic data generation and finetuning of language models.&nbsp;</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="3300" height="1320" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/08/Figure-1-1.png" alt="Bar graph comparing scores of the Mistral-Instruct-7B model and the Mistral-7B post-trained AgentInstruct data (Orca-3). The benchmarks are AGIEval, MMLU, BBH, GSM8K, AlpaceEval, FOFO and Mirage-RAG. The graph shows substantial improvement across different benchmarks for the model fine-tuned with AgentInstruct data." class="wp-image-1063923" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/08/Figure-1-1.png 3300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/Figure-1-1-300x120.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/Figure-1-1-1024x410.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/Figure-1-1-768x307.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/Figure-1-1-1536x614.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/Figure-1-1-2048x819.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/Figure-1-1-240x96.png 240w" sizes="(max-width: 3300px) 100vw, 3300px" /><figcaption class="wp-element-caption">Figure 1: Effect of using AgentInstruct for post-training Mistral-7B.&nbsp;</figcaption></figure>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1661" height="498" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/07/Orca-3_Figure-2.png" alt="The figure shows the three flows used in AgentInstruct: 1) Content Transformation Flow converts the raw seed into an intermediate representation that simplifies the creation of instructions tailored to specific objectives. 2) Seed Instruction Generation Flow, comprising multiple agents, takes as input the transformed seed from the Content Transformation Flow and generates a set of diverse instructions. 3) Instruction Refinement Flow takes as input the instructions from the Seed Instruction Flow and iteratively enhances their complexity and quality." class="wp-image-1063545" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/07/Orca-3_Figure-2.png 1661w, https://www.microsoft.com/en-us/research/uploads/prod/2024/07/Orca-3_Figure-2-300x90.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/07/Orca-3_Figure-2-1024x307.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/07/Orca-3_Figure-2-768x230.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/07/Orca-3_Figure-2-1536x461.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2024/07/Orca-3_Figure-2-240x72.png 240w" sizes="(max-width: 1661px) 100vw, 1661px" /><figcaption class="wp-element-caption">Figure 2. This figure provides a thematic overview of the roles played by different groups of agents. Content Transformation Flow converts the seed into an intermediate representation that makes it easier to create high-quality and diverse data. Seed Instruction Generation Flow creates instances of the target tasks following a taxonomy. Instruction Refinement Flow explores the space further by starting from these initial data points and exploring the neighborhood. The expectation is that by picking a random seed we will be able to cover the entire region of data points.&nbsp;</figcaption></figure>



<p><strong>Synthetic Data Accelerated LLM Development:</strong> Over the past year, using synthetic data has greatly advanced the training of large language models (LLMs). It sped up model training at all stages, from pre-training (e.g., Phi-3) to instruction-tuning (e.g., Orca and WizardLM) and reinforcement learning from human feedback (e.g., Direct Nash Optimization).<span data-contrast="auto" xml:lang="EN-US" lang="EN-US" class="TextRun EmptyTextRun SCXW48469235 BCX8" style="-webkit-user-drag: none; -webkit-tap-highlight-color: transparent; margin: 0px; padding: 0px; user-select: text; font-size: 11pt; line-height: 18.3458px; font-family: Aptos, Aptos_EmbeddedFont, Aptos_MSFontService, sans-serif; font-weight: bold; font-variant-ligatures: none !important;"></span>&nbsp;</p>



<div class="annotations " data-bi-aN="margin-callout">
	<ul class="annotations__list card depth-16 bg-body p-4 annotations__list--left">
		<li class="annotations__list-item">
							<a href="https://www.microsoft.com/en-us/research/video/agentinstruct-methodology/" target="_blank" aria-label="AgentInstruct Methodology" data-bi-type="annotated-link" data-bi-cN="AgentInstruct Methodology" class="annotations__list-thumbnail" >
					<img loading="lazy" decoding="async" width="172" height="96" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/07/j-hYRmUUbLU-240x135.jpg" class="mb-2" alt="AgentInstruct video thumbnail" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/07/j-hYRmUUbLU-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/07/j-hYRmUUbLU-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/07/j-hYRmUUbLU-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/07/j-hYRmUUbLU-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/07/j-hYRmUUbLU-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/07/j-hYRmUUbLU-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/07/j-hYRmUUbLU-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/07/j-hYRmUUbLU-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/07/j-hYRmUUbLU.jpg 1280w" sizes="(max-width: 172px) 100vw, 172px" />				</a>
							<span class="annotations__type d-block text-uppercase font-weight-semibold text-neutral-300 small">Demo video</span>
			<a href="https://www.microsoft.com/en-us/research/video/agentinstruct-methodology/" target="_blank" class="annotations__link font-weight-semibold text-decoration-none" data-bi-type="annotated-link" aria-label="AgentInstruct Methodology" data-bi-aN="margin-callout" data-bi-cN="AgentInstruct Methodology">
				AgentInstruct Methodology&nbsp;<span class="glyph-append glyph-append-chevron-right glyph-append-xsmall"></span>
			</a>
					</li>
	</ul>
</div>



<p><strong>Generating high-quality synthetic data is hard:</strong> On the other hand, research indicates that pre-training models on synthetic data produced by other models can result in model collapse, causing models to progressively degrade. Similar concerns have been raised regarding the use of synthetic data for post-training, suggesting that it might lead to an imitation process where the trained model learns only stylistic features rather than actual capabilities.<span data-contrast="auto" xml:lang="EN-US" lang="EN-US" class="TextRun EmptyTextRun SCXW155638626 BCX8" style="-webkit-user-drag: none; -webkit-tap-highlight-color: transparent; margin: 0px; padding: 0px; user-select: text; font-size: 11pt; line-height: 18.3458px; font-family: Aptos, Aptos_EmbeddedFont, Aptos_MSFontService, sans-serif; font-variant-ligatures: none !important;"></span>&nbsp;</p>



<p>This discrepancy may be attributed to the challenge of generating high-quality and diverse synthetic data.&nbsp; Successful use of synthetic data involves significant human effort in curating and filtering the data to ensure high quality.&nbsp;</p>



<p><strong>Synthetic data meets agents:</strong> Another major development we witnessed during the past year is the rise of agentic (especially multi-agent) workflows, such as with AutoGen. Agentic workflows can generate high-quality data, which surpasses the capabilities of the underlying LLMs, by using flows with reflection and iteration that enable agents to look back at solutions, generate critiques, and improve solutions. They can also use tools like search APIs, calculators, and code interpreters to address LLM limitations.&nbsp;</p>



<p>Multi-agent workflows bring in additional benefits as well, such as simulating scenarios where we can generate both new prompts and the corresponding responses. They also enable automation of data-generation workflows, reducing or eliminating the need for unnecessary human intervention on some tasks.&nbsp;</p>



<p><strong>AgentInstruct:</strong> Generating synthetic data for post-training or finetuning often relies on an existing prompt set that is either used as is or as seeds for generating more instructions. In this work, we generalize the problem settings to a broader objective of generating an abundant amount of diverse, challenging, and high-quality data to teach a particular skill to an AI model. We refer to this setting as <em>generative teaching</em>.&nbsp;&nbsp;&nbsp;</p>



<p>AgentInstruct is an agentic solution for generative teaching. AgentInstruct uses raw documents as input to create demonstration and feedback data. When generic data is used as seeds, AgentInstruct can be used to teach an LLM a general capability, such as writing,&nbsp;reasoning, or retrieval-augmented generation (RAG). Domain specific data, like retail&nbsp;or finance, can also be used as seeds to improve the model in a certain specialization. AgentInstruct can create:&nbsp;</p>



<ol class="wp-block-list">
<li><strong>High-quality data:</strong> AgentInstruct uses GPT-4, coupled with tools like search and code interpreters, to create high-quality data.&nbsp;&nbsp;</li>



<li><strong>Diverse data:</strong> AgentInstruct creates prompts and responses using a set of specialized agents (with powerful LLMs, tools, and reflection flows) and a taxonomy (of more than 100 subcategories), , ensuring diversity and quality.</li>



<li><strong>Large quantities of data:</strong> AgentInstruct can run autonomously. and applyiflows for verification and data filtering. It does not require seed prompts and uses raw documents for seeding.&nbsp;</li>
</ol>



<p>Using raw data as seeds offers two advantages: it is plentiful, allowing AgentInstruct to generate large-scale and diverse datasets, and it encourages learning general skills instead of benchmark-specific ones by avoiding using existing prompts.</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1085523">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">Microsoft research podcast</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/podcast/abstracts-august-15-2024/" aria-label="Abstracts: August 15, 2024" data-bi-cN="Abstracts: August 15, 2024" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/08/Episode-15_Abstracts_Hero_Feature_No_Text_1400x788.jpg" alt="Stylized microphone and sound waves illustration." />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">Abstracts: August 15, 2024</h2>
				
								<p class="large">Advanced AI may make it easier for bad actors to deceive others online. A multidisciplinary research team is exploring one solution: a credential that allows people to show they’re not bots without sharing identifying information. Shrey Jain and Zoë Hitzig explain.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://www.microsoft.com/en-us/research/podcast/abstracts-august-15-2024/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Listen now" data-bi-cN="Abstracts: August 15, 2024" target="_blank">
							Listen now						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<p>We anticipate agentic flows becoming increasingly important throughout the model-training lifecycle, including pre-training, post-training, and specialization, and ultimately enabling the creation of a synthetic data factory for model customization and continuous improvement. This has the potential to drive AI advances across multiple industries by making high-quality model training more efficient and accessible.&nbsp;</p>



<h2 class="wp-block-heading" id="contributors">Contributors:</h2>



<p><a href="https://www.microsoft.com/en-us/research/people/armitra/" target="_blank" rel="noreferrer noopener">Arindam Mitra</a>, Luciano Del Corro, <a href="https://www.microsoft.com/en-us/research/people/zheng/" target="_blank" rel="noreferrer noopener">Guoqing Zheng</a>, <a href="https://www.microsoft.com/en-us/research/people/shmahaj/" target="_blank" rel="noreferrer noopener">Shweti Mahajan</a>, <a href="https://www.microsoft.com/en-us/research/people/danyr/" target="_blank" rel="noreferrer noopener">Dany Rouhana</a>, <a href="https://www.microsoft.com/en-us/research/people/andrescodas/" target="_blank" rel="noreferrer noopener">Andres Codas</a>, Yadong Lu, Wei-ge Chen, <a href="https://www.microsoft.com/en-us/research/people/olvrousg/" target="_blank" rel="noreferrer noopener">Olga Vrousgou</a>, <a href="https://www.microsoft.com/en-us/research/people/corbyrosset/" target="_blank" rel="noreferrer noopener">Corby Rosset</a>, Fillipe Silva, <a href="https://www.microsoft.com/en-us/research/people/hakhanpo/" target="_blank" rel="noreferrer noopener">Hamed Khanpour</a>, <a href="https://www.microsoft.com/en-us/research/people/yashlara/" target="_blank" rel="noreferrer noopener">Yash Lara</a>, and <a href="https://www.microsoft.com/en-us/research/people/hassanam/" target="_blank" rel="noreferrer noopener">Ahmed Awadallah</a></p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/orca-agentinstruct-agentic-flows-can-be-effective-synthetic-data-generators/">Orca-AgentInstruct: Agentic flows can be effective synthetic-data generators</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Abstracts: November 14, 2024</title>
		<link>https://www.microsoft.com/en-us/research/podcast/abstracts-november-14-2024/</link>
		
		<dc:creator><![CDATA[Alyssa Hughes (2ADAPTIVE LLC dba 2A Consulting)]]></dc:creator>
		<pubDate>Thu, 14 Nov 2024 15:00:00 +0000</pubDate>
				<category><![CDATA[Microsoft Research Podcast]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1101918</guid>

					<description><![CDATA[<p>The efficient simulation of molecules has the potential to change how the world understands biological systems and designs new drugs and biomaterials. Tong Wang discusses AI2BMD, an AI-based system designed to simulate large biomolecules with speed and accuracy.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/podcast/abstracts-november-14-2024/">Abstracts: November 14, 2024</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Tong-and-Bonnie_Abstracts_Hero_Feature_No_Text_1400x788.jpg" alt="Outlined illustrations of Tong Wang and Bonnie Kruft for the Microsoft Research Podcast, Abstracts series." class="wp-image-1102266" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Tong-and-Bonnie_Abstracts_Hero_Feature_No_Text_1400x788.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Tong-and-Bonnie_Abstracts_Hero_Feature_No_Text_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Tong-and-Bonnie_Abstracts_Hero_Feature_No_Text_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Tong-and-Bonnie_Abstracts_Hero_Feature_No_Text_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Tong-and-Bonnie_Abstracts_Hero_Feature_No_Text_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Tong-and-Bonnie_Abstracts_Hero_Feature_No_Text_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Tong-and-Bonnie_Abstracts_Hero_Feature_No_Text_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Tong-and-Bonnie_Abstracts_Hero_Feature_No_Text_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Tong-and-Bonnie_Abstracts_Hero_Feature_No_Text_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Tong-and-Bonnie_Abstracts_Hero_Feature_No_Text_1400x788-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure>


<div class="wp-block-msr-podcast-container my-4">
	<iframe loading="lazy" src="https://player.blubrry.com/?podcast_id=138404501&modern=1" class="podcast-player" frameborder="0" height="164px" width="100%" scrolling="no" title="Podcast Player"></iframe>
</div>



<p>Members of the research community at Microsoft work continuously to advance their respective fields. <em>Abstracts</em> brings its audience to the cutting edge with them through short, compelling conversations about new and noteworthy achievements.</p>



<p>In this episode, Microsoft Senior Researcher <a href="https://www.microsoft.com/en-us/research/people/watong/">Tong Wang</a> joins guest host <a href="https://www.microsoft.com/en-us/research/people/bonniekruft/?msockid=35739e94ab6c69d41b738b93aa076831">Bonnie Kruft</a>, partner and deputy director of Microsoft Research AI for Science, to discuss <a href="https://www.microsoft.com/en-us/research/publication/ab-initio-characterization-of-protein-molecular-dynamics-with-ai2bmd/">“Ab initio characterization of protein molecular dynamics with AI<sup>2</sup>BMD.”</a> In the paper, which was published by the scientific journal <em>Nature</em>, Wang and his coauthors detail a system that leverages AI to advance the state of the art in simulating the behavior of large biomolecules. AI<sup>2</sup>BMD, which is generalizable across a wide range of proteins, has the potential to advance solutions to scientific problems and enhance biomedical research in drug discovery, protein design, and enzyme engineering.</p>



<div class="wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex">
<div class="wp-block-button"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/ab-initio-characterization-of-protein-molecular-dynamics-with-ai2bmd/">Read the paper</a></div>



<div class="wp-block-button is-style-fill-github"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://github.com/microsoft/AI2BMD" target="_blank" rel="noreferrer noopener">Get the code</a></div>
</div>


<div class="wp-block-msr-show-more">
	<div class="bg-neutral-100 p-5">
		<div class="show-more-show-less" data-mount="show-more-show-less">
			<div>
				<span>
					

<h2 class="wp-block-heading" id="transcript-1">Transcript</h2>



<p>[MUSIC]</p>



<p><strong>BONNIE KRUFT: </strong>Welcome to <em>Abstracts</em>, a Microsoft Research Podcast that puts the spotlight on world-class research in brief. In this series, members of the research community at Microsoft give us a quick snapshot—or a <em>podcast abstract</em>—of their new and noteworthy papers.</p>



<p>[MUSIC FADES]&nbsp;</p>



<p>I’m Bonnie Kruft, partner and deputy director of Microsoft Research AI for Science and your host for today. Joining me is Tong Wang, a senior researcher at Microsoft. Tong is the lead author of a paper called “Ab initio characterization of protein molecular dynamics with AI<sup>2</sup>BMD,” which has just been published by the top scientific journal <em>Nature</em>. Tong, thanks so much for joining us today on <em>Abstracts</em>!</p>



				</span>
				<span id="show-more-show-less-toggle-2" class="show-more-show-less-toggleable-content">
					



<p><strong>TONG WANG: </strong>Thank you, Bonnie.</p>



<p><strong>KRUFT: </strong>Microsoft Research is one of the earliest institutions to apply AI in biomolecular simulation research. Why did the AI for Science team choose this direction, and—with this work specifically, AI<sup>2</sup>BMD—what problem are you and your coauthors addressing, and why should people know about it?</p>



<p><strong>WANG: </strong>So as Richard Feynman famously said, “Everything that living things do can be understood in terms of the <em>jigglings</em> and the <em>wigglings</em> of atoms.” To study the mechanisms behind the biological processes and to develop biomaterials and drugs requires a computational approach that can accurately characterize the dynamic motions of biomolecules. When we review the computational research for biomolecular structure, we can get two key messages. First, in recent years, predicting the crystal, or <em>static</em>, protein structures with methods powered by AI has achieved great success and just won the Nobel Prize in Chemistry in the last month. However, characterizing the dynamic structures of proteins is more meaningful for biology, drug, and medicine fields but is much more challenging. Second, molecular dynamics simulation, or <em>MD</em>, is one of the most widely used approaches to study protein dynamics, which can be roughly divided into classical molecular dynamics simulation and quantum molecular dynamics simulation. Both approaches have been developed for more than a half century and won Nobel Prize. Classical MD is fast but less accurate, while quantum MD is very accurate but computationally prohibitive for the protein study. However, we need both the accuracy and the efficiency to detect the biomechanisms. Thus, applying AI in biomolecular simulation can become the third way to achieve both ab initio—or <em>first principles</em>—accuracy and high efficiency. In the winter of 2020, we have foreseen the trend that AI can make a difference in biomolecular simulations. Thus, we chose this direction.</p>



<p><strong>KRUFT:</strong> It took four years from the idea to the launch of AI<sup>2</sup>BMD, and there were many important milestones along the way. First, talk about how your work builds on and/or differs from what’s been done previously in this field, and then give our audience a sense of the key moments and challenges along the AI<sup>2</sup>BMD research journey.</p>



<p><strong>WANG: </strong>First, I’d like to say applying AI in biomolecular simulation is a novel research field. For AI-powered MD simulation for large biomolecules, there is no existing dataset, no well-designed machine learning model for the interactions between the atoms and the molecules, no clear technical roadmap, no mature AI-based simulation system. So we face various new challenges every day. Second, there are some other works exploring this area at the same time. I think a significant difference between AI<sup>2</sup>BMD and other works is that other works require to generate new data and train the deep learning models for any new proteins. So it takes a protein-specific solution. As a contrast, AI<sup>2</sup>BMD proposes a generalizable solution for a wide range of proteins. To achieve it, as you mentioned, there are some key milestones during the four-year journey. The first one is we proposed the generalizable protein fragmentation approach that divides proteins into the commonly used 20 kinds of dipeptides. Thus, we don’t need to generate data for various proteins. Instead, we only need to sample the conformational space of such dipeptides. So we built the protein unit dataset that contains about 20 million samples with ab initio accuracy. Then we proposed ViSNet, the graph neural network for molecular geometry modeling as the machine learning potential for AI<sup>2</sup>BMD. Furthermore, we designed AI<sup>2</sup>BMD simulation system by efficiently leveraging CPUs and GPUs at the same time, achieving hundreds of times simulation speed acceleration than one year before and accelerating the AI-driven simulation with only ten to a hundred millisecond per simulation step. Finally, we examined AI<sup>2</sup>BMD on energy, force, free energy, J coupling, and many kinds of property calculations for tens of proteins and also applied AI<sup>2</sup>BMD in the drug development competition. All things are done by the great team with science and engineering expertise and the great leadership and support from AI for Science lab.</p>



<p><strong>KRUFT: </strong>Tell us about how you conducted this research. What was your methodology?</p>



<p><strong>WANG: </strong>As exploring an interdisciplinary research topic, our team consists of experts and students with biology, chemistry, physics, math, computer science, and engineering backgrounds. The teamwork with different expertise is key to AI<sup>2</sup>BMD research. Furthermore, we collaborated and consulted with many senior experts in the molecular dynamics simulation field, and they provided very insightful and constructive suggestions to our research. Another aspect of the methodology I’d like to emphasize is learning from negative results. Negative results happened most of the time during the study. What we do is to constantly analyze the negative results and adjust our algorithm and model accordingly. There’s no perfect solution for a research topic, and we are always on the way.</p>



<p><strong>KRUFT: </strong>AI<sup>2</sup>BMD got some upgrades this year, and as we mentioned at the top of the episode, the work around the latest system was published in the scientific journal <em>Nature</em>. So tell us, Tong—what is new about the latest AI<sup>2</sup>BMD system?&nbsp;</p>



<p><strong>WANG: </strong>Good question. We posted a preliminary version of AI<sup>2</sup>BMD manuscript on bioRxiv last summer. I’d like to share three important upgrades through the past one and a half year. The first is hundreds of times of simulation speed acceleration for AI<sup>2</sup>BMD, which becomes one of the fastest AI-driven MD simulation system and leads to perform much longer simulations than before. The second aspect is AI<sup>2</sup>BMD was applied for many protein property calculations, such as enthalpy, heat capacity, folding free energy, p<em>K</em><sub>a</sub>, and so on. Furthermore, we have been closely collaborating with the Global Health Drug Discovery Institute, GHDDI, a nonprofit research institute founded and supported by the Gates Foundation, to leverage AI<sup>2</sup>BMD and other AI capabilities to accelerate the drug discovery processes.</p>



<p><strong>KRUFT: </strong>What significance does AI<sup>2</sup>BMD hold for research in both biology and AI? And also, what impact does it have outside of the lab, in terms of societal and individual benefits?</p>



<p><strong>WANG: </strong>Good question. For biology, AI<sup>2</sup>BMD provides a much more accurate approach than those used in the past several decades to simulate the protein dynamic motions and study the bioactivity. For AI, AI<sup>2</sup>BMD proves AI can make a big difference to the dynamic protein structure study beyond AI for the protein static structure prediction. Raised by AI<sup>2</sup>BMD and other works, I can foresee there is a coming age of AI-driven biomolecular simulation, providing binding free-energy calculation with quantum simulation accuracy for the complex of drug and the target protein for drug discovery, detecting more flexible biomolecular conformational changes that molecular mechanics cannot do, and opening more opportunities for enzyme engineering and vaccine and antibody design.</p>



<p><strong>KRUFT: </strong>AI is having a profound influence on the speed and breadth of scientific discovery, and we’re excited to see more and more talented people joining us in this space. What do you want our audience to take away from this work, particularly those already working in the AI for Science space or looking to enter it?</p>



<p><strong>WANG: </strong>Good question. I’d like to share three points from my research experience. First is aim high. Exploring a disruptive research topic is better than doing 10 incremental works. In the years of research, our organization always encourages us to do the big things. Second is persistence. I remembered a computer scientist previously said about 90% of the time during research is failure and frustration. The rate is even higher when exploring a new research direction. In AI<sup>2</sup>BMD study, when we suffered from research bottlenecks that cannot be tackled for several months, when we received critical comments from reviewers, when some team members wanted to give up and leave, I always encourage everyone to persist, and we will make it. More importantly, the foundation of persistence is to ensure your research direction is meaningful and constantly adjust your methodology from failures and critical feedback. The third one is real-world applications. Our aim is to leverage AI for advancing science. Proposing scientific problems is a first step, then developing AI tools and evaluating on benchmarks and, more importantly, examining its usefulness in the real-world applications and further developing your AI algorithms. In this way, you can close the loop of AI for Science research.</p>



<p><strong>KRUFT: </strong>And, finally, Tong, what unanswered questions or unsolved problems remain in this area, and what’s next on the agenda for the AI<sup>2</sup>BMD team?</p>



<p><strong>WANG: </strong>Well, I think AI<sup>2</sup>BMD is a starting point for the coming age of AI-driven MD for biomolecules. There are lots of new scientific questions and challenges coming out in this new field. For example, how to expand the simulated molecules from proteins to other kinds of biomolecules; how to describe the biochemical reactions during the simulations; how to further improve the simulation efficiency and robustness; and how to apply it for more real-world scenarios. We warmly welcome any people from both academic and industrial fields to work together with us to make the joint efforts to push the frontier of this new field moving forward.</p>



<p>[MUSIC]</p>



<p><strong>KRUFT: </strong>Well, Tong, thank you for joining us today, and to our listeners, thanks for tuning in. If you want to read the full paper on AI<sup>2</sup>BMD, you can find a link at aka.ms/abstracts, or you can read it on the <em>Nature</em> website. See you next time on <em>Abstracts</em>!</p>



<p>[MUSIC FADES]</p>

				</span>
			</div>
			<button
				class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle"
				aria-expanded="false"
				data-show-less-text="Show less"
				type="button"
				aria-controls="show-more-show-less-toggle-2"
				aria-label="Show more content"
				data-alternate-aria-label="Show less content">
				Show more			</button>
		</div>
	</div>
</div>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/podcast/abstracts-november-14-2024/">Abstracts: November 14, 2024</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Toward modular models: Collaborative AI development enables model accountability and continuous learning</title>
		<link>https://www.microsoft.com/en-us/research/blog/toward-modular-models-collaborative-ai-development-enables-model-accountability-and-continuous-learning/</link>
		
		<dc:creator><![CDATA[Brenda Potts]]></dc:creator>
		<pubDate>Wed, 13 Nov 2024 17:30:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1098456</guid>

					<description><![CDATA[<p>Modular models can democratize AI development while unlocking new benefits and use cases. Modularized AI can be more flexible, more compliant, and cheaper to develop—requiring less data and fewer compute resources to train expert models.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/toward-modular-models-collaborative-ai-development-enables-model-accountability-and-continuous-learning/">Toward modular models: Collaborative AI development enables model accountability and continuous learning</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-large"><img loading="lazy" decoding="async" width="1024" height="576" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/10/Modular-Models-BlogHeroFeature-1400x788-1-1024x576.jpg" alt="Modular Models blog hero" class="wp-image-1098480" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/10/Modular-Models-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/10/Modular-Models-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/10/Modular-Models-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/10/Modular-Models-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/10/Modular-Models-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/10/Modular-Models-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/10/Modular-Models-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/10/Modular-Models-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/10/Modular-Models-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2024/10/Modular-Models-BlogHeroFeature-1400x788-1.jpg 1400w" sizes="(max-width: 1024px) 100vw, 1024px" /></figure>



<p>Today, development of generalizable AI models requires access to sufficient data and compute resources, which may create challenges for some researchers. Democratizing access to technology across the research community can advance the development of generalizable AI models. By applying the core software development concept of modularity to AI, we can build models that are powerful, efficient, adaptable, and transparent.&nbsp;</p>



<p>Until recently, AI models were primarily built using monolithic architecture. Though powerful, these models can be challenging to customize and edit compared to modular models with easily interpretable functional components. Today, developers employ modularity to make services more reliable, faster to refine, and easier for multiple users to contribute to simultaneously. One promising research direction that supports this involves shifting AI development towards a <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://colinraffel.com/blog/a-call-to-build-models-like-we-build-open-source-software.html" target="_blank" rel="noreferrer noopener">modular approach<span class="sr-only"> (opens in new tab)</span></a>, which could enhance flexibility and improve scalability.&nbsp;</p>



<p>One such approach is to use numerous fine-tuned models designed for specific tasks, known as <em>expert models</em>, and coordinate them to solve broader tasks (see&nbsp;<a href="https://www.microsoft.com/en-us/research/publication/towards-modular-llms-by-building-and-reusing-a-library-of-loras/">Towards Modular LLMs by Building and Reusing a Library of LoRAs &#8211; Microsoft Research<span class="sr-only"> (opens in new tab)</span></a>,&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/abs/2402.05859" target="_blank" rel="noreferrer noopener">Learning to Route Among Specialized Experts for Zero-Shot Generalization<span class="sr-only"> (opens in new tab)</span></a>). These expert models can be developed in a decentralized way. Similar to the benefits of using a microservice architecture, this modular AI approach can be more flexible, cheaper to develop, and more compliant with relevant privacy and legal policies. However, while substantial research has been done on training optimization, coordination methods remain largely unexplored.</p>



<p>Our team is exploring the potential of modular models by focusing on two themes: i) optimizing the training of expert models and ii) refining how expert models coordinate to form a collaborative model. One method for coordinating expert models is to adaptively select the most relevant independently developed expert models for specific tasks or queries. This approach, called <em>MoErging</em>, is similar to Mixture-of-Experts (MoE) approaches but differs in that the routing mechanism is learned <em>after</em> the individual experts are trained. As an initial step, we contributed to creating a taxonomy for organizing recent MoErging methods with the goal of helping establish a shared language for the research community and facilitating easier and fairer comparisons between different methods.&nbsp;</p>



<h2 class="wp-block-heading" id="assessing-existing-moerging-methods">Assessing existing MoErging methods</h2>



<p>Most MoErging methods were developed within the past year, so they don’t reference each and are difficult to compare. To enable comparison of MoErging methods, we recently collaborated on a <a href="https://www.microsoft.com/en-us/research/publication/a-survey-on-model-moerging-recycling-and-routing-among-specialized-experts-for-collaborative-learning/" target="_blank" rel="noreferrer noopener">survey</a> that establishes a taxonomy for comparing methods and organizes MoErging design choices into three steps:&nbsp;</p>



<ul class="wp-block-list">
<li><strong>Expert design:</strong> Identifies and uses expert models trained asynchronously by distributed contributors.&nbsp;</li>



<li><strong>Routing design:</strong> Routes tasks to the appropriate expert models.&nbsp;</li>



<li><strong>Application design:</strong> Applies the merged models to specific tasks or domains.&nbsp;</li>
</ul>



<p>Each step is broken down into more detailed choices. For example, in <em>expert design</em>, expert training can be <em>custom</em> or <em>standard</em>, and training data can be <em>private</em> or <em>shared</em>. Custom training requires MoErging to have specific training procedures, while the standard training does not. Similarly, <em>shared data</em> means that the training data must be accessible for routing. Otherwise, the training data is considered private.&nbsp;</p>



<p>The benefits of modular models discussed below assume that training data doesn’t need to be shared. However, a review of current MoErging methods finds that some approaches do require sharing training data, making certain benefits no longer applicable.&nbsp;</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1085502">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">Spotlight: Blog post</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-september-9-2024/" aria-label="Research Focus: Week of September 9, 2024" data-bi-cN="Research Focus: Week of September 9, 2024" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RF49-BlogHeroFeature-1400x788-1.jpg" alt="Research Focus | September 9, 2024" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">Research Focus: Week of September 9, 2024</h2>
				
								<p class="large">Investigating vulnerabilities in LLMs; A novel total-duration-aware (TDA) duration model for text-to-speech (TTS); Generative expert metric system through iterative prompt priming; Integrity protection in 5G fronthaul networks.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-september-9-2024/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Read more" data-bi-cN="Research Focus: Week of September 9, 2024" target="_blank">
							Read more						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<p>The survey evaluates 29 different MoErging methods using its taxonomy, which categorizes the design choices into two expert design choices, five routing design choices, and two application design options, shown in Figure 1.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1017" height="1056" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/10/Modular-Models_Figure1.png" alt="Taxonomy of model MoErging design choices. References in the leaf noes link to sections for specific papers that make some particular design choice. We omit references to methods for which a given choice is not applicable." class="wp-image-1098477" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/10/Modular-Models_Figure1.png 1017w, https://www.microsoft.com/en-us/research/uploads/prod/2024/10/Modular-Models_Figure1-289x300.png 289w, https://www.microsoft.com/en-us/research/uploads/prod/2024/10/Modular-Models_Figure1-986x1024.png 986w, https://www.microsoft.com/en-us/research/uploads/prod/2024/10/Modular-Models_Figure1-768x797.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/10/Modular-Models_Figure1-173x180.png 173w" sizes="(max-width: 1017px) 100vw, 1017px" /><figcaption class="wp-element-caption">Figure 1: Taxonomy of model MoErging design choices. References in the leaf nodes link to sections of specific papers that implement each choice. We omit references to methods where a particular choice is not applicable.&nbsp;</figcaption></figure>



<p>One takeaway from the survey is that most MoErging methods can be grouped into four categories based on their routing design choices:</p>



<ol class="wp-block-list">
<li><strong>Classifier-based routing:</strong> Methods that train the router as a classifier using expert datasets or unseen data.&nbsp;</li>



<li><strong>Embedding-based routing:</strong> Methods that compute embeddings of expert training sets and compare them to a query embedding for routing.&nbsp;</li>



<li><strong>Nonrouter methods:</strong> Methods that do not explicitly train a router but instead initialize the router in an unsupervised manner.&nbsp;&nbsp;</li>



<li><strong>Task-specific routing:</strong> Methods that learn a task-specific routing distribution over the target dataset to improve performance on a specific task.&nbsp;</li>
</ol>



<p>While the differences within each category are minor, the differences across categories are significant because they determine the level of data access required for implementation. As a result, data access is a primary factor in determining which methods are applicable and feasible in various settings.&nbsp;</p>



<p>Our taxonomy also covers recent approaches to building agentic systems, which could be viewed as specific types of MoErging methods where experts are full language models and routing decisions are made on a step-by-step or example-by-example basis. The optimal level for MoErging may vary depending on the task and the computational resources available to each stakeholder.&nbsp;</p>



<h2 class="wp-block-heading" id="potential-benefits-and-use-cases-of-modular-models">Potential benefits and use cases of modular models&nbsp;</h2>



<p>Modular models can unlock new benefits and use cases for AI, offering a promising approach to addressing challenges in current AI development. Moving forward, further substantial research is needed to validate this potential and assess feasibility.&nbsp;&nbsp;</p>



<p>Modular AI may:&nbsp;</p>



<ul class="wp-block-list">
<li><strong>Allow privacy-conscious contributions.</strong> &nbsp;Teams with sensitive or proprietary data, such as personally identifiable information (PII) and copyrighted content, can contribute expert models and benefit from larger projects without sharing their data. This capacity can make it easier to comply with data privacy and legal standards, which could be valuable for healthcare teams that would benefit from general model capabilities without combining their sensitive data with other training data.&nbsp;</li>



<li><strong>Drive model transparency and accountability.</strong> &nbsp;Modular models allow specific expert models to be identified and, if necessary, removed or retrained. For example, if a module trained on PII, copyrighted, or biased data is identified, it can be removed more easily, eliminating the need for retraining and helping ensure compliance with privacy and ethical standards.&nbsp;</li>



<li><strong>Facilitate model extensibility and continual improvement.</strong>&nbsp;Modularity supports continual improvements, allowing new capabilities from expert models to be integrated as they are available. This approach is akin to making localized edits, allowing for continuous, cost-effective improvement.&nbsp;</li>



<li><strong>Lower the barrier to AI development for those with limited compute and data resources.</strong>&nbsp;Modular AI can reduce the need for extensive data and compute by creating a system where pretrained experts can be reused, benefiting academics, startups, and teams focused on niche use cases. For example, an AI agent tasked with booking flights on a specific website with limited training data could leverage general navigation and booking skills from other trained AI experts, enabling generalizable and broadly applicable skills without requiring domain-specific training data. We explore this process of transferring skills across tasks in our paper “<a href="https://www.microsoft.com/en-us/research/publication/multi-head-adapter-routing-for-cross-task-generalization/%22%20%EF%B7%9FHYPERLINK%20%22https://www.microsoft.com/en-us/research/publication/multi-head-adapter-routing-for-cross-task-generalization/">Multi-Head Routing For Cross-Task Generalization</a>.”&nbsp;</li>



<li><strong>Support personalization.</strong> &nbsp;Modular models make it possible to equip AI agents with experts tailored to individual users or systems. For instance, AI designed to emulate five-time World Chess Champion Magnus Carlsen could enhance a player’s preparation to play a match against him. Experiments suggest that storing knowledge or user profiles in on-demand modules can match or surpass the performance of retrieval-augmented generation (RAG), potentially reducing latency and improving the user’s experience in custom AI applications.&nbsp;</li>
</ul>



<h2 class="wp-block-heading" id="current-limitations-and-looking-forward">Current limitations and looking forward&nbsp;</h2>



<p>In this blog, we focused on a type of modular approach that involves training foundation models, which requires substantial compute power and large amounts of data. Despite the advantages of modularity, such as increased flexibility, efficiency, and adaptability, the development of foundation models remains resource-intensive, necessitating high-performance computing and robust datasets to support fine-tuning.&nbsp;&nbsp;</p>



<p>Recent work has begun to address these challenges by <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/abs/2311.08105" target="_blank" rel="noreferrer noopener">distributing the pretraining process of foundation models<span class="sr-only"> (opens in new tab)</span></a>. Looking ahead, a promising research direction focuses on exploring how to create a minimal dataset for training “empty foundation models” while shifting most of their capabilities to external pluggable modules.&nbsp;</p>



<p>Modular methods are evolving rapidly, and we’re excited by their potential. Modularity has the capacity to democratize AI development, improve model accountability, and support efficient continuous learning. With the MoErging taxonomy, we aim to establish a shared language that fosters engagement within the research community. This research is in the early stages, and we welcome community collaboration. If you&#8217;re interested in working with us, please reach out to <a href="mailto:ModularModels@microsoft.com" target="_blank" rel="noreferrer noopener">ModularModels@microsoft.com</a>.&nbsp;</p>



<h2 class="wp-block-heading" id="acknowledgements">Acknowledgements</h2>



<p>We would like to thank paper collaborators: Prateek Yadav, Colin Raffel, Mohammed Muqeeth, Haokun Liu, Tianlong Chen, Mohit Bansal, Leshem Choshen, Edoardo Ponti, Zhan Su, <a href="https://www.microsoft.com/en-us/research/people/matpereira/">Matheus Pereira</a>, Nicolas Le Roux, Nabil Omi, <a href="https://www.microsoft.com/en-us/research/people/sidsen/">Siddhartha Sen</a>, Anurag Sarkar, Jordan T. Ash, Oleksiy Ostapenko, and Laurent Charlin.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/toward-modular-models-collaborative-ai-development-enables-model-accountability-and-continuous-learning/">Toward modular models: Collaborative AI development enables model accountability and continuous learning</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
	</channel>
</rss>
